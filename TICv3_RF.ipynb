{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37bd607-3c78-4c38-9cc1-681ab8be295e",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3c242c-24f5-4e57-ad01-5cb31fe0fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RANDOM FOREST — PRECISION-CONTROLLED v5s4\n",
    "# (SSD/HDD) — Big-mode en TRAIN y TEST (streaming)\n",
    "#   • FIX: NaT-safe en BIG loader\n",
    "#   • TEST 2024 streaming (sin cargar todo a RAM)\n",
    "#   • HDD big-mode: sin rolling (delta, cummax, age_days)\n",
    "# ============================================\n",
    "\n",
    "import os, gc, json, warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "# ===================== UTILS =====================\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "\n",
    "def downcast_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        elif pd.api.types.is_integer_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================== IO Helpers =====================\n",
    "\n",
    "def _list_parquet_files(path: str) -> List[str]:\n",
    "    if os.path.isdir(path):\n",
    "        return [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".parquet\")]\n",
    "    return [path]\n",
    "\n",
    "def _peek_row_group(path: str) -> pd.DataFrame:\n",
    "    # Lee un row-group chiquito para inspeccionar columnas\n",
    "    files = _list_parquet_files(path)\n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        if pf.num_row_groups > 0:\n",
    "            tb = pf.read_row_group(0, columns=None)\n",
    "            return tb.to_pandas().head(100)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def discover_smart_columns(path: str) -> List[str]:\n",
    "    peek = _peek_row_group(path)\n",
    "    if peek.empty:\n",
    "        return []\n",
    "    cols = peek.columns.tolist()\n",
    "    return sorted([c for c in cols if (\"smart\" in c.lower()) and c.endswith(\"_raw\")])\n",
    "\n",
    "\n",
    "def _iter_parquet_row_groups(path: str, years: List[int], columns: Optional[List[str]] = None):\n",
    "    files = _list_parquet_files(path)\n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for i in range(pf.num_row_groups):\n",
    "            tb = pf.read_row_group(i, columns=columns) if columns else pf.read_row_group(i)\n",
    "            df = tb.to_pandas()\n",
    "            if \"date\" not in df.columns:  # ignora row-groups sin fecha\n",
    "                continue\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            mask = df[\"date\"].dt.year.isin(years)\n",
    "            if mask.any():\n",
    "                yield downcast_df(df.loc[mask].reset_index(drop=True))\n",
    "            del df, tb\n",
    "            cleanup()\n",
    "\n",
    "\n",
    "def load_data(path: str, years: List[int], columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    print(f\"Loading {years} from {path}...\")\n",
    "    chunks = []\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=columns):\n",
    "        chunks.append(df)\n",
    "        if len(chunks) >= 16:\n",
    "            chunks = [pd.concat(chunks, ignore_index=True)]\n",
    "            cleanup()\n",
    "    out = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "    print(f\"Loaded {len(out):,} rows\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== BIG MODE (TRAIN y TEST) =====================\n",
    "\n",
    "def scan_fail_dates(path: str, years: List[int]) -> Dict[str, pd.Timestamp]:\n",
    "    \"\"\"Primera pasada: PRIMERA fecha de fallo por serial.\"\"\"\n",
    "    print(\"Scanning earliest failure dates (streaming)...\")\n",
    "    fail_map: Dict[str, pd.Timestamp] = {}\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=[\"serial_number\", \"date\", \"failure\"]):\n",
    "        df = df[df[\"failure\"] == 1]\n",
    "        if df.empty:\n",
    "            continue\n",
    "        sns = df[\"serial_number\"].astype(str).values\n",
    "        dts = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        for sn, dt in zip(sns, dts):\n",
    "            if pd.isna(dt):\n",
    "                continue\n",
    "            if sn not in fail_map or dt < fail_map[sn]:\n",
    "                fail_map[sn] = dt\n",
    "    print(f\"  Found {len(fail_map):,} failed serials\")\n",
    "    return fail_map\n",
    "\n",
    "\n",
    "def load_data_big_filtered(\n",
    "    path: str,\n",
    "    years: List[int],\n",
    "    fail_map: Dict[str, pd.Timestamp],\n",
    "    lookahead_days: int,\n",
    "    hard_window: int,\n",
    "    neg_random_keep_rate: float = 0.0025,\n",
    "    columns: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TRAIN big: conserva todos positivos, negativos cercanos y una muestra de lejanos.\n",
    "    (Evita traer 200M+ filas).\n",
    "    \"\"\"\n",
    "    print(f\"Loading BIG-FILTERED {years} from {path} (keep_rate={neg_random_keep_rate})...\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    kept = []\n",
    "    total_rows = 0\n",
    "    kept_rows = 0\n",
    "\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=columns):\n",
    "        total_rows += len(df)\n",
    "        sn_ser = df[\"serial_number\"].astype(str)\n",
    "        dt = pd.to_datetime(df[\"date\"], errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "\n",
    "        # map fail date y dtf (NaT-safe)\n",
    "        fail_series = sn_ser.map(fail_map)\n",
    "        fdt = pd.to_datetime(fail_series, errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "\n",
    "        dtf = np.full(len(df), 10**9, dtype=np.int64)\n",
    "        valid = ~np.isnat(fdt)\n",
    "        if valid.any():\n",
    "            dd = (fdt[valid] - dt[valid]).astype(\"timedelta64[D]\").astype(\"int64\")\n",
    "            dtf[valid] = dd\n",
    "\n",
    "        failure = (df[\"failure\"].values == 1)\n",
    "        pos_mask = failure | ((dtf >= 0) & (dtf <= lookahead_days))\n",
    "        near_mask = (dtf > lookahead_days) & (dtf <= hard_window)\n",
    "\n",
    "        keep = pos_mask | near_mask\n",
    "        far_neg = (~keep) & (~failure)\n",
    "        if far_neg.any():\n",
    "            # muestreo aleatorio de negativos lejanos\n",
    "            sample = rng.random(far_neg.sum()) < neg_random_keep_rate\n",
    "            sel = np.zeros_like(far_neg, dtype=bool)\n",
    "            sel[np.where(far_neg)[0]] = sample\n",
    "            keep = keep | sel\n",
    "\n",
    "        kept.append(df.loc[keep].reset_index(drop=True))\n",
    "        kept_rows += int(keep.sum())\n",
    "\n",
    "        if sum(len(x) for x in kept) > 3_000_000:\n",
    "            kept = [pd.concat(kept, ignore_index=True)]\n",
    "            cleanup()\n",
    "\n",
    "    out = pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n",
    "    rate = 100 * kept_rows / max(1, total_rows)\n",
    "    print(f\"  BIG-FILTERED kept {kept_rows:,}/{total_rows:,} rows (~{rate:.2f}%)\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== PREP =====================\n",
    "\n",
    "def prepare_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"serial_number\"] = df[\"serial_number\"].astype(str)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"serial_number\", \"date\"])\n",
    "    df = df.sort_values([\"serial_number\", \"date\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================== LABELS =====================\n",
    "\n",
    "def compute_days_to_failure(dfs: pd.DataFrame) -> np.ndarray:\n",
    "    fail_map: Dict[str, pd.Timestamp] = {}\n",
    "    fails = dfs[dfs[\"failure\"] == 1]\n",
    "    for sn, dt in zip(fails[\"serial_number\"], fails[\"date\"]):\n",
    "        fail_map[sn] = min(dt, fail_map.get(sn, dt))\n",
    "\n",
    "    dtf = np.full(len(dfs), 10**9, dtype=np.int64)\n",
    "    for i, (sn, dt) in enumerate(zip(dfs[\"serial_number\"], dfs[\"date\"])):\n",
    "        if sn in fail_map:\n",
    "            dtf[i] = (fail_map[sn] - dt).days\n",
    "    return dtf\n",
    "\n",
    "\n",
    "def create_labels_from_dtf(dtf: np.ndarray, lookahead: int = 7) -> np.ndarray:\n",
    "    return ((dtf >= 0) & (dtf <= lookahead)).astype(np.int8)\n",
    "\n",
    "\n",
    "# ===================== CATEGÓRICOS =====================\n",
    "\n",
    "def extract_vendor(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    return s.str.extract(r\"^([A-Za-z]+)\", expand=False).fillna(\"UNK\")\n",
    "\n",
    "def fit_category_maps(df: pd.DataFrame) -> Dict[str, Dict[str, int]]:\n",
    "    maps: Dict[str, Dict[str, int]] = {}\n",
    "    if \"model\" in df.columns:\n",
    "        models = pd.Index(df[\"model\"].astype(str).unique())\n",
    "        maps[\"model\"] = {m: i for i, m in enumerate(models)}\n",
    "        vendors = pd.Index(extract_vendor(df[\"model\"]).unique())\n",
    "        maps[\"vendor\"] = {v: i for i, v in enumerate(vendors)}\n",
    "    return maps\n",
    "\n",
    "def apply_category_maps(df: pd.DataFrame, maps: Optional[Dict[str, Dict[str, int]]]) -> Tuple[pd.Series, pd.Series]:\n",
    "    if maps and \"model\" in df.columns and \"model\" in maps and \"vendor\" in maps:\n",
    "        m = df[\"model\"].astype(str)\n",
    "        v = extract_vendor(m)\n",
    "        model_map = maps[\"model\"]\n",
    "        vendor_map = maps[\"vendor\"]\n",
    "        model_codes = m.map(model_map).fillna(-1).astype(int)\n",
    "        vendor_codes = v.map(vendor_map).fillna(-1).astype(int)\n",
    "        return model_codes, vendor_codes\n",
    "    n = len(df)\n",
    "    return pd.Series(np.zeros(n, dtype=np.int32), index=df.index), pd.Series(np.zeros(n, dtype=np.int32), index=df.index)\n",
    "\n",
    "\n",
    "# ===================== FEATURES =====================\n",
    "\n",
    "def create_features_joined(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    dataset_type: str,\n",
    "    add_rolling: bool,\n",
    "    fit_cats_on_train: bool = True,\n",
    "    category_maps: Optional[Dict[str, Dict[str, int]]] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str], Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"Modo estándar (RAM): TRAIN∪TEST → features → split.\"\"\"\n",
    "    print(f\"Creating features (temporal join) for {dataset_type}...\")\n",
    "\n",
    "    df_train = df_train.copy(); df_train[\"__subset__\"] = \"train\"\n",
    "    df_test  = df_test.copy();  df_test[\"__subset__\"]  = \"test\"\n",
    "    df_all = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    df_all.sort_values([\"serial_number\", \"date\"], inplace=True)\n",
    "\n",
    "    all_cols = df_all.columns.tolist()\n",
    "    smart_cols = [c for c in all_cols if (\"smart\" in c.lower()) and (\"_raw\" in c.lower())]\n",
    "    print(f\"  Found {len(smart_cols)} SMART attributes\")\n",
    "\n",
    "    for c in smart_cols:\n",
    "        df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    for c in smart_cols:\n",
    "        df_all[f\"delta_{c}\"] = df_all.groupby(\"serial_number\", sort=False)[c].diff().fillna(0)\n",
    "        df_all[f\"max_{c}\"]   = df_all.groupby(\"serial_number\", sort=False)[c].cummax()\n",
    "\n",
    "    if add_rolling:\n",
    "        for c in smart_cols:\n",
    "            r = df_all.groupby(\"serial_number\", sort=False)[c]\n",
    "            df_all[f\"rmean7_{c}\"] = r.rolling(window=7, min_periods=2).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "            df_all[f\"rstd7_{c}\"]  = r.rolling(window=7, min_periods=2).std().reset_index(level=0, drop=True).fillna(0)\n",
    "\n",
    "    df_all[\"age_days\"] = df_all.groupby(\"serial_number\", sort=False).cumcount()\n",
    "    d = df_all[\"date\"]\n",
    "    df_all[\"month\"] = d.dt.month\n",
    "    df_all[\"day_of_week\"] = d.dt.dayofweek\n",
    "\n",
    "    if fit_cats_on_train:\n",
    "        cat_maps = fit_category_maps(df_train)\n",
    "    else:\n",
    "        cat_maps = category_maps or {}\n",
    "    model_code, vendor_code = apply_category_maps(df_all, cat_maps)\n",
    "    df_all[\"model_code\"] = model_code\n",
    "    df_all[\"vendor_code\"] = vendor_code\n",
    "\n",
    "    drop_cols = [\"serial_number\", \"date\", \"failure\", \"model\"]\n",
    "    X_all = df_all.drop(columns=[c for c in drop_cols if c in df_all.columns], errors=\"ignore\")\n",
    "\n",
    "    for c in X_all.columns:\n",
    "        if X_all[c].dtype == \"object\":\n",
    "            X_all[c] = pd.Categorical(X_all[c]).codes\n",
    "        X_all[c] = pd.to_numeric(X_all[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    var = X_all.var()\n",
    "    keep = var[var > 0].index.tolist()\n",
    "    X_all = X_all[keep].astype(np.float32)\n",
    "\n",
    "    X_train = X_all[df_all[\"__subset__\"] == \"train\"].reset_index(drop=True)\n",
    "    X_test  = X_all[df_all[\"__subset__\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "    print(f\"  Final features: {X_all.shape[1]}\")\n",
    "    return X_train, X_test, keep, cat_maps\n",
    "\n",
    "\n",
    "# ========== STREAMING FEATURES (TEST BIG MODE, sin rolling) ==========\n",
    "\n",
    "class StreamDeltaCummaxBuilder:\n",
    "    \"\"\"\n",
    "    Construye features de TEST en streaming (sin rolling).\n",
    "    Mantiene por disco:\n",
    "      • last_vals[smart]   • cummax_vals[smart]   • age_days\n",
    "    \"\"\"\n",
    "    def __init__(self, smart_cols: List[str], cat_maps: Dict[str, Dict[str, int]]):\n",
    "        self.smart_cols = smart_cols\n",
    "        self.cat_maps = cat_maps\n",
    "        self.last_vals: Dict[str, Dict[str, float]] = {}     # serial -> {smart: last}\n",
    "        self.cummax_vals: Dict[str, Dict[str, float]] = {}   # serial -> {smart: cummax}\n",
    "        self.age: Dict[str, int] = {}                        # serial -> age counter\n",
    "\n",
    "    def _ensure_serial(self, sn: str):\n",
    "        if sn not in self.last_vals:\n",
    "            self.last_vals[sn] = {c: 0.0 for c in self.smart_cols}\n",
    "            self.cummax_vals[sn] = {c: 0.0 for c in self.smart_cols}\n",
    "            self.age[sn] = 0\n",
    "\n",
    "    def transform_chunk(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Orden por serial, date\n",
    "        df = df.sort_values([\"serial_number\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "        # Categóricos\n",
    "        model_code, vendor_code = apply_category_maps(df, self.cat_maps)\n",
    "        df[\"model_code\"] = model_code\n",
    "        df[\"vendor_code\"] = vendor_code\n",
    "\n",
    "        # Calendario\n",
    "        df[\"month\"] = df[\"date\"].dt.month\n",
    "        df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n",
    "\n",
    "        # Inicializa features\n",
    "        for c in self.smart_cols:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "        # Calcula delta/cummax/age en streaming por serial\n",
    "        deltas = {f\"delta_{c}\": [] for c in self.smart_cols}\n",
    "        cummaxs = {f\"max_{c}\": [] for c in self.smart_cols}\n",
    "        ages = []\n",
    "\n",
    "        serials = df[\"serial_number\"].astype(str).values\n",
    "        for idx, sn in enumerate(serials):\n",
    "            self._ensure_serial(sn)\n",
    "            ages.append(self.age[sn])\n",
    "            self.age[sn] += 1\n",
    "\n",
    "            for c in self.smart_cols:\n",
    "                v = float(df.at[idx, c])\n",
    "                d = v - self.last_vals[sn][c]\n",
    "                self.last_vals[sn][c] = v\n",
    "                self.cummax_vals[sn][c] = max(self.cummax_vals[sn][c], v)\n",
    "                deltas[f\"delta_{c}\"].append(d)\n",
    "                cummaxs[f\"max_{c}\"].append(self.cummax_vals[sn][c])\n",
    "\n",
    "        for k, arr in deltas.items():\n",
    "            df[k] = arr\n",
    "        for k, arr in cummaxs.items():\n",
    "            df[k] = arr\n",
    "        df[\"age_days\"] = ages\n",
    "\n",
    "        # Deja solo columnas de features posibles\n",
    "        feat_cols = (\n",
    "            self.smart_cols\n",
    "            + [f\"delta_{c}\" for c in self.smart_cols]\n",
    "            + [f\"max_{c}\" for c in self.smart_cols]\n",
    "            + [\"age_days\", \"month\", \"day_of_week\", \"model_code\", \"vendor_code\"]\n",
    "        )\n",
    "        return df[feat_cols].astype(np.float32)\n",
    "\n",
    "\n",
    "# ===================== CV, MODEL, MÉTRICAS =====================\n",
    "\n",
    "def make_group_folds(serials: pd.Series, y: np.ndarray, n_splits: int = 5, random_state: int = 42):\n",
    "    serials = serials.astype(str).values\n",
    "    uniq_serials, inverse = np.unique(serials, return_inverse=True)\n",
    "    y_disk = np.zeros(len(uniq_serials), dtype=np.int8)\n",
    "    for idx_row, disk_idx in enumerate(inverse):\n",
    "        if y[idx_row] == 1:\n",
    "            y_disk[disk_idx] = 1\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for tr_d, va_d in skf.split(uniq_serials, y_disk):\n",
    "        tr_mask = np.isin(inverse, tr_d)\n",
    "        va_mask = np.isin(inverse, va_d)\n",
    "        yield np.where(tr_mask)[0], np.where(va_mask)[0]\n",
    "\n",
    "\n",
    "def sample_negatives_hard(\n",
    "    X: pd.DataFrame, y: np.ndarray, dtf: np.ndarray, lookahead: int,\n",
    "    neg_pos_ratio: int = 3, hard_window: int = 60, hard_fraction: float = 0.7,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    if len(pos_idx) == 0:\n",
    "        raise ValueError(\"No positives in training fold for hard-negative sampling\")\n",
    "\n",
    "    n_pos = len(pos_idx)\n",
    "    n_neg_needed = n_pos * neg_pos_ratio\n",
    "\n",
    "    hard_mask = (dtf > lookahead) & (dtf <= hard_window)\n",
    "    hard_idx = np.where(hard_mask & (y == 0))[0]\n",
    "    easy_idx = np.where(~hard_mask & (y == 0))[0]\n",
    "\n",
    "    n_hard = min(int(n_neg_needed * hard_fraction), len(hard_idx))\n",
    "    n_easy = min(n_neg_needed - n_hard, len(easy_idx))\n",
    "\n",
    "    chosen_hard = rng.choice(hard_idx, size=n_hard, replace=False) if n_hard > 0 else np.empty(0, dtype=int)\n",
    "    chosen_easy = rng.choice(easy_idx, size=n_easy, replace=False) if n_easy > 0 else np.empty(0, dtype=int)\n",
    "\n",
    "    sel_idx = np.concatenate([pos_idx, chosen_hard, chosen_easy])\n",
    "    Xb = X.iloc[sel_idx].reset_index(drop=True)\n",
    "    yb = y[sel_idx]\n",
    "    return Xb, yb\n",
    "\n",
    "\n",
    "def train_rf(\n",
    "    X: pd.DataFrame, y: np.ndarray,\n",
    "    n_estimators: int = 500, max_depth: int = 16,\n",
    "    min_samples_leaf: int = 20, max_features: float | str = 0.3,\n",
    "    random_state: int = 42\n",
    ") -> RandomForestClassifier:\n",
    "    print(f\"  Train RF: n={len(y):,} (pos={int(y.sum()):,}) | trees={n_estimators} depth={max_depth} leaf={min_samples_leaf}\")\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        class_weight=None,\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        verbose=0,\n",
    "    )\n",
    "    rf.fit(X, y)\n",
    "    return rf\n",
    "\n",
    "\n",
    "def pick_threshold(\n",
    "    y_true: np.ndarray, proba: np.ndarray,\n",
    "    min_precision: float = 0.90, min_recall: float = 0.10,\n",
    "    top_k_rate: float = 1e-4, min_alerts: int = 5\n",
    "):\n",
    "    precision, recall, thr = precision_recall_curve(y_true, proba)\n",
    "    pr_auc = average_precision_score(y_true, proba)\n",
    "\n",
    "    valid = (precision >= min_precision) & (recall >= min_recall)\n",
    "    if valid.any():\n",
    "        idx = int(np.argmax(recall * valid))\n",
    "        chosen = thr[idx if idx < len(thr) else len(thr)-1]\n",
    "    else:\n",
    "        beta = 0.5\n",
    "        fbeta = ((1 + beta**2) * precision * recall) / (beta**2 * precision + recall + 1e-10)\n",
    "        idx = int(np.argmax(fbeta))\n",
    "        chosen = thr[idx if idx < len(thr) else len(thr)-1]\n",
    "\n",
    "    alerts = int((proba >= chosen).sum())\n",
    "    if alerts < max(1, min_alerts):\n",
    "        if top_k_rate and top_k_rate > 0:\n",
    "            k = max(max(1, min_alerts), int(len(proba) * top_k_rate))\n",
    "            chosen = float(np.partition(proba, -k)[-k])\n",
    "        else:\n",
    "            chosen = float(np.quantile(proba, 0.9999))\n",
    "\n",
    "    return float(chosen), float(pr_auc)\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true: np.ndarray, proba: np.ndarray, thr: float) -> Dict:\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        'confusion_matrix': [[int(tn), int(fp)], [int(fn), int(tp)]],\n",
    "        'fpr': float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "# ===================== MAIN PIPELINE =====================\n",
    "\n",
    "def train_random_forest_precision_pipeline(\n",
    "    train_parquet: str,\n",
    "    test_parquet: str | None,\n",
    "    dataset_type: str,\n",
    "    train_years: List[int] = [2020, 2021, 2022, 2023],\n",
    "    test_years: List[int] | None = [2024],\n",
    "    lookahead_days: int = 7,\n",
    "    n_splits: int = 5,\n",
    "    neg_pos_ratio: int = 3,\n",
    "    hard_window: int = 60,\n",
    "    hard_fraction: float = 0.7,\n",
    "    rf_n_estimators: int = 500,\n",
    "    rf_max_depth: int = 16,\n",
    "    rf_min_samples_leaf: int = 20,\n",
    "    rf_max_features: float | str = 0.3,\n",
    "    min_precision: float = 0.85,\n",
    "    min_recall: float = 0.05,\n",
    "    top_k_rate: float = 2e-4,\n",
    "    min_alerts: int = 20,\n",
    "    output_dir: str = './models_rf_precision_v5s4',\n",
    "    random_state: int = 42,\n",
    "    # BIG mode switches\n",
    "    big_mode: bool = False,                 # TRAIN big\n",
    "    test_big_mode: bool = False,            # TEST big\n",
    "    neg_random_keep_rate: float = 0.0025,   # TRAIN big keep rate\n",
    "    add_rolling: bool = True,               # usa rolling? (desactivar en HDD big)\n",
    "):\n",
    "    print(\"=\"*100)\n",
    "    print(f\"RANDOM FOREST (PRECISION-CONTROLLED v5s4) - {dataset_type.upper()}\")\n",
    "    print(\"=\"*100)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Descubrir columnas SMART para leer solo lo necesario\n",
    "    smart_cols = discover_smart_columns(train_parquet)\n",
    "    base_cols = ['serial_number', 'date', 'failure', 'model']\n",
    "    read_cols = base_cols + smart_cols\n",
    "\n",
    "    # ---------- Load TRAIN ----------\n",
    "    if big_mode:\n",
    "        fail_map_train = scan_fail_dates(train_parquet, train_years)\n",
    "        df_train_raw = load_data_big_filtered(\n",
    "            train_parquet, train_years, fail_map_train,\n",
    "            lookahead_days=lookahead_days, hard_window=hard_window,\n",
    "            neg_random_keep_rate=neg_random_keep_rate, columns=read_cols\n",
    "        )\n",
    "    else:\n",
    "        df_train_raw = load_data(train_parquet, train_years, columns=read_cols)\n",
    "\n",
    "    # ---------- Load TEST (si no es big, RAM) ----------\n",
    "    if test_parquet and test_years and (not test_big_mode):\n",
    "        df_test_raw = load_data(test_parquet, test_years, columns=read_cols)\n",
    "    else:\n",
    "        df_test_raw = pd.DataFrame()  # TEST big se hace en streaming más adelante\n",
    "\n",
    "    if df_train_raw.empty:\n",
    "        raise ValueError(\"Training data is empty!\")\n",
    "\n",
    "    # ---------- Prepare ----------\n",
    "    df_train = prepare_df(df_train_raw)\n",
    "    df_test  = prepare_df(df_test_raw) if not df_test_raw.empty else pd.DataFrame()\n",
    "\n",
    "    # ---------- Labels / DTF ----------\n",
    "    dtf_all = compute_days_to_failure(df_train)\n",
    "    y_all = create_labels_from_dtf(dtf_all, lookahead_days)\n",
    "    print(f\"Labels (train): {int(y_all.sum()):,} positive ({100*y_all.mean():.4f}%)\")\n",
    "    if y_all.sum() < 50:\n",
    "        raise ValueError(f\"Insufficient positive samples in TRAIN: {y_all.sum()}\")\n",
    "\n",
    "    # ---------- Features (TRAIN, y TEST si no es big) ----------\n",
    "    X_all, X_test, feature_names, cat_maps = create_features_joined(\n",
    "        df_train, df_test, dataset_type,\n",
    "        add_rolling=add_rolling,\n",
    "        fit_cats_on_train=True, category_maps=None\n",
    "    )\n",
    "\n",
    "    # ---------- Grouped CV en TRAIN ----------\n",
    "    serials_all = df_train['serial_number']\n",
    "    oof_proba = np.zeros(len(y_all), dtype=np.float32)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(make_group_folds(serials_all, y_all, n_splits=n_splits, random_state=random_state), start=1):\n",
    "        X_tr, y_tr = X_all.iloc[tr_idx].reset_index(drop=True), y_all[tr_idx]\n",
    "        dtf_tr = dtf_all[tr_idx]\n",
    "        X_va, y_va = X_all.iloc[va_idx].reset_index(drop=True), y_all[va_idx]\n",
    "\n",
    "        print(f\"\\nFold {fold}/{n_splits}: train={len(y_tr):,} (pos={int(y_tr.sum()):,}) | val={len(y_va):,} (pos={int(y_va.sum()):,})\")\n",
    "\n",
    "        Xb, yb = sample_negatives_hard(\n",
    "            X_tr, y_tr, dtf_tr,\n",
    "            lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio,\n",
    "            hard_window=hard_window,\n",
    "            hard_fraction=hard_fraction,\n",
    "            seed=random_state,\n",
    "        )\n",
    "        print(f\"  After hard-neg sampling: {len(yb):,} (pos={int(yb.sum()):,}, neg={len(yb)-int(yb.sum()):,})\")\n",
    "\n",
    "        model = train_rf(\n",
    "            Xb, yb,\n",
    "            n_estimators=rf_n_estimators,\n",
    "            max_depth=rf_max_depth,\n",
    "            min_samples_leaf=rf_min_samples_leaf,\n",
    "            max_features=rf_max_features,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        proba_va = model.predict_proba(X_va)[:, 1]\n",
    "        thr, pr_auc = pick_threshold(\n",
    "            y_va, proba_va,\n",
    "            min_precision=min_precision,\n",
    "            min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate,\n",
    "            min_alerts=min_alerts,\n",
    "        )\n",
    "        oof_proba[va_idx] = proba_va\n",
    "\n",
    "        m = metrics_at_threshold(y_va, proba_va, thr)\n",
    "        m.update({'pr_auc': float(pr_auc), 'threshold': float(thr), 'fold': int(fold)})\n",
    "        fold_metrics.append(m)\n",
    "\n",
    "        del X_tr, y_tr, X_va, y_va, Xb, yb, model\n",
    "        cleanup()\n",
    "\n",
    "    # ---------- Global threshold from OOF ----------\n",
    "    print(\"\\nAggregating OOF predictions to choose a global threshold...\")\n",
    "    thr_global, pr_auc_oof = pick_threshold(\n",
    "        y_all, oof_proba,\n",
    "        min_precision=min_precision,\n",
    "        min_recall=min_recall,\n",
    "        top_k_rate=top_k_rate,\n",
    "        min_alerts=min_alerts,\n",
    "    )\n",
    "    agg_metrics = metrics_at_threshold(y_all, oof_proba, thr_global)\n",
    "    agg_metrics.update({'pr_auc': float(pr_auc_oof), 'threshold': float(thr_global)})\n",
    "\n",
    "    print(\"\\nOOF Performance (using global threshold):\")\n",
    "    print(json.dumps(agg_metrics, indent=2))\n",
    "\n",
    "    # ---------- FINAL model ----------\n",
    "    Xb_full, yb_full = sample_negatives_hard(\n",
    "        X_all, y_all, dtf_all,\n",
    "        lookahead=lookahead_days,\n",
    "        neg_pos_ratio=neg_pos_ratio,\n",
    "        hard_window=hard_window,\n",
    "        hard_fraction=hard_fraction,\n",
    "        seed=random_state,\n",
    "    )\n",
    "    final_model = train_rf(\n",
    "        Xb_full, yb_full,\n",
    "        n_estimators=rf_n_estimators,\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_leaf=rf_min_samples_leaf,\n",
    "        max_features=rf_max_features,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_prefix = os.path.join(output_dir, f\"{dataset_type}_precv5s4_{timestamp}\")\n",
    "    joblib.dump(final_model, f\"{model_prefix}_model.pkl\")\n",
    "    with open(f\"{model_prefix}_features.json\", 'w') as f:\n",
    "        json.dump({'feature_names': feature_names, 'threshold': float(thr_global), 'category_maps': cat_maps}, f, indent=2)\n",
    "    print(f\"\\n✓ Final model saved: {model_prefix}_model.pkl\")\n",
    "\n",
    "    # ---------- TEST (RAM normal) ----------\n",
    "    test_metrics = None\n",
    "    if not df_test.empty and (not test_big_mode):\n",
    "        proba_test = final_model.predict_proba(X_test)[:, 1]\n",
    "        dtf_test = compute_days_to_failure(df_test)\n",
    "        y_test = create_labels_from_dtf(dtf_test, lookahead_days)\n",
    "        test_metrics = metrics_at_threshold(y_test, proba_test, thr_global)\n",
    "        test_metrics.update({'pr_auc': float(average_precision_score(y_test, proba_test)), 'threshold_used': float(thr_global)})\n",
    "\n",
    "    # ---------- Metadata ----------\n",
    "    metadata = {\n",
    "        'dataset_type': dataset_type,\n",
    "        'train_years': train_years,\n",
    "        'test_years': test_years,\n",
    "        'lookahead_days': lookahead_days,\n",
    "        'n_splits': n_splits,\n",
    "        'neg_pos_ratio': neg_pos_ratio,\n",
    "        'hard_window': hard_window,\n",
    "        'hard_fraction': hard_fraction,\n",
    "        'rf_params': {\n",
    "            'n_estimators': rf_n_estimators,\n",
    "            'max_depth': rf_max_depth,\n",
    "            'min_samples_leaf': rf_min_samples_leaf,\n",
    "            'max_features': rf_max_features,\n",
    "        },\n",
    "        'min_precision': min_precision,\n",
    "        'min_recall': min_recall,\n",
    "        'top_k_rate': top_k_rate,\n",
    "        'min_alerts': min_alerts,\n",
    "        'oof_metrics': agg_metrics,\n",
    "        'fold_metrics': fold_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'feature_names': feature_names,\n",
    "        'smart_cols': smart_cols\n",
    "    }\n",
    "\n",
    "    meta_path = os.path.join(output_dir, f\"{dataset_type}_precv5s4_{timestamp}_metadata.json\")\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"\\n✓ Metadata saved: {meta_path}\")\n",
    "    print(\"=\"*100)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# ===================== TEST-ONLY 2024 (STREAMING, BIG) =====================\n",
    "\n",
    "def evaluate_saved_model_2024_streaming(\n",
    "    model_path: str,\n",
    "    features_meta_path: str,  # el *_features.json (con feature_names, thr, cat_maps)\n",
    "    parquet_path: str,\n",
    "    train_years: List[int],   # para continuidad categórica, no cargamos train completo\n",
    "    test_years: List[int],    # usualmente [2024]\n",
    "    lookahead_days: int = 7,\n",
    "    chunk_limit_rows: int = 0 # 0 = todos los row-groups\n",
    "):\n",
    "    \"\"\"\n",
    "    Evalúa 2024 en streaming:\n",
    "      • NO carga todo 2024 a RAM.\n",
    "      • Construye labels con fail_map (primera fecha de fallo).\n",
    "      • Features sin rolling (delta, cummax, age_days) con estado por serial.\n",
    "      • Devuelve métricas a nivel row y disk usando thr del meta.\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"EVALUATE SAVED MODEL — TEST 2024 (STREAMING)\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    with open(features_meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    feature_names = meta[\"feature_names\"]\n",
    "    thr = float(meta.get(\"threshold\", 0.5))\n",
    "    cat_maps = meta.get(\"category_maps\", {})\n",
    "\n",
    "    smart_cols = discover_smart_columns(parquet_path)\n",
    "    base_cols = ['serial_number', 'date', 'failure', 'model']\n",
    "    read_cols = base_cols + smart_cols\n",
    "\n",
    "    # Labeling por fail_map del TEST\n",
    "    fail_map_test = scan_fail_dates(parquet_path, test_years)\n",
    "\n",
    "    builder = StreamDeltaCummaxBuilder(smart_cols=smart_cols, cat_maps=cat_maps)\n",
    "\n",
    "    # Acumuladores de métricas en streaming (para umbral thr)\n",
    "    tn=fp=fn=tp=0\n",
    "    # Para métricas por disco: guardamos max proba y max label por serial al vuelo\n",
    "    disk_stat: Dict[str, Tuple[float, int]] = {}  # serial -> (max_proba, max_y)\n",
    "\n",
    "    processed_groups = 0\n",
    "    for df in _iter_parquet_row_groups(parquet_path, test_years, columns=read_cols):\n",
    "        if chunk_limit_rows and processed_groups >= chunk_limit_rows:\n",
    "            break\n",
    "\n",
    "        # Labels con lookahead\n",
    "        sn_ser = df[\"serial_number\"].astype(str)\n",
    "        dt = pd.to_datetime(df[\"date\"], errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "        fdt = pd.to_datetime(sn_ser.map(fail_map_test), errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "        dtf = np.full(len(df), 10**9, dtype=np.int64)\n",
    "        valid = ~np.isnat(fdt)\n",
    "        if valid.any():\n",
    "            dd = (fdt[valid] - dt[valid]).astype(\"timedelta64[D]\").astype(\"int64\")\n",
    "            dtf[valid] = dd\n",
    "        y_chunk = ((dtf >= 0) & (dtf <= lookahead_days)).astype(np.int8)\n",
    "\n",
    "        # Features streaming sin rolling\n",
    "        X_chunk = builder.transform_chunk(df)\n",
    "\n",
    "        # Reordena columnas faltantes/sobrantes para el modelo\n",
    "        for m in feature_names:\n",
    "            if m not in X_chunk.columns:\n",
    "                X_chunk[m] = 0.0\n",
    "        X_chunk = X_chunk[feature_names].astype(np.float32)\n",
    "\n",
    "        proba = model.predict_proba(X_chunk)[:, 1]\n",
    "        y_pred = (proba >= thr).astype(np.int8)\n",
    "\n",
    "        # Confusion row-level\n",
    "        cm = confusion_matrix(y_chunk, y_pred, labels=[0,1])\n",
    "        tn += int(cm[0,0]); fp += int(cm[0,1]); fn += int(cm[1,0]); tp += int(cm[1,1])\n",
    "\n",
    "        # Disk-level: actualiza max proba y max y por serial\n",
    "        for s, p, y in zip(sn_ser.values, proba, y_chunk):\n",
    "            if s not in disk_stat:\n",
    "                disk_stat[s] = (p, int(y))\n",
    "            else:\n",
    "                mp, my = disk_stat[s]\n",
    "                disk_stat[s] = (max(mp, p), max(my, int(y)))\n",
    "\n",
    "        processed_groups += 1\n",
    "        cleanup()\n",
    "\n",
    "    # Métricas fila\n",
    "    row_metrics = {\n",
    "        'precision': float(tp / max(1, tp + fp)),\n",
    "        'recall': float(tp / max(1, tp + fn)),\n",
    "        'f1': float((2*tp) / max(1, 2*tp + fp + fn)),\n",
    "        'confusion_matrix': [[tn, fp], [fn, tp]],\n",
    "        'fpr': float(fp / max(1, fp + tn))\n",
    "    }\n",
    "\n",
    "    # Métricas por disco\n",
    "    y_disk = np.array([v[1] for v in disk_stat.values()], dtype=np.int8)\n",
    "    yhat_disk = np.array([1 if v[0] >= thr else 0 for v in disk_stat.values()], dtype=np.int8)\n",
    "    cm_d = confusion_matrix(y_disk, yhat_disk, labels=[0,1])\n",
    "    tn_d, fp_d, fn_d, tp_d = int(cm_d[0,0]), int(cm_d[0,1]), int(cm_d[1,0]), int(cm_d[1,1])\n",
    "    disk_metrics = {\n",
    "        'precision': float(tp_d / max(1, tp_d + fp_d)),\n",
    "        'recall': float(tp_d / max(1, tp_d + fn_d)),\n",
    "        'f1': float((2*tp_d) / max(1, 2*tp_d + fp_d + fn_d)),\n",
    "        'confusion_matrix': [[tn_d, fp_d], [fn_d, tp_d]],\n",
    "        'n_disks': int(len(disk_stat))\n",
    "    }\n",
    "\n",
    "    print(\"\\nRow-level TEST metrics (streaming 2024):\")\n",
    "    print(json.dumps(row_metrics, indent=2))\n",
    "    print(\"\\nDisk-level TEST metrics (streaming 2024):\")\n",
    "    print(json.dumps(disk_metrics, indent=2))\n",
    "    print(\"=\"*100)\n",
    "    return {\"row\": row_metrics, \"disk\": disk_metrics}\n",
    "\n",
    "\n",
    "# ===================== WRAPPERS =====================\n",
    "\n",
    "def train_ssd_precision_v5s4():\n",
    "    # SSD cabe en RAM → rolling ON, test normal\n",
    "    return train_random_forest_precision_pipeline(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        train_years=[2020, 2021, 2022, 2023],\n",
    "        test_years=[2024],\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "        neg_pos_ratio=3,\n",
    "        hard_window=60,\n",
    "        hard_fraction=0.7,\n",
    "        rf_n_estimators=500,\n",
    "        rf_max_depth=16,\n",
    "        rf_min_samples_leaf=20,\n",
    "        rf_max_features=0.3,\n",
    "        min_precision=0.85,\n",
    "        min_recall=0.05,\n",
    "        top_k_rate=2e-4,\n",
    "        min_alerts=20,\n",
    "        output_dir='./models_rf_ssd_precv5s4',\n",
    "        big_mode=False,\n",
    "        test_big_mode=False,\n",
    "        neg_random_keep_rate=0.0025,\n",
    "        add_rolling=True\n",
    "    )\n",
    "\n",
    "def train_hdd_precision_v5s4(neg_random_keep_rate: float = 0.0025):\n",
    "    # HDD enorme → TRAIN big + TEST streaming; rolling OFF para poder hacer streaming\n",
    "    return train_random_forest_precision_pipeline(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        dataset_type='HDD',\n",
    "        train_years=[2020, 2021, 2022, 2023],\n",
    "        test_years=[2024],\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "        neg_pos_ratio=3,\n",
    "        hard_window=90,\n",
    "        hard_fraction=0.7,\n",
    "        rf_n_estimators=400,\n",
    "        rf_max_depth=16,\n",
    "        rf_min_samples_leaf=20,\n",
    "        rf_max_features=0.3,\n",
    "        min_precision=0.85,\n",
    "        min_recall=0.05,\n",
    "        top_k_rate=1e-4,\n",
    "        min_alerts=20,\n",
    "        output_dir='./models_rf_hdd_precv5s4',\n",
    "        big_mode=True,\n",
    "        test_big_mode=True,\n",
    "        neg_random_keep_rate=neg_random_keep_rate,\n",
    "        add_rolling=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ce110a-c7dc-4135-b5a6-a34a2d3412b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "RANDOM FOREST (PRECISION-CONTROLLED v5s4) - SSD\n",
      "====================================================================================================\n",
      "Loading [2020, 2021, 2022, 2023] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 2,124,111 rows\n",
      "Loading [2024] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 1,220,745 rows\n",
      "Labels (train): 1,325 positive (0.0624%)\n",
      "Creating features (temporal join) for SSD...\n",
      "  Found 13 SMART attributes\n",
      "  Final features: 71\n",
      "\n",
      "Fold 1/5: train=1,709,653 (pos=1,079) | val=414,458 (pos=246)\n",
      "  After hard-neg sampling: 4,316 (pos=1,079, neg=3,237)\n",
      "  Train RF: n=4,316 (pos=1,079) | trees=500 depth=16 leaf=20\n",
      "\n",
      "Fold 2/5: train=1,707,701 (pos=1,054) | val=416,410 (pos=271)\n",
      "  After hard-neg sampling: 4,216 (pos=1,054, neg=3,162)\n",
      "  Train RF: n=4,216 (pos=1,054) | trees=500 depth=16 leaf=20\n",
      "\n",
      "Fold 3/5: train=1,682,081 (pos=1,061) | val=442,030 (pos=264)\n",
      "  After hard-neg sampling: 4,244 (pos=1,061, neg=3,183)\n",
      "  Train RF: n=4,244 (pos=1,061) | trees=500 depth=16 leaf=20\n",
      "\n",
      "Fold 4/5: train=1,691,757 (pos=1,061) | val=432,354 (pos=264)\n",
      "  After hard-neg sampling: 4,244 (pos=1,061, neg=3,183)\n",
      "  Train RF: n=4,244 (pos=1,061) | trees=500 depth=16 leaf=20\n",
      "\n",
      "Fold 5/5: train=1,705,252 (pos=1,045) | val=418,859 (pos=280)\n",
      "  After hard-neg sampling: 4,180 (pos=1,045, neg=3,135)\n",
      "  Train RF: n=4,180 (pos=1,045) | trees=500 depth=16 leaf=20\n",
      "\n",
      "Aggregating OOF predictions to choose a global threshold...\n",
      "\n",
      "OOF Performance (using global threshold):\n",
      "{\n",
      "  \"precision\": 0.8514056224899599,\n",
      "  \"recall\": 0.16,\n",
      "  \"f1\": 0.26937738246505716,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      2122749,\n",
      "      37\n",
      "    ],\n",
      "    [\n",
      "      1113,\n",
      "      212\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 1.7429924636774504e-05,\n",
      "  \"pr_auc\": 0.25883732136200555,\n",
      "  \"threshold\": 0.8173822164535522\n",
      "}\n",
      "  Train RF: n=5,300 (pos=1,325) | trees=500 depth=16 leaf=20\n",
      "\n",
      "✓ Final model saved: ./models_rf_ssd_precv5s4/SSD_precv5s4_20251106_033824_model.pkl\n",
      "\n",
      "✓ Metadata saved: ./models_rf_ssd_precv5s4/SSD_precv5s4_20251106_033824_metadata.json\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'SSD',\n",
       " 'train_years': [2020, 2021, 2022, 2023],\n",
       " 'test_years': [2024],\n",
       " 'lookahead_days': 7,\n",
       " 'n_splits': 5,\n",
       " 'neg_pos_ratio': 3,\n",
       " 'hard_window': 60,\n",
       " 'hard_fraction': 0.7,\n",
       " 'rf_params': {'n_estimators': 500,\n",
       "  'max_depth': 16,\n",
       "  'min_samples_leaf': 20,\n",
       "  'max_features': 0.3},\n",
       " 'min_precision': 0.85,\n",
       " 'min_recall': 0.05,\n",
       " 'top_k_rate': 0.0002,\n",
       " 'min_alerts': 20,\n",
       " 'oof_metrics': {'precision': 0.8514056224899599,\n",
       "  'recall': 0.16,\n",
       "  'f1': 0.26937738246505716,\n",
       "  'confusion_matrix': [[2122749, 37], [1113, 212]],\n",
       "  'fpr': 1.7429924636774504e-05,\n",
       "  'pr_auc': 0.25883732136200555,\n",
       "  'threshold': 0.8173822164535522},\n",
       " 'fold_metrics': [{'precision': 0.8507462686567164,\n",
       "   'recall': 0.23170731707317074,\n",
       "   'f1': 0.36421725239616615,\n",
       "   'confusion_matrix': [[414202, 10], [189, 57]],\n",
       "   'fpr': 2.414222668585169e-05,\n",
       "   'pr_auc': 0.28124776008354263,\n",
       "   'threshold': 0.6681490961467218,\n",
       "   'fold': 1},\n",
       "  {'precision': 0.8548387096774194,\n",
       "   'recall': 0.19557195571955718,\n",
       "   'f1': 0.3183183183183183,\n",
       "   'confusion_matrix': [[416130, 9], [218, 53]],\n",
       "   'fpr': 2.1627388925335044e-05,\n",
       "   'pr_auc': 0.2500446531101222,\n",
       "   'threshold': 0.7871673338244447,\n",
       "   'fold': 2},\n",
       "  {'precision': 0.8529411764705882,\n",
       "   'recall': 0.2196969696969697,\n",
       "   'f1': 0.3493975903614458,\n",
       "   'confusion_matrix': [[441756, 10], [206, 58]],\n",
       "   'fpr': 2.263641837533898e-05,\n",
       "   'pr_auc': 0.31962678105287345,\n",
       "   'threshold': 0.794434454575357,\n",
       "   'fold': 3},\n",
       "  {'precision': 0.8571428571428571,\n",
       "   'recall': 0.11363636363636363,\n",
       "   'f1': 0.20066889632107024,\n",
       "   'confusion_matrix': [[432085, 5], [234, 30]],\n",
       "   'fpr': 1.1571663310884307e-05,\n",
       "   'pr_auc': 0.2591525978734663,\n",
       "   'threshold': 0.8843755157072507,\n",
       "   'fold': 4},\n",
       "  {'precision': 0.8571428571428571,\n",
       "   'recall': 0.12857142857142856,\n",
       "   'f1': 0.2236024844720497,\n",
       "   'confusion_matrix': [[418573, 6], [244, 36]],\n",
       "   'fpr': 1.4334211701972627e-05,\n",
       "   'pr_auc': 0.27443537451168953,\n",
       "   'threshold': 0.7949688005602604,\n",
       "   'fold': 5}],\n",
       " 'test_metrics': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'confusion_matrix': [[1219596, 922], [227, 0]],\n",
       "  'fpr': 0.0007554169623061684,\n",
       "  'pr_auc': 0.0008853032722148741,\n",
       "  'threshold_used': 0.8173822164535522},\n",
       " 'feature_names': ['smart_173_raw',\n",
       "  'smart_174_raw',\n",
       "  'smart_184_raw',\n",
       "  'smart_187_raw',\n",
       "  'smart_194_raw',\n",
       "  'smart_199_raw',\n",
       "  'smart_230_raw',\n",
       "  'smart_231_raw',\n",
       "  'smart_233_raw',\n",
       "  'smart_241_raw',\n",
       "  'smart_242_raw',\n",
       "  'smart_5_raw',\n",
       "  'smart_9_raw',\n",
       "  '__subset__',\n",
       "  'delta_smart_173_raw',\n",
       "  'max_smart_173_raw',\n",
       "  'delta_smart_174_raw',\n",
       "  'max_smart_174_raw',\n",
       "  'delta_smart_184_raw',\n",
       "  'max_smart_184_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'delta_smart_194_raw',\n",
       "  'max_smart_194_raw',\n",
       "  'delta_smart_199_raw',\n",
       "  'max_smart_199_raw',\n",
       "  'delta_smart_230_raw',\n",
       "  'max_smart_230_raw',\n",
       "  'delta_smart_231_raw',\n",
       "  'max_smart_231_raw',\n",
       "  'delta_smart_233_raw',\n",
       "  'max_smart_233_raw',\n",
       "  'delta_smart_241_raw',\n",
       "  'max_smart_241_raw',\n",
       "  'delta_smart_242_raw',\n",
       "  'max_smart_242_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'delta_smart_9_raw',\n",
       "  'max_smart_9_raw',\n",
       "  'rmean7_smart_173_raw',\n",
       "  'rstd7_smart_173_raw',\n",
       "  'rmean7_smart_174_raw',\n",
       "  'rstd7_smart_174_raw',\n",
       "  'rmean7_smart_184_raw',\n",
       "  'rstd7_smart_184_raw',\n",
       "  'rmean7_smart_187_raw',\n",
       "  'rstd7_smart_187_raw',\n",
       "  'rmean7_smart_194_raw',\n",
       "  'rstd7_smart_194_raw',\n",
       "  'rmean7_smart_199_raw',\n",
       "  'rstd7_smart_199_raw',\n",
       "  'rmean7_smart_230_raw',\n",
       "  'rstd7_smart_230_raw',\n",
       "  'rmean7_smart_231_raw',\n",
       "  'rstd7_smart_231_raw',\n",
       "  'rmean7_smart_233_raw',\n",
       "  'rstd7_smart_233_raw',\n",
       "  'rmean7_smart_241_raw',\n",
       "  'rstd7_smart_241_raw',\n",
       "  'rmean7_smart_242_raw',\n",
       "  'rstd7_smart_242_raw',\n",
       "  'rmean7_smart_5_raw',\n",
       "  'rstd7_smart_5_raw',\n",
       "  'rmean7_smart_9_raw',\n",
       "  'rstd7_smart_9_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week',\n",
       "  'model_code',\n",
       "  'vendor_code'],\n",
       " 'smart_cols': ['smart_173_raw',\n",
       "  'smart_174_raw',\n",
       "  'smart_184_raw',\n",
       "  'smart_187_raw',\n",
       "  'smart_194_raw',\n",
       "  'smart_199_raw',\n",
       "  'smart_230_raw',\n",
       "  'smart_231_raw',\n",
       "  'smart_233_raw',\n",
       "  'smart_241_raw',\n",
       "  'smart_242_raw',\n",
       "  'smart_5_raw',\n",
       "  'smart_9_raw']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_precision_v5s4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79c3402-7d34-48e6-8b91-036d461c7004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "RANDOM FOREST (PRECISION-CONTROLLED v5s4) - HDD\n",
      "====================================================================================================\n",
      "Scanning earliest failure dates (streaming)...\n",
      "  Found 8,044 failed serials\n",
      "Loading BIG-FILTERED [2020, 2021, 2022, 2023] from ./Procesados/finales/HDD_FULL_CLEAN.parquet (keep_rate=0.0025)...\n",
      "  BIG-FILTERED kept 1,184,183/215,762,596 rows (~0.55%)\n",
      "Labels (train): 62,131 positive (5.2467%)\n",
      "Creating features (temporal join) for HDD...\n",
      "  Found 5 SMART attributes\n",
      "  Final features: 20\n",
      "\n",
      "Fold 1/5: train=948,389 (pos=49,699) | val=235,794 (pos=12,432)\n",
      "  After hard-neg sampling: 198,796 (pos=49,699, neg=149,097)\n",
      "  Train RF: n=198,796 (pos=49,699) | trees=400 depth=16 leaf=20\n",
      "\n",
      "Fold 2/5: train=946,580 (pos=49,693) | val=237,603 (pos=12,438)\n",
      "  After hard-neg sampling: 198,772 (pos=49,693, neg=149,079)\n",
      "  Train RF: n=198,772 (pos=49,693) | trees=400 depth=16 leaf=20\n",
      "\n",
      "Fold 3/5: train=948,183 (pos=49,752) | val=236,000 (pos=12,379)\n",
      "  After hard-neg sampling: 199,008 (pos=49,752, neg=149,256)\n",
      "  Train RF: n=199,008 (pos=49,752) | trees=400 depth=16 leaf=20\n",
      "\n",
      "Fold 4/5: train=946,121 (pos=49,655) | val=238,062 (pos=12,476)\n",
      "  After hard-neg sampling: 198,620 (pos=49,655, neg=148,965)\n",
      "  Train RF: n=198,620 (pos=49,655) | trees=400 depth=16 leaf=20\n",
      "\n",
      "Fold 5/5: train=947,459 (pos=49,725) | val=236,724 (pos=12,406)\n",
      "  After hard-neg sampling: 198,900 (pos=49,725, neg=149,175)\n",
      "  Train RF: n=198,900 (pos=49,725) | trees=400 depth=16 leaf=20\n",
      "\n",
      "Aggregating OOF predictions to choose a global threshold...\n",
      "\n",
      "OOF Performance (using global threshold):\n",
      "{\n",
      "  \"precision\": 0.85000591326091,\n",
      "  \"recall\": 0.8097568041718305,\n",
      "  \"f1\": 0.8293933399274646,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      1113174,\n",
      "      8878\n",
      "    ],\n",
      "    [\n",
      "      11820,\n",
      "      50311\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 0.007912289270016007,\n",
      "  \"pr_auc\": 0.8902504742465743,\n",
      "  \"threshold\": 0.5678298473358154\n",
      "}\n",
      "  Train RF: n=248,524 (pos=62,131) | trees=400 depth=16 leaf=20\n",
      "\n",
      "✓ Final model saved: ./models_rf_hdd_precv5s4/HDD_precv5s4_20251106_032905_model.pkl\n",
      "\n",
      "✓ Metadata saved: ./models_rf_hdd_precv5s4/HDD_precv5s4_20251106_032905_metadata.json\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'HDD',\n",
       " 'train_years': [2020, 2021, 2022, 2023],\n",
       " 'test_years': [2024],\n",
       " 'lookahead_days': 7,\n",
       " 'n_splits': 5,\n",
       " 'neg_pos_ratio': 3,\n",
       " 'hard_window': 90,\n",
       " 'hard_fraction': 0.7,\n",
       " 'rf_params': {'n_estimators': 400,\n",
       "  'max_depth': 16,\n",
       "  'min_samples_leaf': 20,\n",
       "  'max_features': 0.3},\n",
       " 'min_precision': 0.85,\n",
       " 'min_recall': 0.05,\n",
       " 'top_k_rate': 0.0001,\n",
       " 'min_alerts': 20,\n",
       " 'oof_metrics': {'precision': 0.85000591326091,\n",
       "  'recall': 0.8097568041718305,\n",
       "  'f1': 0.8293933399274646,\n",
       "  'confusion_matrix': [[1113174, 8878], [11820, 50311]],\n",
       "  'fpr': 0.007912289270016007,\n",
       "  'pr_auc': 0.8902504742465743,\n",
       "  'threshold': 0.5678298473358154},\n",
       " 'fold_metrics': [{'precision': 0.8500422654268808,\n",
       "   'recall': 0.8088803088803089,\n",
       "   'f1': 0.8289506223724342,\n",
       "   'confusion_matrix': [[221588, 1774], [2376, 10056]],\n",
       "   'fpr': 0.007942264127291123,\n",
       "   'pr_auc': 0.8899268014273016,\n",
       "   'threshold': 0.5486192780535112,\n",
       "   'fold': 1},\n",
       "  {'precision': 0.8500588334173811,\n",
       "   'recall': 0.813153240070751,\n",
       "   'f1': 0.8311965811965812,\n",
       "   'confusion_matrix': [[223381, 1784], [2324, 10114]],\n",
       "   'fpr': 0.00792307863122599,\n",
       "   'pr_auc': 0.8903831057975007,\n",
       "   'threshold': 0.5724622833492793,\n",
       "   'fold': 2},\n",
       "  {'precision': 0.8500598392887673,\n",
       "   'recall': 0.8032959043541481,\n",
       "   'f1': 0.8260165302986252,\n",
       "   'confusion_matrix': [[221867, 1754], [2435, 9944]],\n",
       "   'fpr': 0.007843628281780333,\n",
       "   'pr_auc': 0.8851307264770006,\n",
       "   'threshold': 0.5581990060263177,\n",
       "   'fold': 3},\n",
       "  {'precision': 0.850049817336433,\n",
       "   'recall': 0.8206155819172812,\n",
       "   'f1': 0.835073409461664,\n",
       "   'confusion_matrix': [[223780, 1806], [2238, 10238]],\n",
       "   'fpr': 0.008005815963756616,\n",
       "   'pr_auc': 0.895816551286007,\n",
       "   'threshold': 0.571306364114754,\n",
       "   'fold': 4},\n",
       "  {'precision': 0.8500127975428717,\n",
       "   'recall': 0.8030791552474609,\n",
       "   'f1': 0.8258797198159739,\n",
       "   'confusion_matrix': [[222560, 1758], [2443, 9963]],\n",
       "   'fpr': 0.007837088419119285,\n",
       "   'pr_auc': 0.8906386818540093,\n",
       "   'threshold': 0.5892073028139437,\n",
       "   'fold': 5}],\n",
       " 'test_metrics': None,\n",
       " 'feature_names': ['smart_187_raw',\n",
       "  'smart_188_raw',\n",
       "  'smart_197_raw',\n",
       "  'smart_198_raw',\n",
       "  'smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'delta_smart_188_raw',\n",
       "  'max_smart_188_raw',\n",
       "  'delta_smart_197_raw',\n",
       "  'max_smart_197_raw',\n",
       "  'delta_smart_198_raw',\n",
       "  'max_smart_198_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week',\n",
       "  'model_code',\n",
       "  'vendor_code'],\n",
       " 'smart_cols': ['smart_187_raw',\n",
       "  'smart_188_raw',\n",
       "  'smart_197_raw',\n",
       "  'smart_198_raw',\n",
       "  'smart_5_raw']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hdd_precision_v5s4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310d268-a44c-4da8-b812-14c7091e0962",
   "metadata": {},
   "source": [
    "# Demás Técnicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a66e5d-f8cd-457b-8d2d-8aad7d7eea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CPU-ONLY RANDOM FOREST PIPELINE\n",
    "Works without GPU - uses scikit-learn instead of cuML\n",
    "\n",
    "Same functionality, just slower but will actually run!\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, json, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "def load_data(path, years):\n",
    "    print(f\"Loading {years}...\")\n",
    "    chunks = []\n",
    "    if os.path.isdir(path):\n",
    "        files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.parquet')]\n",
    "    else:\n",
    "        files = [path]\n",
    "    \n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for i in range(pf.num_row_groups):\n",
    "            df = pf.read_row_group(i).to_pandas()\n",
    "            if 'date' not in df.columns: continue\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            df = df[df['date'].dt.year.isin(years)]\n",
    "            if len(df) > 0: chunks.append(df)\n",
    "            if len(chunks) >= 20:\n",
    "                chunks = [pd.concat(chunks, ignore_index=True)]\n",
    "                cleanup()\n",
    "    \n",
    "    result = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "    print(f\"Loaded {len(result):,} rows\")\n",
    "    return result\n",
    "\n",
    "# ============== CREATE LABELS ==============\n",
    "def create_labels(df, lookahead=7):\n",
    "    print(f\"Creating labels (lookahead={lookahead})...\")\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    fail_map = {}\n",
    "    fails = df[df['failure'] == 1]\n",
    "    for sn, dt in zip(fails['serial_number'], fails['date']):\n",
    "        sn = str(sn)\n",
    "        if sn not in fail_map:\n",
    "            fail_map[sn] = dt\n",
    "        else:\n",
    "            fail_map[sn] = min(fail_map[sn], dt)\n",
    "    \n",
    "    y = np.zeros(len(df), dtype=np.int8)\n",
    "    for i, (sn, dt) in enumerate(zip(df['serial_number'].astype(str), df['date'])):\n",
    "        if sn in fail_map:\n",
    "            days = (fail_map[sn] - dt).days\n",
    "            if 0 <= days <= lookahead:\n",
    "                y[i] = 1\n",
    "    \n",
    "    print(f\"Labels: {y.sum():,} positive ({100*y.sum()/len(y):.4f}%)\")\n",
    "    return y\n",
    "\n",
    "# ============== FEATURES ==============\n",
    "def create_features(df, dataset_type):\n",
    "    \"\"\"Uses ALL available SMART attributes from the data.\"\"\"\n",
    "    print(f\"Creating features for {dataset_type}...\")\n",
    "    df = df.sort_values(['serial_number', 'date']).copy()\n",
    "    \n",
    "    # Auto-detect SMART attributes\n",
    "    all_cols = df.columns.tolist()\n",
    "    smart_cols = [c for c in all_cols if 'smart' in c.lower() and '_raw' in c.lower()]\n",
    "    \n",
    "    print(f\"  Found {len(smart_cols)} SMART attributes\")\n",
    "    \n",
    "    # Ensure numeric\n",
    "    for c in smart_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Delta features\n",
    "    for c in smart_cols:\n",
    "        df[f'delta_{c}'] = df.groupby('serial_number')[c].diff().fillna(0)\n",
    "    \n",
    "    # Cumulative max\n",
    "    for c in smart_cols:\n",
    "        df[f'max_{c}'] = df.groupby('serial_number')[c].cummax()\n",
    "    \n",
    "    # Age\n",
    "    df['age_days'] = df.groupby('serial_number').cumcount()\n",
    "    \n",
    "    # Temporal\n",
    "    df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "    df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek\n",
    "    \n",
    "    # Prepare\n",
    "    drop_cols = ['serial_number', 'date', 'failure', 'model']\n",
    "    X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == 'object':\n",
    "            X[c] = pd.Categorical(X[c]).codes\n",
    "        X[c] = pd.to_numeric(X[c], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Remove zero-variance\n",
    "    var = X.var()\n",
    "    keep_cols = var[var > 0].index.tolist()\n",
    "    X = X[keep_cols]\n",
    "    \n",
    "    print(f\"  Final features: {len(X.columns)}\")\n",
    "    \n",
    "    return X.astype(np.float32)\n",
    "\n",
    "# ============== BALANCING ==============\n",
    "def apply_balance(X, y, strategy, target=50000, seed=42):\n",
    "    n_pos = int(y.sum())\n",
    "    n_neg = len(y) - n_pos\n",
    "    print(f\"\\n{strategy.upper()} balancing:\")\n",
    "    print(f\"  Before: {len(y):,} samples ({n_pos:,} pos, {n_neg:,} neg)\")\n",
    "    \n",
    "    if strategy == 'none':\n",
    "        print(f\"  After: {len(y):,} samples (no resampling)\")\n",
    "        return X, y\n",
    "    \n",
    "    elif strategy == 'under':\n",
    "        n_per_class = min(target // 2, n_pos, n_neg)\n",
    "        rus = RandomUnderSampler(\n",
    "            sampling_strategy={0: n_per_class, 1: n_per_class},\n",
    "            random_state=seed\n",
    "        )\n",
    "        X_res, y_res = rus.fit_resample(X, y)\n",
    "        print(f\"  After: {len(y_res):,} samples ({y_res.sum():,} pos)\")\n",
    "        return X_res, y_res\n",
    "    \n",
    "    elif strategy == 'smote':\n",
    "        if n_pos < 6:\n",
    "            print(\"  Not enough positives for SMOTE, using under\")\n",
    "            return apply_balance(X, y, 'under', target, seed)\n",
    "        \n",
    "        max_neg = min(n_pos * 20, n_neg)\n",
    "        if max_neg < n_neg:\n",
    "            rus = RandomUnderSampler(\n",
    "                sampling_strategy={0: max_neg, 1: n_pos},\n",
    "                random_state=seed\n",
    "            )\n",
    "            X, y = rus.fit_resample(X, y)\n",
    "            n_neg = max_neg\n",
    "        \n",
    "        target_pos = min(n_neg // 2, target // 2, n_pos * 3)\n",
    "        k = min(5, n_pos - 1)\n",
    "        \n",
    "        sm = SMOTE(\n",
    "            sampling_strategy={1: target_pos},\n",
    "            k_neighbors=k,\n",
    "            random_state=seed\n",
    "        )\n",
    "        X_res, y_res = sm.fit_resample(X, y)\n",
    "        \n",
    "        if len(X_res) > target:\n",
    "            idx = np.random.choice(len(X_res), target, replace=False)\n",
    "            X_res, y_res = X_res.iloc[idx], y_res[idx]\n",
    "        \n",
    "        print(f\"  After: {len(y_res):,} samples ({y_res.sum():,} pos)\")\n",
    "        return X_res, y_res\n",
    "    \n",
    "    elif strategy == 'smote_enn':\n",
    "        if n_pos < 6:\n",
    "            print(\"  Not enough positives for SMOTE-ENN, using under\")\n",
    "            return apply_balance(X, y, 'under', target, seed)\n",
    "        \n",
    "        max_neg = min(n_pos * 20, n_neg)\n",
    "        if max_neg < n_neg:\n",
    "            rus = RandomUnderSampler(\n",
    "                sampling_strategy={0: max_neg, 1: n_pos},\n",
    "                random_state=seed\n",
    "            )\n",
    "            X, y = rus.fit_resample(X, y)\n",
    "        \n",
    "        print(\"  Applying SMOTE-ENN (slow)...\")\n",
    "        smenn = SMOTEENN(random_state=seed)\n",
    "        X_res, y_res = smenn.fit_resample(X, y)\n",
    "        \n",
    "        if len(X_res) > target:\n",
    "            idx = np.random.choice(len(X_res), target, replace=False)\n",
    "            X_res, y_res = X_res.iloc[idx], y_res[idx]\n",
    "        \n",
    "        print(f\"  After: {len(y_res):,} samples ({y_res.sum():,} pos)\")\n",
    "        return X_res, y_res\n",
    "    \n",
    "    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "# ============== TRAIN ==============\n",
    "def train_rf(X, y, use_class_weights=True, n_estimators=300, max_depth=20, random_state=42):\n",
    "    \"\"\"Train Random Forest on CPU using scikit-learn\"\"\"\n",
    "    print(f\"\\nTraining Random Forest (CPU):\")\n",
    "    print(f\"  Samples: {len(y):,} ({y.sum():,} pos)\")\n",
    "    print(f\"  Trees: {n_estimators}, Depth: {max_depth}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weight = None\n",
    "    if use_class_weights and y.sum() < len(y) // 2:\n",
    "        n_samples = len(y)\n",
    "        n_pos = y.sum()\n",
    "        n_neg = n_samples - n_pos\n",
    "        \n",
    "        w_pos = n_samples / (2 * n_pos) if n_pos > 0 else 1.0\n",
    "        w_neg = n_samples / (2 * n_neg) if n_neg > 0 else 1.0\n",
    "        \n",
    "        class_weight = {0: w_neg, 1: w_pos}\n",
    "        print(f\"  Class weights: pos={w_pos:.1f}, neg={w_neg:.1f}\")\n",
    "    \n",
    "    # Train\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        max_features=0.5,\n",
    "        class_weight=class_weight,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rf.fit(X, y)\n",
    "    print(\"  Training complete!\")\n",
    "    \n",
    "    return rf\n",
    "\n",
    "# ============== EVALUATE ==============\n",
    "def evaluate(model, X, y, feature_names, min_precision=0.60, min_recall=0.30):\n",
    "    print(f\"\\nEvaluating:\")\n",
    "    print(f\"  Samples: {len(y):,} ({y.sum():,} pos)\")\n",
    "    \n",
    "    # Ensure same features\n",
    "    X = X[feature_names]\n",
    "    \n",
    "    # Predict\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # PR curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y, proba)\n",
    "    pr_auc = average_precision_score(y, proba)\n",
    "    \n",
    "    print(f\"  PR-AUC: {pr_auc:.6f}\")\n",
    "    \n",
    "    # Find threshold\n",
    "    beta = 2.0\n",
    "    f_beta = ((1 + beta**2) * precision * recall) / (beta**2 * precision + recall + 1e-10)\n",
    "    \n",
    "    valid_mask = (precision >= min_precision) & (recall >= min_recall)\n",
    "    \n",
    "    if valid_mask.any():\n",
    "        idx = np.argmax(f_beta * valid_mask)\n",
    "        threshold = thresholds[idx]\n",
    "        print(f\"  Optimal threshold: {threshold:.4f}\")\n",
    "        print(f\"    Precision: {precision[idx]:.4f}\")\n",
    "        print(f\"    Recall: {recall[idx]:.4f}\")\n",
    "    else:\n",
    "        idx = np.argmax(f_beta)\n",
    "        threshold = thresholds[idx]\n",
    "        print(f\"  ⚠️  No threshold meets constraints\")\n",
    "        print(f\"  Best F2 threshold: {threshold:.4f}\")\n",
    "        print(f\"    Precision: {precision[idx]:.4f}\")\n",
    "        print(f\"    Recall: {recall[idx]:.4f}\")\n",
    "    \n",
    "    # Metrics\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    \n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"    TN: {tn:,}  |  FP: {fp:,}\")\n",
    "    print(f\"    FN: {fn:,}  |  TP: {tp:,}\")\n",
    "    print(f\"  FPR: {fp/(fp+tn):.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'pr_auc': float(pr_auc),\n",
    "        'threshold': float(threshold),\n",
    "        'precision': float(precision_score(y, y_pred, zero_division=0)),\n",
    "        'recall': float(recall_score(y, y_pred, zero_division=0)),\n",
    "        'f1': float(f1_score(y, y_pred, zero_division=0)),\n",
    "        'confusion_matrix': [[int(tn), int(fp)], [int(fn), int(tp)]],\n",
    "        'fpr': float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "# ============== MAIN PIPELINE ==============\n",
    "def train_random_forest_pipeline(\n",
    "    train_parquet,\n",
    "    test_parquet,\n",
    "    dataset_type,\n",
    "    balancing_strategy='none',\n",
    "    train_years=[2020, 2021, 2022, 2023],\n",
    "    test_years=[2024],\n",
    "    lookahead_days=7,\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    target_samples=50000,\n",
    "    min_precision=0.60,\n",
    "    min_recall=0.30,\n",
    "    output_dir='./models_rf',\n",
    "    random_state=42\n",
    "):\n",
    "    print(\"=\"*70)\n",
    "    print(f\"RANDOM FOREST PIPELINE (CPU) - {dataset_type.upper()}\")\n",
    "    print(f\"Strategy: {balancing_strategy.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load\n",
    "    df_train = load_data(train_parquet, train_years)\n",
    "    df_test = load_data(test_parquet, test_years) if test_parquet else pd.DataFrame()\n",
    "    \n",
    "    if df_train.empty:\n",
    "        raise ValueError(\"Training data is empty!\")\n",
    "    \n",
    "    # Labels\n",
    "    y_train = create_labels(df_train, lookahead_days)\n",
    "    y_test = create_labels(df_test, lookahead_days) if not df_test.empty else np.array([])\n",
    "    \n",
    "    if y_train.sum() < 50:\n",
    "        raise ValueError(f\"Insufficient positive samples: {y_train.sum()}\")\n",
    "    \n",
    "    # Features\n",
    "    X_train = create_features(df_train, dataset_type)\n",
    "    X_test = create_features(df_test, dataset_type) if not df_test.empty else pd.DataFrame()\n",
    "    \n",
    "    feature_names = list(X_train.columns)\n",
    "    print(f\"\\nTotal features: {len(feature_names)}\")\n",
    "    \n",
    "    if not X_test.empty:\n",
    "        for col in feature_names:\n",
    "            if col not in X_test.columns:\n",
    "                X_test[col] = 0\n",
    "        X_test = X_test[feature_names]\n",
    "    \n",
    "    del df_train, df_test\n",
    "    cleanup()\n",
    "    \n",
    "    # Split\n",
    "    n_val = int(0.2 * len(X_train))\n",
    "    X_val = X_train.iloc[-n_val:].reset_index(drop=True)\n",
    "    y_val = y_train[-n_val:]\n",
    "    \n",
    "    X_train = X_train.iloc[:-n_val].reset_index(drop=True)\n",
    "    y_train = y_train[:-n_val]\n",
    "    \n",
    "    print(f\"\\nData splits:\")\n",
    "    print(f\"  Training:   {len(y_train):,} ({y_train.sum():,} pos)\")\n",
    "    print(f\"  Validation: {len(y_val):,} ({y_val.sum():,} pos)\")\n",
    "    if len(y_test) > 0:\n",
    "        print(f\"  Test:       {len(y_test):,} ({y_test.sum():,} pos)\")\n",
    "    \n",
    "    # Balance\n",
    "    X_train_bal, y_train_bal = apply_balance(\n",
    "        X_train, y_train,\n",
    "        strategy=balancing_strategy,\n",
    "        target=target_samples,\n",
    "        seed=random_state\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    use_weights = (balancing_strategy == 'none')\n",
    "    model = train_rf(\n",
    "        X_train_bal, y_train_bal,\n",
    "        use_class_weights=use_weights,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    del X_train, y_train, X_train_bal, y_train_bal\n",
    "    cleanup()\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, X_val, y_val, feature_names, min_precision, min_recall)\n",
    "    \n",
    "    del X_val, y_val\n",
    "    cleanup()\n",
    "    \n",
    "    test_metrics = None\n",
    "    if not X_test.empty:\n",
    "        test_metrics = evaluate(model, X_test, y_test, feature_names, min_precision, min_recall)\n",
    "        del X_test, y_test\n",
    "        cleanup()\n",
    "    \n",
    "    # Save\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_prefix = os.path.join(output_dir, f\"{dataset_type}_{balancing_strategy}_{timestamp}\")\n",
    "    \n",
    "    joblib.dump(model, f\"{model_prefix}_model.pkl\")\n",
    "    print(f\"\\n✓ Model saved: {model_prefix}_model.pkl\")\n",
    "    \n",
    "    metadata = {\n",
    "        'dataset_type': dataset_type,\n",
    "        'balancing_strategy': balancing_strategy,\n",
    "        'train_years': train_years,\n",
    "        'test_years': test_years,\n",
    "        'lookahead_days': lookahead_days,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'feature_names': feature_names,\n",
    "        'validation_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics\n",
    "    }\n",
    "    \n",
    "    with open(f\"{model_prefix}_metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Metadata saved\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "# ============== INDIVIDUAL FUNCTIONS ==============\n",
    "\n",
    "# HDD\n",
    "def train_hdd_none():\n",
    "    return train_random_forest_pipeline(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        dataset_type='HDD',\n",
    "        balancing_strategy='none',\n",
    "        lookahead_days=7,\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        target_samples=50000,\n",
    "        output_dir='./models_rf_hdd_none'\n",
    "    )\n",
    "\n",
    "def train_hdd_under():\n",
    "    return train_random_forest_pipeline(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        dataset_type='HDD',\n",
    "        balancing_strategy='under',\n",
    "        lookahead_days=7,\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        target_samples=50000,\n",
    "        output_dir='./models_rf_hdd_under'\n",
    "    )\n",
    "\n",
    "def train_hdd_smote():\n",
    "    return train_random_forest_pipeline(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        dataset_type='HDD',\n",
    "        balancing_strategy='smote',\n",
    "        lookahead_days=7,\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        target_samples=50000,\n",
    "        output_dir='./models_rf_hdd_smote'\n",
    "    )\n",
    "\n",
    "def train_hdd_smote_enn():\n",
    "    return train_random_forest_pipeline(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        dataset_type='HDD',\n",
    "        balancing_strategy='smote_enn',\n",
    "        lookahead_days=7,\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        target_samples=50000,\n",
    "        output_dir='./models_rf_hdd_smote_enn'\n",
    "    )\n",
    "\n",
    "# SSD\n",
    "def train_ssd_none():\n",
    "    return train_random_forest_pipeline(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        balancing_strategy='none',\n",
    "        lookahead_days=7,\n",
    "        n_estimators=400,\n",
    "        max_depth=25,\n",
    "        target_samples=30000,\n",
    "        min_precision=0.50,\n",
    "        min_recall=0.25,\n",
    "        output_dir='./models_rf_ssd_none'\n",
    "    )\n",
    "\n",
    "def train_ssd_under():\n",
    "    return train_random_forest_pipeline(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        balancing_strategy='under',\n",
    "        lookahead_days=7,\n",
    "        n_estimators=400,\n",
    "        max_depth=25,\n",
    "        target_samples=30000,\n",
    "        min_precision=0.50,\n",
    "        min_recall=0.25,\n",
    "        output_dir='./models_rf_ssd_under'\n",
    "    )\n",
    "\n",
    "def train_ssd_smote():\n",
    "    return train_random_forest_pipeline(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        balancing_strategy='smote',\n",
    "        lookahead_days=7,\n",
    "        n_estimators=400,\n",
    "        max_depth=25,\n",
    "        target_samples=30000,\n",
    "        min_precision=0.50,\n",
    "        min_recall=0.25,\n",
    "        output_dir='./models_rf_ssd_smote'\n",
    "    )\n",
    "\n",
    "def train_ssd_smote_enn():\n",
    "    return train_random_forest_pipeline(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        balancing_strategy='smote_enn',\n",
    "        lookahead_days=7,\n",
    "        n_estimators=400,\n",
    "        max_depth=25,\n",
    "        target_samples=30000,\n",
    "        min_precision=0.50,\n",
    "        min_recall=0.25,\n",
    "        output_dir='./models_rf_ssd_smote_enn'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56590019-ecc4-49be-b868-156a57ff2790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANDOM FOREST PIPELINE (CPU) - SSD\n",
      "Strategy: NONE\n",
      "======================================================================\n",
      "Loading [2020, 2021, 2022, 2023]...\n",
      "Loaded 2,124,111 rows\n",
      "Loading [2024]...\n",
      "Loaded 1,220,745 rows\n",
      "Creating labels (lookahead=7)...\n",
      "Labels: 1,325 positive (0.0624%)\n",
      "Creating labels (lookahead=7)...\n",
      "Labels: 227 positive (0.0186%)\n",
      "Creating features for SSD...\n",
      "  Found 13 SMART attributes\n",
      "  Final features: 40\n",
      "Creating features for SSD...\n",
      "  Found 13 SMART attributes\n",
      "  Final features: 42\n",
      "\n",
      "Total features: 40\n",
      "\n",
      "Data splits:\n",
      "  Training:   1,699,289 (1,240 pos)\n",
      "  Validation: 424,822 (85 pos)\n",
      "  Test:       1,220,745 (227 pos)\n",
      "\n",
      "NONE balancing:\n",
      "  Before: 1,699,289 samples (1,240 pos, 1,698,049 neg)\n",
      "  After: 1,699,289 samples (no resampling)\n",
      "\n",
      "Training Random Forest (CPU):\n",
      "  Samples: 1,699,289 (1,240 pos)\n",
      "  Trees: 400, Depth: 25\n",
      "  Class weights: pos=685.2, neg=0.5\n",
      "  Training complete!\n",
      "\n",
      "Evaluating:\n",
      "  Samples: 424,822 (85 pos)\n",
      "  PR-AUC: 0.000251\n",
      "  ⚠️  No threshold meets constraints\n",
      "  Best F2 threshold: 0.2239\n",
      "    Precision: 0.0005\n",
      "    Recall: 0.0235\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN: 421,046  |  FP: 3,691\n",
      "    FN: 83  |  TP: 2\n",
      "  FPR: 0.008690\n",
      "\n",
      "Evaluating:\n",
      "  Samples: 1,220,745 (227 pos)\n",
      "  PR-AUC: 0.000202\n",
      "  ⚠️  No threshold meets constraints\n",
      "  Best F2 threshold: 0.0528\n",
      "    Precision: 0.0009\n",
      "    Recall: 0.0132\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN: 1,217,205  |  FP: 3,313\n",
      "    FN: 224  |  TP: 3\n",
      "  FPR: 0.002714\n",
      "\n",
      "✓ Model saved: ./models_rf_ssd_none/SSD_none_20251104_162405_model.pkl\n",
      "✓ Metadata saved\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'SSD',\n",
       " 'balancing_strategy': 'none',\n",
       " 'train_years': [2020, 2021, 2022, 2023],\n",
       " 'test_years': [2024],\n",
       " 'lookahead_days': 7,\n",
       " 'n_estimators': 400,\n",
       " 'max_depth': 25,\n",
       " 'feature_names': ['capacity_bytes',\n",
       "  'smart_5_raw',\n",
       "  'smart_187_raw',\n",
       "  'smart_194_raw',\n",
       "  'smart_231_raw',\n",
       "  'smart_233_raw',\n",
       "  'smart_241_raw',\n",
       "  'smart_242_raw',\n",
       "  'smart_9_raw',\n",
       "  'smart_173_raw',\n",
       "  'smart_174_raw',\n",
       "  'smart_199_raw',\n",
       "  'smart_230_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'delta_smart_194_raw',\n",
       "  'delta_smart_231_raw',\n",
       "  'delta_smart_233_raw',\n",
       "  'delta_smart_241_raw',\n",
       "  'delta_smart_242_raw',\n",
       "  'delta_smart_9_raw',\n",
       "  'delta_smart_173_raw',\n",
       "  'delta_smart_174_raw',\n",
       "  'delta_smart_199_raw',\n",
       "  'delta_smart_230_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'max_smart_194_raw',\n",
       "  'max_smart_231_raw',\n",
       "  'max_smart_233_raw',\n",
       "  'max_smart_241_raw',\n",
       "  'max_smart_242_raw',\n",
       "  'max_smart_9_raw',\n",
       "  'max_smart_173_raw',\n",
       "  'max_smart_174_raw',\n",
       "  'max_smart_199_raw',\n",
       "  'max_smart_230_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week'],\n",
       " 'validation_metrics': {'pr_auc': 0.00025096806924422544,\n",
       "  'threshold': 0.2238952951136584,\n",
       "  'precision': 0.0005415651232060655,\n",
       "  'recall': 0.023529411764705882,\n",
       "  'f1': 0.0010587612493382743,\n",
       "  'confusion_matrix': [[421046, 3691], [83, 2]],\n",
       "  'fpr': 0.008690083510501793},\n",
       " 'test_metrics': {'pr_auc': 0.0002019739755410303,\n",
       "  'threshold': 0.05280583904904236,\n",
       "  'precision': 0.0009047044632086852,\n",
       "  'recall': 0.013215859030837005,\n",
       "  'f1': 0.001693480101608806,\n",
       "  'confusion_matrix': [[1217205, 3313], [224, 3]],\n",
       "  'fpr': 0.0027144212539266114}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_none()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a115ba2d-38af-4075-b158-f40227843815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANDOM FOREST PIPELINE (CPU) - SSD\n",
      "Strategy: SMOTE\n",
      "======================================================================\n",
      "Loading [2020, 2021, 2022, 2023]...\n",
      "Loaded 2,124,111 rows\n",
      "Loading [2024]...\n",
      "Loaded 1,220,745 rows\n",
      "Creating labels (lookahead=7)...\n",
      "Labels: 1,325 positive (0.0624%)\n",
      "Creating labels (lookahead=7)...\n",
      "Labels: 227 positive (0.0186%)\n",
      "Creating features for SSD...\n",
      "  Found 13 SMART attributes\n",
      "  Final features: 40\n",
      "Creating features for SSD...\n",
      "  Found 13 SMART attributes\n",
      "  Final features: 42\n",
      "\n",
      "Total features: 40\n",
      "\n",
      "Data splits:\n",
      "  Training:   1,699,289 (1,240 pos)\n",
      "  Validation: 424,822 (85 pos)\n",
      "  Test:       1,220,745 (227 pos)\n",
      "\n",
      "SMOTE balancing:\n",
      "  Before: 1,699,289 samples (1,240 pos, 1,698,049 neg)\n",
      "  After: 28,520 samples (3,720 pos)\n",
      "\n",
      "Training Random Forest (CPU):\n",
      "  Samples: 28,520 (3,720 pos)\n",
      "  Trees: 400, Depth: 25\n",
      "  Training complete!\n",
      "\n",
      "Evaluating:\n",
      "  Samples: 424,822 (85 pos)\n",
      "  PR-AUC: 0.000239\n",
      "  ⚠️  No threshold meets constraints\n",
      "  Best F2 threshold: 0.3586\n",
      "    Precision: 0.0005\n",
      "    Recall: 0.0118\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN: 422,864  |  FP: 1,873\n",
      "    FN: 84  |  TP: 1\n",
      "  FPR: 0.004410\n",
      "\n",
      "Evaluating:\n",
      "  Samples: 1,220,745 (227 pos)\n",
      "  PR-AUC: 0.000195\n",
      "  ⚠️  No threshold meets constraints\n",
      "  Best F2 threshold: 0.6025\n",
      "    Precision: 0.0007\n",
      "    Recall: 0.0044\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN: 1,219,014  |  FP: 1,504\n",
      "    FN: 226  |  TP: 1\n",
      "  FPR: 0.001232\n",
      "\n",
      "✓ Model saved: ./models_rf_ssd_smote/SSD_smote_20251104_155827_model.pkl\n",
      "✓ Metadata saved\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'SSD',\n",
       " 'balancing_strategy': 'smote',\n",
       " 'train_years': [2020, 2021, 2022, 2023],\n",
       " 'test_years': [2024],\n",
       " 'lookahead_days': 7,\n",
       " 'n_estimators': 400,\n",
       " 'max_depth': 25,\n",
       " 'feature_names': ['capacity_bytes',\n",
       "  'smart_5_raw',\n",
       "  'smart_187_raw',\n",
       "  'smart_194_raw',\n",
       "  'smart_231_raw',\n",
       "  'smart_233_raw',\n",
       "  'smart_241_raw',\n",
       "  'smart_242_raw',\n",
       "  'smart_9_raw',\n",
       "  'smart_173_raw',\n",
       "  'smart_174_raw',\n",
       "  'smart_199_raw',\n",
       "  'smart_230_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'delta_smart_194_raw',\n",
       "  'delta_smart_231_raw',\n",
       "  'delta_smart_233_raw',\n",
       "  'delta_smart_241_raw',\n",
       "  'delta_smart_242_raw',\n",
       "  'delta_smart_9_raw',\n",
       "  'delta_smart_173_raw',\n",
       "  'delta_smart_174_raw',\n",
       "  'delta_smart_199_raw',\n",
       "  'delta_smart_230_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'max_smart_194_raw',\n",
       "  'max_smart_231_raw',\n",
       "  'max_smart_233_raw',\n",
       "  'max_smart_241_raw',\n",
       "  'max_smart_242_raw',\n",
       "  'max_smart_9_raw',\n",
       "  'max_smart_173_raw',\n",
       "  'max_smart_174_raw',\n",
       "  'max_smart_199_raw',\n",
       "  'max_smart_230_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week'],\n",
       " 'validation_metrics': {'pr_auc': 0.0002393888200639602,\n",
       "  'threshold': 0.3586420870430566,\n",
       "  'precision': 0.0005336179295624333,\n",
       "  'recall': 0.011764705882352941,\n",
       "  'f1': 0.0010209290454313426,\n",
       "  'confusion_matrix': [[422864, 1873], [84, 1]],\n",
       "  'fpr': 0.004409787703920308},\n",
       " 'test_metrics': {'pr_auc': 0.0001949229821723246,\n",
       "  'threshold': 0.6025,\n",
       "  'precision': 0.000664451827242525,\n",
       "  'recall': 0.004405286343612335,\n",
       "  'f1': 0.0011547344110854503,\n",
       "  'confusion_matrix': [[1219014, 1504], [226, 1]],\n",
       "  'fpr': 0.0012322636782087606}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03cdbb9d-6441-4c2f-99f3-20e3e069bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SSD | Balance=under | Lookahead=2d ===\n",
      "RF params: {'n_estimators': 250, 'max_depth': 18, 'max_features': 0.35, 'n_bins': 128, 'random_state': 42, 'n_streams': 1}\n",
      "\n",
      "--- Fold 1 | Train [2020, 2021, 2022] | Val [2023] ---\n",
      "Train: 792 rows (396 pos, 50.00%)\n",
      "Fold 1: PR-AUC≈0.000163 (rows=1,137,101, pos=104)\n",
      "\n",
      "--- Fold 2 | Train [2020, 2023] | Val [2021, 2022] ---\n",
      "Train: 138 rows (69 pos, 50.00%)\n",
      "Fold 2: PR-AUC≈0.000369 (rows=705,084, pos=395)\n",
      "\n",
      "--- Fold 3 | Train [2021, 2022, 2023] | Val [2020] ---\n",
      "Train: 864 rows (432 pos, 50.00%)\n",
      "Fold 3: PR-AUC≈0.000048 (rows=281,926, pos=10)\n",
      "\n",
      "Selected threshold: 0.2320 (precision_target=0.95)\n",
      "\n",
      "Held-out 2024 Test (streamed + persistence):\n",
      "  Precision=0.000074 | Recall=0.781609 | F1=0.000148 | PR-AUC≈0.000123\n",
      "  Persistence: k=2, n=3 | Alerts=921,274\n",
      "  Confusion Matrix: [[TN=299452, FP=921206], [FN=19, TP=68]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv_metrics': [{'fold': 1,\n",
       "   'val_rows': 1137101,\n",
       "   'val_pos': 104,\n",
       "   'pr_auc_approx': 0.0001630692760735205},\n",
       "  {'fold': 2,\n",
       "   'val_rows': 705084,\n",
       "   'val_pos': 395,\n",
       "   'pr_auc_approx': 0.0003685138678518559},\n",
       "  {'fold': 3,\n",
       "   'val_rows': 281926,\n",
       "   'val_pos': 10,\n",
       "   'pr_auc_approx': 4.827346895895134e-05}],\n",
       " 'threshold': 0.231990231990232,\n",
       " 'saved_prefix': './models_rf_temporal_ssd/SSD_RFgpu_temporal_under_L2_20251031_165658',\n",
       " 'test_metrics': {'precision': 7.381083152243523e-05,\n",
       "  'recall': 0.7816091954022989,\n",
       "  'f1': 0.0001476077237910287,\n",
       "  'pr_auc_approx': 0.00012286151845665422,\n",
       "  'alerts': 921274,\n",
       "  'positives_rows': 87,\n",
       "  'confusion_matrix': [[299452, 921206], [19, 68]],\n",
       "  'persistence_k': 2,\n",
       "  'persistence_n': 3}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_under()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8862aecb-3da8-4289-9195-b34cd31c3b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SSD | Balance=smote_enn | Lookahead=2d ===\n",
      "RF params: {'n_estimators': 250, 'max_depth': 18, 'max_features': 0.35, 'n_bins': 128, 'random_state': 42, 'n_streams': 1}\n",
      "\n",
      "--- Fold 1 | Train [2020, 2021, 2022] | Val [2023] ---\n",
      "Train: 10,000 rows (5,000 pos, 50.00%)\n",
      "Fold 1: PR-AUC≈0.000135 (rows=1,137,101, pos=104)\n",
      "\n",
      "--- Fold 2 | Train [2020, 2023] | Val [2021, 2022] ---\n",
      "Train: 2,510 rows (1,255 pos, 50.00%)\n",
      "Fold 2: PR-AUC≈0.000647 (rows=705,084, pos=395)\n",
      "\n",
      "--- Fold 3 | Train [2021, 2022, 2023] | Val [2020] ---\n",
      "Train: 10,000 rows (5,000 pos, 50.00%)\n",
      "Fold 3: PR-AUC≈0.000046 (rows=281,926, pos=10)\n",
      "\n",
      "Selected threshold: 0.2598 (precision_target=0.95)\n",
      "\n",
      "Held-out 2024 Test (streamed + persistence):\n",
      "  Precision=0.000076 | Recall=0.701149 | F1=0.000151 | PR-AUC≈0.000106\n",
      "  Persistence: k=2, n=3 | Alerts=805,366\n",
      "  Confusion Matrix: [[TN=415353, FP=805305], [FN=26, TP=61]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv_metrics': [{'fold': 1,\n",
       "   'val_rows': 1137101,\n",
       "   'val_pos': 104,\n",
       "   'pr_auc_approx': 0.00013548275867919293},\n",
       "  {'fold': 2,\n",
       "   'val_rows': 705084,\n",
       "   'val_pos': 395,\n",
       "   'pr_auc_approx': 0.0006473176056100102},\n",
       "  {'fold': 3,\n",
       "   'val_rows': 281926,\n",
       "   'val_pos': 10,\n",
       "   'pr_auc_approx': 4.6483844137817164e-05}],\n",
       " 'threshold': 0.25982905982905985,\n",
       " 'saved_prefix': './models_rf_temporal_ssd/SSD_RFgpu_temporal_smote_enn_L2_20251031_200543',\n",
       " 'test_metrics': {'precision': 7.574196079794776e-05,\n",
       "  'recall': 0.7011494252873564,\n",
       "  'f1': 0.00015146755924905117,\n",
       "  'pr_auc_approx': 0.00010550914989970685,\n",
       "  'alerts': 805366,\n",
       "  'positives_rows': 87,\n",
       "  'confusion_matrix': [[415353, 805305], [26, 61]],\n",
       "  'persistence_k': 2,\n",
       "  'persistence_n': 3}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_smote_enn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e017c27-dbba-4132-aa20-0106c50463a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HDD | Balance=none | Lookahead=8d ===\n",
      "RF params: {'n_estimators': 320, 'max_depth': 18, 'max_features': 0.4, 'n_bins': 128, 'random_state': 42, 'n_streams': 1}\n",
      "\n",
      "--- Fold 1 | Train [2020, 2021, 2022] | Val [2023] ---\n",
      "Train: 112,519 rows (32,519 pos, 28.90%)\n"
     ]
    }
   ],
   "source": [
    "train_hdd_none()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
