{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b88f5b-b76a-4b36-8d56-e491c98d9b87",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb10597-67fd-450a-b60d-3fc1bdc1d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# =====================================================================\n",
    "# XGBOOST — PRECISION-CONTROLLED v72s4 (GPU) con BIG-MODE (TRAIN/TEST)\n",
    "#   • TRAIN big: streaming + filtro (positivos, cercanos, lejanos muestreados)\n",
    "#   • TEST 2024 streaming: sin rolling (delta, cummax, age_days)\n",
    "#   • SSD (RAM): rolling + normalización por modelo + categóricas nativas\n",
    "#   • HDD (BIG): sin rolling + categóricos mapeados a códigos estables\n",
    "# =====================================================================\n",
    "\n",
    "import os, gc, json, warnings, re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "# ===================== UTILS =====================\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "\n",
    "def downcast_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        elif pd.api.types.is_integer_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================== IO Helpers (streaming parquet) =====================\n",
    "\n",
    "def _list_parquet_files(path: str) -> List[str]:\n",
    "    if os.path.isdir(path):\n",
    "        return [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".parquet\")]\n",
    "    return [path]\n",
    "\n",
    "def _peek_row_group(path: str) -> pd.DataFrame:\n",
    "    files = _list_parquet_files(path)\n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        if pf.num_row_groups > 0:\n",
    "            tb = pf.read_row_group(0, columns=None)\n",
    "            return tb.to_pandas().head(100)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def discover_smart_columns(path: str) -> List[str]:\n",
    "    peek = _peek_row_group(path)\n",
    "    if peek.empty:\n",
    "        return []\n",
    "    cols = peek.columns.tolist()\n",
    "    return sorted([c for c in cols if (\"smart\" in c.lower()) and c.endswith(\"_raw\")])\n",
    "\n",
    "def _iter_parquet_row_groups(path: str, years: List[int], columns: Optional[List[str]] = None):\n",
    "    files = _list_parquet_files(path)\n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for i in range(pf.num_row_groups):\n",
    "            tb = pf.read_row_group(i, columns=columns) if columns else pf.read_row_group(i)\n",
    "            df = tb.to_pandas()\n",
    "            if \"date\" not in df.columns:\n",
    "                continue\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            mask = df[\"date\"].dt.year.isin(years)\n",
    "            if mask.any():\n",
    "                yield downcast_df(df.loc[mask].reset_index(drop=True))\n",
    "            del df, tb\n",
    "            cleanup()\n",
    "\n",
    "def load_data(path: str, years: List[int], columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    print(f\"Loading {years} from {path}...\")\n",
    "    chunks = []\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=columns):\n",
    "        chunks.append(df)\n",
    "        if len(chunks) >= 16:\n",
    "            chunks = [pd.concat(chunks, ignore_index=True)]\n",
    "            cleanup()\n",
    "    out = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "    print(f\"Loaded {len(out):,} rows\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== BIG MODE (TRAIN y TEST) =====================\n",
    "\n",
    "def scan_fail_dates(path: str, years: List[int]) -> Dict[str, pd.Timestamp]:\n",
    "    \"\"\"Primera pasada: PRIMERA fecha de fallo por serial (streaming).\"\"\"\n",
    "    print(\"Scanning earliest failure dates (streaming)...\")\n",
    "    fail_map: Dict[str, pd.Timestamp] = {}\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=[\"serial_number\", \"date\", \"failure\"]):\n",
    "        df = df[df[\"failure\"] == 1]\n",
    "        if df.empty:\n",
    "            continue\n",
    "        sns = df[\"serial_number\"].astype(str).values\n",
    "        dts = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        for sn, dt in zip(sns, dts):\n",
    "            if pd.isna(dt):\n",
    "                continue\n",
    "            if sn not in fail_map or dt < fail_map[sn]:\n",
    "                fail_map[sn] = dt\n",
    "    print(f\"  Found {len(fail_map):,} failed serials\")\n",
    "    return fail_map\n",
    "\n",
    "\n",
    "def load_data_big_filtered(\n",
    "    path: str,\n",
    "    years: List[int],\n",
    "    fail_map: Dict[str, pd.Timestamp],\n",
    "    lookahead_days: int,\n",
    "    hard_window: int,\n",
    "    neg_random_keep_rate: float = 0.0025,\n",
    "    columns: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TRAIN big: conserva todos positivos, negativos cercanos (hard) y\n",
    "    una muestra de negativos lejanos. NaT-safe.\n",
    "    \"\"\"\n",
    "    print(f\"Loading BIG-FILTERED {years} from {path} (keep_rate={neg_random_keep_rate})...\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    kept = []\n",
    "    total_rows = 0\n",
    "    kept_rows = 0\n",
    "\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=columns):\n",
    "        total_rows += len(df)\n",
    "        sn_ser = df[\"serial_number\"].astype(str)\n",
    "        dt = pd.to_datetime(df[\"date\"], errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "\n",
    "        # dtf con fail_map (NaT-safe)\n",
    "        fdt = pd.to_datetime(sn_ser.map(fail_map), errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "        dtf = np.full(len(df), 10**9, dtype=np.int64)\n",
    "        valid = ~np.isnat(fdt)\n",
    "        if valid.any():\n",
    "            dd = (fdt[valid] - dt[valid]).astype(\"timedelta64[D]\").astype(\"int64\")\n",
    "            dtf[valid] = dd\n",
    "\n",
    "        failure = (df[\"failure\"].values == 1)\n",
    "        pos_mask = failure | ((dtf >= 0) & (dtf <= lookahead_days))\n",
    "        near_mask = (dtf > lookahead_days) & (dtf <= hard_window)\n",
    "\n",
    "        keep = pos_mask | near_mask\n",
    "        far_neg = (~keep) & (~failure)\n",
    "        if far_neg.any():\n",
    "            sample = rng.random(far_neg.sum()) < neg_random_keep_rate\n",
    "            sel = np.zeros_like(far_neg, dtype=bool)\n",
    "            sel[np.where(far_neg)[0]] = sample\n",
    "            keep = keep | sel\n",
    "\n",
    "        kept.append(df.loc[keep].reset_index(drop=True))\n",
    "        kept_rows += int(keep.sum())\n",
    "\n",
    "        if sum(len(x) for x in kept) > 3_000_000:\n",
    "            kept = [pd.concat(kept, ignore_index=True)]\n",
    "            cleanup()\n",
    "\n",
    "    out = pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n",
    "    rate = 100 * kept_rows / max(1, total_rows)\n",
    "    print(f\"  BIG-FILTERED kept {kept_rows:,}/{total_rows:,} rows (~{rate:.2f}%)\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== PREP & LABELS =====================\n",
    "\n",
    "def prepare_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"serial_number\"] = df[\"serial_number\"].astype(str)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"serial_number\", \"date\"])\n",
    "    df = df.sort_values([\"serial_number\", \"date\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def compute_days_to_failure(dfs: pd.DataFrame) -> np.ndarray:\n",
    "    fail_map: Dict[str, pd.Timestamp] = {}\n",
    "    fails = dfs[dfs[\"failure\"] == 1]\n",
    "    for sn, dt in zip(fails[\"serial_number\"], fails[\"date\"]):\n",
    "        fail_map[sn] = min(dt, fail_map.get(sn, dt))\n",
    "\n",
    "    dtf = np.full(len(dfs), 10**9, dtype=np.int64)\n",
    "    for i, (sn, dt) in enumerate(zip(dfs[\"serial_number\"], dfs[\"date\"])):\n",
    "        if sn in fail_map:\n",
    "            dtf[i] = (fail_map[sn] - dt).days\n",
    "    return dtf\n",
    "\n",
    "def create_labels_from_dtf(dtf: np.ndarray, lookahead: int = 7) -> np.ndarray:\n",
    "    return ((dtf >= 0) & (dtf <= lookahead)).astype(np.int8)\n",
    "\n",
    "\n",
    "# ===================== CATEGÓRICOS (map a códigos para BIG) =====================\n",
    "\n",
    "def extract_vendor(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    return s.str.extract(r\"^([A-Za-z]+)\", expand=False).fillna(\"UNK\")\n",
    "\n",
    "def fit_category_maps(df: pd.DataFrame) -> Dict[str, Dict[str, int]]:\n",
    "    maps: Dict[str, Dict[str, int]] = {}\n",
    "    if \"model\" in df.columns:\n",
    "        models = pd.Index(df[\"model\"].astype(str).unique())\n",
    "        maps[\"model\"] = {m: i for i, m in enumerate(models)}\n",
    "        vendors = pd.Index(extract_vendor(df[\"model\"]).unique())\n",
    "        maps[\"vendor\"] = {v: i for i, v in enumerate(vendors)}\n",
    "    return maps\n",
    "\n",
    "def apply_category_maps(df: pd.DataFrame, maps: Optional[Dict[str, Dict[str, int]]]) -> Tuple[pd.Series, pd.Series]:\n",
    "    if maps and \"model\" in df.columns and \"model\" in maps and \"vendor\" in maps:\n",
    "        m = df[\"model\"].astype(str)\n",
    "        v = extract_vendor(m)\n",
    "        model_map = maps[\"model\"]; vendor_map = maps[\"vendor\"]\n",
    "        model_codes = m.map(model_map).fillna(-1).astype(int)\n",
    "        vendor_codes = v.map(vendor_map).fillna(-1).astype(int)\n",
    "        return model_codes, vendor_codes\n",
    "    n = len(df)\n",
    "    return pd.Series(np.zeros(n, dtype=np.int32), index=df.index), pd.Series(np.zeros(n, dtype=np.int32), index=df.index)\n",
    "\n",
    "\n",
    "# ===================== FEATURES: RAM (SSD) vs BIG (HDD) =====================\n",
    "\n",
    "def create_features_joined_ram_v72(\n",
    "    df_train: pd.DataFrame, df_dev: pd.DataFrame, df_test: pd.DataFrame,\n",
    "    add_rolling: bool\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    RAM (SSD): TRAIN∪DEV∪TEST → deltas/cummax (+ rolling), age, calendario,\n",
    "    categóricas nativas (model, vendor). Compatible con XGB enable_categorical=True.\n",
    "    \"\"\"\n",
    "    df_train = df_train.copy(); df_train[\"__subset__\"]=\"train\"\n",
    "    df_dev   = df_dev.copy();   df_dev[\"__subset__\"]=\"dev\"\n",
    "    df_test  = df_test.copy();  df_test[\"__subset__\"]=\"test\"\n",
    "    df_all = pd.concat([df_train, df_dev, df_test], ignore_index=True)\n",
    "    df_all.sort_values([\"serial_number\",\"date\"], inplace=True)\n",
    "\n",
    "    all_cols = df_all.columns.tolist()\n",
    "    smart_cols = [c for c in all_cols if (\"smart\" in c.lower()) and (\"_raw\" in c.lower())]\n",
    "    for c in smart_cols:\n",
    "        df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    for c in smart_cols:\n",
    "        g = df_all.groupby(\"serial_number\", sort=False)[c]\n",
    "        df_all[f\"delta_{c}\"] = g.diff().fillna(0.0)\n",
    "        df_all[f\"max_{c}\"]   = g.cummax()\n",
    "\n",
    "    if add_rolling:\n",
    "        for c in smart_cols:\n",
    "            r = df_all.groupby(\"serial_number\", sort=False)[c]\n",
    "            df_all[f\"rmean7_{c}\"] = r.rolling(window=7, min_periods=2).mean().reset_index(level=0, drop=True).fillna(0.0)\n",
    "            df_all[f\"rstd7_{c}\"]  = r.rolling(window=7, min_periods=2).std().reset_index(level=0, drop=True).fillna(0.0)\n",
    "\n",
    "    df_all[\"age_days\"] = df_all.groupby(\"serial_number\", sort=False).cumcount()\n",
    "    d = df_all[\"date\"]\n",
    "    df_all[\"month\"] = d.dt.month\n",
    "    df_all[\"day_of_week\"] = d.dt.dayofweek\n",
    "\n",
    "    # Categóricas nativas\n",
    "    if \"model\" not in df_all.columns: df_all[\"model\"] = \"UNK\"\n",
    "    df_all[\"model\"] = df_all[\"model\"].astype(str).fillna(\"UNK\")\n",
    "    df_all[\"vendor\"] = extract_vendor(df_all[\"model\"]).astype(str).fillna(\"UNK\")\n",
    "\n",
    "    # Ensamble de X\n",
    "    drop_cols = [\"serial_number\",\"date\",\"failure\"]\n",
    "    X_all = df_all.drop(columns=[c for c in drop_cols if c in df_all.columns], errors=\"ignore\")\n",
    "\n",
    "    # Dtypes: num -> float32, cat -> category\n",
    "    cat_cols = [c for c in [\"model\",\"vendor\"] if c in X_all.columns]\n",
    "    for c in X_all.columns:\n",
    "        if c in cat_cols:\n",
    "            X_all[c] = X_all[c].astype(\"category\")\n",
    "        else:\n",
    "            X_all[c] = pd.to_numeric(X_all[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "    # Quitar constantes\n",
    "    num_cols = [c for c in X_all.columns if c not in cat_cols]\n",
    "    var = X_all[num_cols].var()\n",
    "    keep_num = var[var > 0].index.tolist()\n",
    "    X_all = pd.concat([X_all[cat_cols], X_all[keep_num]], axis=1)\n",
    "\n",
    "    X_tr = X_all[df_all[\"__subset__\"]==\"train\"].reset_index(drop=True)\n",
    "    X_dev= X_all[df_all[\"__subset__\"]==\"dev\"].reset_index(drop=True)\n",
    "    X_te = X_all[df_all[\"__subset__\"]==\"test\"].reset_index(drop=True)\n",
    "\n",
    "    feature_names = list(X_all.columns)\n",
    "    return X_tr, X_dev, X_te, feature_names, cat_cols\n",
    "\n",
    "\n",
    "def create_features_joined_big_codes(\n",
    "    df_train: pd.DataFrame, df_dev: pd.DataFrame, df_test: pd.DataFrame,\n",
    "    add_rolling: bool, fit_cats_on_train: bool = True, category_maps: Optional[Dict[str, Dict[str, int]]] = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str], Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    BIG (HDD): TRAIN∪DEV∪TEST → deltas/cummax (sin rolling), age, calendario,\n",
    "    categóricas mapeadas a códigos (model_code/vendor_code). Robusto y liviano.\n",
    "    \"\"\"\n",
    "    df_train = df_train.copy(); df_train[\"__subset__\"]=\"train\"\n",
    "    df_dev   = df_dev.copy();   df_dev[\"__subset__\"]=\"dev\"\n",
    "    df_test  = df_test.copy();  df_test[\"__subset__\"]=\"test\"\n",
    "    df_all = pd.concat([df_train, df_dev, df_test], ignore_index=True)\n",
    "    df_all.sort_values([\"serial_number\",\"date\"], inplace=True)\n",
    "\n",
    "    all_cols = df_all.columns.tolist()\n",
    "    smart_cols = [c for c in all_cols if (\"smart\" in c.lower()) and (\"_raw\" in c.lower())]\n",
    "    for c in smart_cols:\n",
    "        df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # deltas/cummax (no rolling en BIG)\n",
    "    for c in smart_cols:\n",
    "        g = df_all.groupby(\"serial_number\", sort=False)[c]\n",
    "        df_all[f\"delta_{c}\"] = g.diff().fillna(0.0)\n",
    "        df_all[f\"max_{c}\"]   = g.cummax()\n",
    "\n",
    "    df_all[\"age_days\"] = df_all.groupby(\"serial_number\", sort=False).cumcount()\n",
    "    d = df_all[\"date\"]\n",
    "    df_all[\"month\"] = d.dt.month\n",
    "    df_all[\"day_of_week\"] = d.dt.dayofweek\n",
    "\n",
    "    # Categóricos → códigos\n",
    "    if fit_cats_on_train:\n",
    "        cat_maps = fit_category_maps(df_train)\n",
    "    else:\n",
    "        cat_maps = category_maps or {}\n",
    "    model_code, vendor_code = apply_category_maps(df_all, cat_maps)\n",
    "    df_all[\"model_code\"] = model_code\n",
    "    df_all[\"vendor_code\"] = vendor_code\n",
    "\n",
    "    drop_cols = [\"serial_number\",\"date\",\"failure\",\"model\"]\n",
    "    X_all = df_all.drop(columns=[c for c in drop_cols if c in df_all.columns], errors=\"ignore\")\n",
    "\n",
    "    for c in X_all.columns:\n",
    "        X_all[c] = pd.to_numeric(X_all[c], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "    # quitar constantes\n",
    "    var = X_all.var()\n",
    "    keep = var[var > 0].index.tolist()\n",
    "    X_all = X_all[keep].astype(np.float32)\n",
    "\n",
    "    X_tr = X_all[df_all[\"__subset__\"]==\"train\"].reset_index(drop=True)\n",
    "    X_dev= X_all[df_all[\"__subset__\"]==\"dev\"].reset_index(drop=True)\n",
    "    X_te = X_all[df_all[\"__subset__\"]==\"test\"].reset_index(drop=True)\n",
    "\n",
    "    feature_names = list(X_all.columns)\n",
    "    return X_tr, X_dev, X_te, feature_names, cat_maps\n",
    "\n",
    "\n",
    "# ========== STREAMING FEATURES (TEST BIG MODE, sin rolling) ==========\n",
    "\n",
    "class StreamDeltaCummaxBuilderXGB:\n",
    "    \"\"\"\n",
    "    Construye features para TEST en streaming (BIG) con mapeo categórico fijo.\n",
    "    Salida: smart, delta_*, max_*, age_days, month, day_of_week, model_code, vendor_code.\n",
    "    \"\"\"\n",
    "    def __init__(self, smart_cols: List[str], cat_maps: Dict[str, Dict[str, int]]):\n",
    "        self.smart_cols = smart_cols\n",
    "        self.cat_maps = cat_maps\n",
    "        self.last_vals: Dict[str, Dict[str, float]] = {}\n",
    "        self.cummax_vals: Dict[str, Dict[str, float]] = {}\n",
    "        self.age: Dict[str, int] = {}\n",
    "\n",
    "    def _ensure_serial(self, sn: str):\n",
    "        if sn not in self.last_vals:\n",
    "            self.last_vals[sn] = {c: 0.0 for c in self.smart_cols}\n",
    "            self.cummax_vals[sn] = {c: 0.0 for c in self.smart_cols}\n",
    "            self.age[sn] = 0\n",
    "\n",
    "    def transform_chunk(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.sort_values([\"serial_number\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "        # Categóricos (códigos fijos)\n",
    "        model_code, vendor_code = apply_category_maps(df, self.cat_maps)\n",
    "        df[\"model_code\"] = model_code\n",
    "        df[\"vendor_code\"] = vendor_code\n",
    "\n",
    "        # Calendario\n",
    "        df[\"month\"] = df[\"date\"].dt.month\n",
    "        df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n",
    "\n",
    "        # Cast SMART\n",
    "        for c in self.smart_cols:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        deltas = {f\"delta_{c}\": [] for c in self.smart_cols}\n",
    "        cummaxs= {f\"max_{c}\": [] for c in self.smart_cols}\n",
    "        ages = []\n",
    "\n",
    "        serials = df[\"serial_number\"].astype(str).values\n",
    "        for idx, sn in enumerate(serials):\n",
    "            self._ensure_serial(sn)\n",
    "            ages.append(self.age[sn]); self.age[sn] += 1\n",
    "            for c in self.smart_cols:\n",
    "                v = float(df.at[idx, c])\n",
    "                d = v - self.last_vals[sn][c]\n",
    "                self.last_vals[sn][c] = v\n",
    "                self.cummax_vals[sn][c] = max(self.cummax_vals[sn][c], v)\n",
    "                deltas[f\"delta_{c}\"].append(d)\n",
    "                cummaxs[f\"max_{c}\"].append(self.cummax_vals[sn][c])\n",
    "\n",
    "        for k, arr in deltas.items(): df[k] = arr\n",
    "        for k, arr in cummaxs.items(): df[k] = arr\n",
    "        df[\"age_days\"] = ages\n",
    "\n",
    "        feat_cols = (\n",
    "            self.smart_cols\n",
    "            + [f\"delta_{c}\" for c in self.smart_cols]\n",
    "            + [f\"max_{c}\" for c in self.smart_cols]\n",
    "            + [\"age_days\",\"month\",\"day_of_week\",\"model_code\",\"vendor_code\"]\n",
    "        )\n",
    "        return df[feat_cols].astype(np.float32)\n",
    "\n",
    "\n",
    "# ===================== CV, SAMPLING, THRESHOLDING =====================\n",
    "\n",
    "def make_group_folds(serials: pd.Series, y: np.ndarray, n_splits: int = 5, random_state: int = 42):\n",
    "    serials = serials.astype(str).values\n",
    "    uniq_serials, inverse = np.unique(serials, return_inverse=True)\n",
    "    y_disk = np.zeros(len(uniq_serials), dtype=np.int8)\n",
    "    for idx_row, disk_idx in enumerate(inverse):\n",
    "        if y[idx_row] == 1:\n",
    "            y_disk[disk_idx] = 1\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for tr_d, va_d in skf.split(uniq_serials, y_disk):\n",
    "        tr_mask = np.isin(inverse, tr_d)\n",
    "        va_mask = np.isin(inverse, va_d)\n",
    "        yield np.where(tr_mask)[0], np.where(va_mask)[0]\n",
    "\n",
    "def sample_negatives_hard(\n",
    "    X: pd.DataFrame, y: np.ndarray, dtf: np.ndarray, lookahead: int,\n",
    "    neg_pos_ratio: int = 3, hard_window: int = 60, hard_fraction: float = 0.7,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    if len(pos_idx) == 0:\n",
    "        raise ValueError(\"No positives in training fold for sampling\")\n",
    "    n_pos = len(pos_idx)\n",
    "    n_neg_needed = max(n_pos * neg_pos_ratio, 1)\n",
    "    hard_mask = (dtf > lookahead) & (dtf <= hard_window)\n",
    "    hard_idx = np.where((y == 0) & hard_mask)[0]\n",
    "    easy_idx = np.where((y == 0) & (~hard_mask))[0]\n",
    "    n_hard = min(int(n_neg_needed * hard_fraction), len(hard_idx))\n",
    "    n_easy = min(n_neg_needed - n_hard, len(easy_idx))\n",
    "    chosen_hard = rng.choice(hard_idx, size=n_hard, replace=False) if n_hard > 0 else np.empty(0, dtype=int)\n",
    "    chosen_easy = rng.choice(easy_idx, size=n_easy, replace=False) if n_easy > 0 else np.empty(0, dtype=int)\n",
    "    sel_idx = np.sort(np.concatenate([pos_idx, chosen_hard, chosen_easy]))\n",
    "    Xb = X.iloc[sel_idx].reset_index(drop=True); yb = y[sel_idx]\n",
    "    return Xb, yb, sel_idx\n",
    "\n",
    "def pick_threshold_precision_first(\n",
    "    y_true: np.ndarray, proba: np.ndarray,\n",
    "    min_precision: float = 0.90, min_recall: float = 0.03,\n",
    "    top_k_rate: float = 1e-4, min_alerts: int = 5\n",
    "):\n",
    "    precision, recall, thr = precision_recall_curve(y_true, proba)\n",
    "    pr_auc = average_precision_score(y_true, proba)\n",
    "    valid = (precision >= min_precision) & (recall >= min_recall)\n",
    "    if valid.any():\n",
    "        idxs = np.where(valid)[0]\n",
    "        idx = idxs[-1] - 1 if idxs[-1] >= len(thr) else idxs[-1]\n",
    "        idx = max(0, min(idx, len(thr)-1))\n",
    "        chosen = thr[idx]\n",
    "    else:\n",
    "        k = max(max(1, min_alerts), int(len(proba) * max(top_k_rate, 1e-6)))\n",
    "        chosen = float(np.partition(proba, -k)[-k])\n",
    "    return float(chosen), float(pr_auc)\n",
    "\n",
    "def metrics_at_threshold(y_true: np.ndarray, proba: np.ndarray, thr: float) -> Dict:\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        'confusion_matrix': [[int(tn), int(fp)], [int(fn), int(tp)]],\n",
    "        'fpr': float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "# ===================== XGBoost GPU (xgb.train + wrapper robusto) =====================\n",
    "\n",
    "def get_xgb_default_params(\n",
    "    random_state: int = 42,\n",
    "    enable_categorical: bool = True,\n",
    "    max_depth: int = 8,\n",
    "    n_estimators: int = 1500,\n",
    "    learning_rate: float = 0.06,\n",
    "    min_child_weight: float = 8.0,\n",
    "    subsample: float = 0.8,\n",
    "    colsample_bytree: float = 0.6,\n",
    "    reg_lambda: float = 1.0,\n",
    "    reg_alpha: float = 0.0,\n",
    "    max_bin: int = 256,\n",
    "    max_cat_to_onehot: int = 16,\n",
    "):\n",
    "    params = dict(\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_lambda=reg_lambda,\n",
    "        reg_alpha=reg_alpha,\n",
    "        max_bin=max_bin,\n",
    "        enable_categorical=enable_categorical,\n",
    "        max_cat_to_onehot=max_cat_to_onehot,\n",
    "        random_state=random_state,\n",
    "        nthread=4,\n",
    "        verbosity=0,\n",
    "    )\n",
    "    return params, int(n_estimators)\n",
    "\n",
    "class XGBBoosterWrapper:\n",
    "    \"\"\"Wrapper sobre xgb.Booster con predict_proba robusto a ausencia de early stopping.\"\"\"\n",
    "    def __init__(self, booster: xgb.Booster):\n",
    "        self.booster = booster\n",
    "    def predict_proba(self, X: pd.DataFrame | np.ndarray) -> np.ndarray:\n",
    "        # Ajuste de categorías si llegan como object (RAM path)\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X2 = X.copy()\n",
    "            for c in X2.columns:\n",
    "                if str(X2[c].dtype) == 'object' and X2[c].nunique() < 10000:\n",
    "                    X2[c] = X2[c].astype('category')\n",
    "            d = xgb.DMatrix(X2, enable_categorical=True)\n",
    "        else:\n",
    "            d = xgb.DMatrix(X)\n",
    "        best_it = getattr(self.booster, \"best_iteration\", None)\n",
    "        if best_it is not None:\n",
    "            try:\n",
    "                pred = self.booster.predict(d, iteration_range=(0, int(best_it)+1))\n",
    "            except TypeError:\n",
    "                ntree_limit = getattr(self.booster, \"best_ntree_limit\", int(best_it)+1)\n",
    "                pred = self.booster.predict(d, ntree_limit=int(ntree_limit))\n",
    "        else:\n",
    "            pred = self.booster.predict(d)\n",
    "        return np.vstack([1 - pred, pred]).T\n",
    "    def __getstate__(self):\n",
    "        return {'raw': self.booster.save_raw()}\n",
    "    def __setstate__(self, state):\n",
    "        bst = xgb.Booster(); bst.load_model(bytearray(state['raw'])); self.booster = bst\n",
    "\n",
    "def train_xgb_gpu(\n",
    "    X_tr: pd.DataFrame, y_tr: np.ndarray,\n",
    "    X_va: Optional[pd.DataFrame] = None, y_va: Optional[np.ndarray] = None,\n",
    "    params: Optional[Dict] = None, n_estimators: Optional[int] = None,\n",
    "    early_stopping_rounds: int = 200\n",
    ") -> XGBBoosterWrapper:\n",
    "    base_params, base_n = get_xgb_default_params()\n",
    "    if params: base_params.update(params)\n",
    "    if n_estimators is None: n_estimators = base_n\n",
    "\n",
    "    # Ajuste de dtypes para categóricas si enable_categorical=True\n",
    "    X_tr2 = X_tr.copy()\n",
    "    if base_params.get(\"enable_categorical\", False) and isinstance(X_tr2, pd.DataFrame):\n",
    "        for c in X_tr2.columns:\n",
    "            if str(X_tr2[c].dtype) == 'object':\n",
    "                X_tr2[c] = X_tr2[c].astype('category')\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr2, label=y_tr, enable_categorical=base_params.get(\"enable_categorical\", False))\n",
    "    evals = []\n",
    "    if X_va is not None and y_va is not None:\n",
    "        X_va2 = X_va.copy()\n",
    "        if base_params.get(\"enable_categorical\", False) and isinstance(X_va2, pd.DataFrame):\n",
    "            for c in X_va2.columns:\n",
    "                if str(X_va2[c].dtype) == 'object':\n",
    "                    X_va2[c] = X_va2[c].astype('category')\n",
    "        dvalid = xgb.DMatrix(X_va2, label=y_va, enable_categorical=base_params.get(\"enable_categorical\", False))\n",
    "        evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=base_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=int(n_estimators),\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=early_stopping_rounds if evals else 0,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    return XGBBoosterWrapper(booster)\n",
    "\n",
    "\n",
    "# ===================== MAIN PIPELINE (v72s4 con BIG-MODE) =====================\n",
    "\n",
    "def train_xgb_precision_pipeline_v72s4(\n",
    "    train_parquet: str,\n",
    "    test_parquet: Optional[str],\n",
    "    dataset_type: str,\n",
    "\n",
    "    # Splits por año\n",
    "    train_years: List[int] = [2020, 2021, 2022, 2023],\n",
    "    dev_years:   List[int] = [2024],\n",
    "    test_years:  List[int] = [2025],\n",
    "\n",
    "    lookahead_days: int = 7,\n",
    "    n_splits: int = 5,\n",
    "\n",
    "    # Hard-neg\n",
    "    neg_pos_ratio: int = 3,        # HDD sugerido 5\n",
    "    hard_window: int = 60,         # HDD 90\n",
    "    hard_fraction: float = 0.7,\n",
    "\n",
    "    # XGB\n",
    "    xgb_params: Optional[Dict] = None,\n",
    "    xgb_n_estimators: int = 1500,\n",
    "    early_stopping_rounds: int = 200,\n",
    "\n",
    "    # Objetivo de umbral\n",
    "    min_precision: float = 0.90,\n",
    "    min_recall: float = 0.03,\n",
    "    top_k_rate: float = 1e-4,\n",
    "    min_alerts: int = 20,\n",
    "\n",
    "    # BIG switches\n",
    "    big_mode: bool = False,              # TRAIN big (HDD)\n",
    "    test_big_mode: bool = False,         # TEST streaming (2024)\n",
    "    neg_random_keep_rate: float = 0.0025,\n",
    "    add_rolling_ram: bool = True,        # RAM (SSD) usa rolling; BIG (HDD) siempre sin rolling\n",
    "\n",
    "    output_dir: str = './models_xgb_precision_v72s4',\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    print(\"=\"*100)\n",
    "    print(f\"XGBOOST (GPU, PRECISION v72s4) - {dataset_type.upper()}\")\n",
    "    print(f\"Big-mode TRAIN={big_mode} | TEST streaming={test_big_mode}\")\n",
    "    print(\"=\"*100)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Descubrir SMART para lectura minimal\n",
    "    smart_cols = discover_smart_columns(train_parquet)\n",
    "    base_cols = ['serial_number','date','failure','model']\n",
    "    read_cols = base_cols + smart_cols\n",
    "\n",
    "    # ---------- LOAD TRAIN ----------\n",
    "    if big_mode:\n",
    "        fail_map_train = scan_fail_dates(train_parquet, train_years)\n",
    "        df_train_raw = load_data_big_filtered(\n",
    "            train_parquet, train_years, fail_map_train,\n",
    "            lookahead_days=lookahead_days, hard_window=hard_window,\n",
    "            neg_random_keep_rate=neg_random_keep_rate, columns=read_cols\n",
    "        )\n",
    "    else:\n",
    "        df_train_raw = load_data(train_parquet, train_years, columns=read_cols)\n",
    "\n",
    "    # ---------- LOAD DEV/TEST (RAM si test_big_mode=False) ----------\n",
    "    df_dev_raw  = load_data(train_parquet, dev_years, columns=read_cols) if dev_years and (not big_mode) else pd.DataFrame()\n",
    "    df_test_raw = load_data(test_parquet, test_years, columns=read_cols) if (test_parquet and test_years and (not test_big_mode)) else pd.DataFrame()\n",
    "\n",
    "    if df_train_raw.empty:\n",
    "        raise ValueError(\"Training data is empty!\")\n",
    "\n",
    "    # ---------- PREP ----------\n",
    "    df_tr  = prepare_df(df_train_raw)\n",
    "    df_dev = prepare_df(df_dev_raw) if not df_dev_raw.empty else pd.DataFrame()\n",
    "    df_te  = prepare_df(df_test_raw) if not df_test_raw.empty else pd.DataFrame()\n",
    "\n",
    "    # ---------- LABELS ----------\n",
    "    dtf_tr = compute_days_to_failure(df_tr)\n",
    "    y_tr   = create_labels_from_dtf(dtf_tr, lookahead_days)\n",
    "    print(f\"TRAIN labels: {int(y_tr.sum()):,} pos ({100*y_tr.mean():.4f}%)\")\n",
    "    if y_tr.sum() < 50:\n",
    "        raise ValueError(f\"Insufficient positive samples in TRAIN: {y_tr.sum()}\")\n",
    "\n",
    "    # ---------- FEATURES ----------\n",
    "    if big_mode:\n",
    "        # BIG: sin rolling + códigos\n",
    "        X_tr, X_dev, X_te, feature_names, cat_maps = create_features_joined_big_codes(\n",
    "            df_tr, df_dev, df_te, add_rolling=False, fit_cats_on_train=True, category_maps=None\n",
    "        )\n",
    "        feature_style = \"codes_no_rolling\"\n",
    "        enable_categorical = False\n",
    "    else:\n",
    "        # RAM: rolling + categóricas nativas\n",
    "        X_tr, X_dev, X_te, feature_names, cat_cols = create_features_joined_ram_v72(\n",
    "            df_tr, df_dev, df_te, add_rolling=add_rolling_ram\n",
    "        )\n",
    "        cat_maps = {}    # no se usan en RAM\n",
    "        feature_style = \"categorical_with_rolling\"\n",
    "        enable_categorical = True\n",
    "\n",
    "    # ---------- CV por disco ----------\n",
    "    serials_tr = df_tr['serial_number']\n",
    "    oof_proba = np.zeros(len(y_tr), dtype=np.float32)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(make_group_folds(serials_tr, y_tr, n_splits=n_splits, random_state=random_state), start=1):\n",
    "        X_tr_fold, y_tr_fold = X_tr.iloc[tr_idx].reset_index(drop=True), y_tr[tr_idx]\n",
    "        dtf_tr_fold = dtf_tr[tr_idx]\n",
    "        X_va_fold, y_va_fold = X_tr.iloc[va_idx].reset_index(drop=True), y_tr[va_idx]\n",
    "\n",
    "        print(f\"\\nFold {fold}/{n_splits}: train={len(y_tr_fold):,} (pos={int(y_tr_fold.sum()):,}) | \"\n",
    "              f\"val={len(y_va_fold):,} (pos={int(y_va_fold.sum()):,})\")\n",
    "\n",
    "        # Hard-neg\n",
    "        Xb, yb, _ = sample_negatives_hard(\n",
    "            X_tr_fold, y_tr_fold, dtf_tr_fold,\n",
    "            lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio if dataset_type.upper()=='SSD' else max(neg_pos_ratio, 5),\n",
    "            hard_window=hard_window if dataset_type.upper()=='SSD' else max(hard_window, 90),\n",
    "            hard_fraction=hard_fraction,\n",
    "            seed=random_state,\n",
    "        )\n",
    "        print(f\"  After hard-neg sampling: {len(yb):,} (pos={int(yb.sum()):,}, neg={len(yb)-int(yb.sum()):,})\")\n",
    "\n",
    "        # XGB params\n",
    "        params, _ndef = get_xgb_default_params(enable_categorical=enable_categorical)\n",
    "        if xgb_params: params.update(xgb_params)\n",
    "\n",
    "        model = train_xgb_gpu(\n",
    "            Xb, yb, X_va_fold, y_va_fold,\n",
    "            params=params, n_estimators=xgb_n_estimators,\n",
    "            early_stopping_rounds=early_stopping_rounds\n",
    "        )\n",
    "\n",
    "        proba_va = model.predict_proba(X_va_fold)[:, 1]\n",
    "        oof_proba[va_idx] = proba_va\n",
    "\n",
    "        thr_fold, pr_auc = pick_threshold_precision_first(\n",
    "            y_va_fold, proba_va, min_precision=min_precision, min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "        )\n",
    "        m = metrics_at_threshold(y_va_fold, proba_va, thr_fold)\n",
    "        m.update({'pr_auc': float(pr_auc), 'threshold': float(thr_fold), 'fold': int(fold)})\n",
    "        fold_metrics.append(m)\n",
    "        print(f\"  Fold {fold} @thr={thr_fold:.4f} | P={m['precision']:.3f} R={m['recall']:.3f} F1={m['f1']:.3f} PR-AUC={pr_auc:.4f}\")\n",
    "\n",
    "        del X_tr_fold, y_tr_fold, X_va_fold, y_va_fold, Xb, yb, model\n",
    "        cleanup()\n",
    "\n",
    "    # ---------- OOF (umbral global provisional) ----------\n",
    "    thr_oof_global, pr_auc_oof = pick_threshold_precision_first(\n",
    "        y_tr, oof_proba, min_precision=min_precision, min_recall=min_recall,\n",
    "        top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "    )\n",
    "    agg_metrics = metrics_at_threshold(y_tr, oof_proba, thr_oof_global)\n",
    "    agg_metrics.update({'pr_auc': float(pr_auc_oof), 'threshold': float(thr_oof_global)})\n",
    "    print(\"\\nOOF (diag):\")\n",
    "    print(json.dumps(agg_metrics, indent=2))\n",
    "\n",
    "    # ---------- Calibración en DEV (si está disponible y no estamos en big_mode) ----------\n",
    "    thr_prod = float(thr_oof_global)\n",
    "    dev_metrics = None\n",
    "    if not df_dev.empty and not big_mode:\n",
    "        # Reentrena con todo TRAIN (hard-neg) y calibra umbral en DEV\n",
    "        Xb_full, yb_full, _ = sample_negatives_hard(\n",
    "            X_tr, y_tr, dtf_tr,\n",
    "            lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio,\n",
    "            hard_window=hard_window if dataset_type.upper()=='SSD' else max(hard_window, 90),\n",
    "            hard_fraction=hard_fraction, seed=random_state\n",
    "        )\n",
    "        params, _ = get_xgb_default_params(enable_categorical=enable_categorical)\n",
    "        if xgb_params: params.update(xgb_params)\n",
    "        final_model_dev = train_xgb_gpu(\n",
    "            Xb_full, yb_full, params=params, n_estimators=xgb_n_estimators, early_stopping_rounds=0\n",
    "        )\n",
    "        proba_dev = final_model_dev.predict_proba(X_dev)[:, 1]\n",
    "        thr_prod, pr_auc_dev = pick_threshold_precision_first(\n",
    "            y_true=create_labels_from_dtf(compute_days_to_failure(df_dev), lookahead_days),\n",
    "            proba=proba_dev, min_precision=min_precision, min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "        )\n",
    "        dev_metrics = metrics_at_threshold(create_labels_from_dtf(compute_days_to_failure(df_dev), lookahead_days), proba_dev, thr_prod)\n",
    "        dev_metrics.update({'pr_auc': float(pr_auc_dev), 'threshold': float(thr_prod)})\n",
    "        print(\"\\nDEV calibration metrics (used for PROD threshold):\")\n",
    "        print(json.dumps(dev_metrics, indent=2))\n",
    "        del final_model_dev; cleanup()\n",
    "\n",
    "    # ---------- FINAL (entrena con todo TRAIN y guarda artefactos) ----------\n",
    "    Xb_full, yb_full, _ = sample_negatives_hard(\n",
    "        X_tr, y_tr, dtf_tr,\n",
    "        lookahead=lookahead_days,\n",
    "        neg_pos_ratio=neg_pos_ratio if dataset_type.upper()=='SSD' else max(neg_pos_ratio, 5),\n",
    "        hard_window=hard_window if dataset_type.upper()=='SSD' else max(hard_window, 90),\n",
    "        hard_fraction=hard_fraction, seed=random_state\n",
    "    )\n",
    "    params, _ = get_xgb_default_params(enable_categorical=enable_categorical)\n",
    "    if xgb_params: params.update(xgb_params)\n",
    "\n",
    "    final_model = train_xgb_gpu(\n",
    "        Xb_full, yb_full, params=params, n_estimators=xgb_n_estimators, early_stopping_rounds=0\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_prefix = os.path.join(output_dir, f\"{dataset_type}_xgb_v72s4_{timestamp}\")\n",
    "    joblib.dump(final_model, f\"{model_prefix}_model.pkl\")\n",
    "    final_model.booster.save_model(f\"{model_prefix}_model.json\")\n",
    "\n",
    "    features_meta = {\n",
    "        'feature_names': feature_names,\n",
    "        'feature_style': feature_style,    # 'codes_no_rolling' | 'categorical_with_rolling'\n",
    "        'threshold': float(thr_prod),\n",
    "        'category_maps': cat_maps,         # vacío en RAM\n",
    "        'smart_cols': smart_cols,\n",
    "        'xgb_params': params | {'n_estimators': int(xgb_n_estimators)}\n",
    "    }\n",
    "    with open(f\"{model_prefix}_features.json\", 'w') as f:\n",
    "        json.dump(features_meta, f, indent=2)\n",
    "    print(f\"\\n✓ Final model saved: {model_prefix}_model.pkl/.json\")\n",
    "\n",
    "    # ---------- TEST (solo si no es streaming) ----------\n",
    "    test_metrics = None\n",
    "    if not df_te.empty and (not test_big_mode):\n",
    "        y_test = create_labels_from_dtf(compute_days_to_failure(df_te), lookahead_days)\n",
    "        proba_test = final_model.predict_proba(X_te)[:, 1]\n",
    "        test_metrics = metrics_at_threshold(y_test, proba_test, float(thr_prod))\n",
    "        test_metrics.update({'pr_auc': float(average_precision_score(y_test, proba_test)), 'threshold_used': float(thr_prod)})\n",
    "        print(\"\\nTEST metrics:\")\n",
    "        print(json.dumps(test_metrics, indent=2))\n",
    "    else:\n",
    "        print(\"\\nTEST skipped (streaming test can be run later with evaluate_saved_model_2024_streaming_xgb).\")\n",
    "\n",
    "    # ---------- Metadata ----------\n",
    "    metadata = {\n",
    "        'dataset_type': dataset_type,\n",
    "        'train_years': train_years,\n",
    "        'dev_years': dev_years,\n",
    "        'test_years': test_years,\n",
    "        'lookahead_days': lookahead_days,\n",
    "        'n_splits': n_splits,\n",
    "        'hard_negative_sampling': {'neg_pos_ratio': neg_pos_ratio, 'hard_window': hard_window, 'hard_fraction': hard_fraction},\n",
    "        'xgb_params': params | {'n_estimators': int(xgb_n_estimators)},\n",
    "        'min_precision': min_precision, 'min_recall': min_recall,\n",
    "        'top_k_rate': top_k_rate, 'min_alerts': min_alerts,\n",
    "        'oof_metrics': {\n",
    "            'precision': agg_metrics['precision'],\n",
    "            'recall': agg_metrics['recall'],\n",
    "            'f1': agg_metrics['f1'],\n",
    "            'confusion_matrix': agg_metrics['confusion_matrix'],\n",
    "            'fpr': agg_metrics['fpr'],\n",
    "            'pr_auc': agg_metrics['pr_auc'],\n",
    "            'threshold': agg_metrics['threshold']\n",
    "        },\n",
    "        'dev_metrics': dev_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'feature_names': feature_names,\n",
    "        'feature_style': feature_style,\n",
    "        'smart_cols': smart_cols\n",
    "    }\n",
    "    meta_path = os.path.join(output_dir, f\"{dataset_type}_xgb_v72s4_{timestamp}_metadata.json\")\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"\\n✓ Metadata saved: {meta_path}\")\n",
    "    print(\"=\"*100)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# ===================== TEST-ONLY 2024 (STREAMING, BIG) =====================\n",
    "\n",
    "def evaluate_saved_model_2024_streaming_xgb(\n",
    "    model_path: str,\n",
    "    features_meta_path: str,   # *_features.json (feature_names, feature_style, thr, cat_maps, smart_cols)\n",
    "    parquet_path: str,\n",
    "    test_years: List[int] = [2024],\n",
    "    lookahead_days: int = 7,\n",
    "    chunk_limit_rows: int = 0   # 0 = todos los row-groups; útil para “probar” en pequeño\n",
    "):\n",
    "    \"\"\"\n",
    "    Evalúa TEST (p.ej. 2024) en streaming:\n",
    "      • Requiere modelos entrenados en estilo 'codes_no_rolling' (HDD big-mode)\n",
    "      • Construye labels con fail_map\n",
    "      • Genera features sin rolling con estado por serial\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"EVALUATE SAVED MODEL — TEST 2024 (STREAMING, XGB)\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    model: XGBBoosterWrapper = joblib.load(model_path)\n",
    "    with open(features_meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    feature_names = meta[\"feature_names\"]\n",
    "    feature_style = meta.get(\"feature_style\", \"codes_no_rolling\")\n",
    "    if feature_style != \"codes_no_rolling\":\n",
    "        raise ValueError(f\"evaluate_saved_model_2024_streaming_xgb requiere feature_style='codes_no_rolling', got '{feature_style}'\")\n",
    "    thr = float(meta.get(\"threshold\", 0.5))\n",
    "    cat_maps = meta.get(\"category_maps\", {})\n",
    "    smart_cols = meta.get(\"smart_cols\", discover_smart_columns(parquet_path))\n",
    "\n",
    "    base_cols = ['serial_number','date','failure','model']\n",
    "    read_cols = base_cols + smart_cols\n",
    "\n",
    "    # Labeling con fail_map del TEST\n",
    "    fail_map_test = scan_fail_dates(parquet_path, test_years)\n",
    "\n",
    "    builder = StreamDeltaCummaxBuilderXGB(smart_cols=smart_cols, cat_maps=cat_maps)\n",
    "\n",
    "    tn=fp=fn=tp=0\n",
    "    disk_stat: Dict[str, Tuple[float, int]] = {}\n",
    "\n",
    "    processed_groups = 0\n",
    "    for df in _iter_parquet_row_groups(parquet_path, test_years, columns=read_cols):\n",
    "        if chunk_limit_rows and processed_groups >= chunk_limit_rows:\n",
    "            break\n",
    "\n",
    "        sn_ser = df[\"serial_number\"].astype(str)\n",
    "        dt = pd.to_datetime(df[\"date\"], errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "        fdt = pd.to_datetime(sn_ser.map(fail_map_test), errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "\n",
    "        dtf = np.full(len(df), 10**9, dtype=np.int64)\n",
    "        valid = ~np.isnat(fdt)\n",
    "        if valid.any():\n",
    "            dd = (fdt[valid] - dt[valid]).astype(\"timedelta64[D]\").astype(\"int64\")\n",
    "            dtf[valid] = dd\n",
    "        y_chunk = ((dtf >= 0) & (dtf <= lookahead_days)).astype(np.int8)\n",
    "\n",
    "        X_chunk = builder.transform_chunk(df)\n",
    "\n",
    "        # Alinear columnas al orden guardado\n",
    "        for m in feature_names:\n",
    "            if m not in X_chunk.columns: X_chunk[m] = 0.0\n",
    "        X_chunk = X_chunk[feature_names].astype(np.float32)\n",
    "\n",
    "        proba = model.predict_proba(X_chunk)[:, 1]\n",
    "        y_pred = (proba >= thr).astype(np.int8)\n",
    "\n",
    "        cm = confusion_matrix(y_chunk, y_pred, labels=[0,1])\n",
    "        tn += int(cm[0,0]); fp += int(cm[0,1]); fn += int(cm[1,0]); tp += int(cm[1,1])\n",
    "\n",
    "        for s, p, y in zip(sn_ser.values, proba, y_chunk):\n",
    "            if s not in disk_stat: disk_stat[s] = (p, int(y))\n",
    "            else:\n",
    "                mp, my = disk_stat[s]\n",
    "                disk_stat[s] = (max(mp, p), max(my, int(y)))\n",
    "\n",
    "        processed_groups += 1\n",
    "        cleanup()\n",
    "\n",
    "    # Row metrics\n",
    "    row_metrics = {\n",
    "        'precision': float(tp / max(1, tp + fp)),\n",
    "        'recall': float(tp / max(1, tp + fn)),\n",
    "        'f1': float((2*tp) / max(1, 2*tp + fp + fn)),\n",
    "        'confusion_matrix': [[tn, fp], [fn, tp]],\n",
    "        'fpr': float(fp / max(1, fp + tn))\n",
    "    }\n",
    "    # Disk metrics\n",
    "    y_disk = np.array([v[1] for v in disk_stat.values()], dtype=np.int8)\n",
    "    yhat_disk = np.array([1 if v[0] >= thr else 0 for v in disk_stat.values()], dtype=np.int8)\n",
    "    cm_d = confusion_matrix(y_disk, yhat_disk, labels=[0,1])\n",
    "    tn_d, fp_d, fn_d, tp_d = int(cm_d[0,0]), int(cm_d[0,1]), int(cm_d[1,0]), int(cm_d[1,1])\n",
    "    disk_metrics = {\n",
    "        'precision': float(tp_d / max(1, tp_d + fp_d)),\n",
    "        'recall': float(tp_d / max(1, tp_d + fn_d)),\n",
    "        'f1': float((2*tp_d) / max(1, 2*tp_d + fp_d + fn_d)),\n",
    "        'confusion_matrix': [[tn_d, fp_d], [fn_d, tp_d]],\n",
    "        'n_disks': int(len(disk_stat))\n",
    "    }\n",
    "\n",
    "    print(\"\\nRow-level TEST metrics (streaming 2024):\")\n",
    "    print(json.dumps(row_metrics, indent=2))\n",
    "    print(\"\\nDisk-level TEST metrics (streaming 2024):\")\n",
    "    print(json.dumps(disk_metrics, indent=2))\n",
    "    print(\"=\"*100)\n",
    "    return {\"row\": row_metrics, \"disk\": disk_metrics}\n",
    "\n",
    "\n",
    "# ===================== WRAPPERS =====================\n",
    "\n",
    "def train_ssd_precision_v72s4_xgb():\n",
    "    \"\"\"\n",
    "    SSD:\n",
    "     - RAM (sin big_mode), rolling ON, categóricas nativas\n",
    "     - Umbral calibrado con DEV=2024 (si está disponible)\n",
    "    \"\"\"\n",
    "    xgb_params = dict(\n",
    "        learning_rate=0.06,\n",
    "        max_depth=8,\n",
    "        min_child_weight=8.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_lambda=1.2,\n",
    "        reg_alpha=0.0,\n",
    "        max_bin=256,\n",
    "        enable_categorical=True,     # RAM path\n",
    "        max_cat_to_onehot=16,\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "    return train_xgb_precision_pipeline_v72s4(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        train_years=[2020, 2021, 2022, 2023],\n",
    "        dev_years=[2024],\n",
    "        test_years=[2025],              # si quieres, cámbialo a [2024] y desactiva test_big_mode\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "        neg_pos_ratio=3,\n",
    "        hard_window=60,\n",
    "        hard_fraction=0.7,\n",
    "        xgb_params=xgb_params,\n",
    "        xgb_n_estimators=1500,\n",
    "        early_stopping_rounds=200,\n",
    "        min_precision=0.90,\n",
    "        min_recall=0.03,\n",
    "        top_k_rate=1e-4,\n",
    "        min_alerts=20,\n",
    "        big_mode=False,                 # RAM\n",
    "        test_big_mode=False,            # test normal\n",
    "        neg_random_keep_rate=0.0025,\n",
    "        add_rolling_ram=True,\n",
    "        output_dir='./models_xgb_ssd_v72s4'\n",
    "    )\n",
    "\n",
    "\n",
    "def train_hdd_precision_v72s4_xgb(neg_random_keep_rate: float = 0.0025):\n",
    "    \"\"\"\n",
    "    HDD:\n",
    "     - TRAIN big-mode (por partes, filtro streaming), sin rolling, categorías a códigos\n",
    "     - TEST 2024: se ejecuta aparte con evaluate_saved_model_2024_streaming_xgb(...)\n",
    "    \"\"\"\n",
    "    xgb_params = dict(\n",
    "        learning_rate=0.06,\n",
    "        max_depth=8,\n",
    "        min_child_weight=10.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.0,\n",
    "        max_bin=256,\n",
    "        enable_categorical=False,    # BIG path (todo numérico/códigos)\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "    return train_xgb_precision_pipeline_v72s4(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',  # no se carga en RAM si test_big_mode=True\n",
    "        dataset_type='HDD',\n",
    "        train_years=[2020, 2021, 2022, 2023],\n",
    "        dev_years=[],                      # opcionalmente vacío para evitar RAM extra\n",
    "        test_years=[2024],                 # usamos función streaming aparte\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "        neg_pos_ratio=5,\n",
    "        hard_window=90,\n",
    "        hard_fraction=0.7,\n",
    "        xgb_params=xgb_params,\n",
    "        xgb_n_estimators=1500,\n",
    "        early_stopping_rounds=200,\n",
    "        min_precision=0.90,\n",
    "        min_recall=0.03,\n",
    "        top_k_rate=7.5e-5,\n",
    "        min_alerts=30,\n",
    "        big_mode=True,                     # TRAIN por partes\n",
    "        test_big_mode=True,                # TEST lo harás con evaluate_saved_model_2024_streaming_xgb\n",
    "        neg_random_keep_rate=neg_random_keep_rate,\n",
    "        add_rolling_ram=False,             # sin rolling en BIG\n",
    "        output_dir='./models_xgb_hdd_v72s4'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cfb5621-9d4f-4444-9c05-df2cfbe41e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "XGBOOST (PRECISION-CONTROLLED v5, GPU) - SSD\n",
      "============================================================================================\n",
      "Loading [2020, 2021, 2022, 2023] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 2,124,111 rows\n",
      "Loading [2024] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 1,220,745 rows\n",
      "Labels (train): 1,325 positive (0.0624%)\n",
      "Creating features (temporal join) for SSD...\n",
      "  Found 13 SMART attributes (_raw)\n",
      "  Final features: 72\n",
      "  Partitioning enabled: SSD young\n",
      "\n",
      "Fold 1/5: train=1,709,653 (pos=1,079) | val=414,458 (pos=246)\n",
      "  [G1] After sampling: n=1,476 (pos=738, neg=738)\n",
      "  [G0] After sampling: n=682 (pos=341, neg=341)\n",
      "\n",
      "Fold 2/5: train=1,707,701 (pos=1,054) | val=416,410 (pos=271)\n",
      "  [G1] After sampling: n=1,522 (pos=761, neg=761)\n",
      "  [G0] After sampling: n=586 (pos=293, neg=293)\n",
      "\n",
      "Fold 3/5: train=1,682,081 (pos=1,061) | val=442,030 (pos=264)\n",
      "  [G1] After sampling: n=1,464 (pos=732, neg=732)\n",
      "  [G0] After sampling: n=658 (pos=329, neg=329)\n",
      "\n",
      "Fold 4/5: train=1,691,757 (pos=1,061) | val=432,354 (pos=264)\n",
      "  [G1] After sampling: n=1,560 (pos=780, neg=780)\n",
      "  [G0] After sampling: n=562 (pos=281, neg=281)\n",
      "\n",
      "Fold 5/5: train=1,705,252 (pos=1,045) | val=418,859 (pos=280)\n",
      "  [G1] After sampling: n=1,594 (pos=797, neg=797)\n",
      "  [G0] After sampling: n=496 (pos=248, neg=248)\n",
      "\n",
      "Aggregating OOF predictions to choose a global threshold...\n",
      "\n",
      "OOF Performance (using global threshold):\n",
      "{\n",
      "  \"precision\": 0.8571428571428571,\n",
      "  \"recall\": 0.07245283018867925,\n",
      "  \"f1\": 0.1336116910229645,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      2122770,\n",
      "      16\n",
      "    ],\n",
      "    [\n",
      "      1229,\n",
      "      96\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 7.5372647077943795e-06,\n",
      "  \"pr_auc\": 0.10935226487534482,\n",
      "  \"threshold\": 0.8790041208267212\n",
      "}\n",
      "\n",
      "Preparing TEST set (held-out years) with temporal continuity...\n",
      "\n",
      "✓ Final model(s) saved with prefix: ./models_xgb_ssd_precv5/SSD_xgb_preckit_v5_20251105_190501\n",
      "\n",
      "✓ Metadata saved: ./models_xgb_ssd_precv5/SSD_xgb_preckit_v5_20251105_190515_metadata.json\n",
      "============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'SSD',\n",
       " 'train_years': [2020, 2021, 2022, 2023],\n",
       " 'test_years': [2024],\n",
       " 'lookahead_days': 7,\n",
       " 'n_splits': 5,\n",
       " 'neg_pos_ratio': 1,\n",
       " 'hard_window': 60,\n",
       " 'hard_fraction': 0.7,\n",
       " 'xgb_params': {'n_estimators': 1300,\n",
       "  'learning_rate': 0.05,\n",
       "  'max_depth': 9,\n",
       "  'min_child_weight': 8,\n",
       "  'subsample': 0.8,\n",
       "  'colsample_bytree': 0.6,\n",
       "  'reg_lambda': 1.5,\n",
       "  'reg_alpha': 0.0,\n",
       "  'max_bin': 256,\n",
       "  'tree_method': 'gpu_hist',\n",
       "  'predictor': 'gpu_predictor',\n",
       "  'objective': 'binary:logistic',\n",
       "  'eval_metric': 'aucpr',\n",
       "  'nthread': 4,\n",
       "  'verbosity': 0,\n",
       "  'random_state': 42},\n",
       " 'min_precision': 0.85,\n",
       " 'min_recall': 0.05,\n",
       " 'top_k_rate': 0.0002,\n",
       " 'min_alerts': 20,\n",
       " 'partitioning': {'enabled': True,\n",
       "  'ssd_age_days_thresh': 90,\n",
       "  'hdd_hfh_thresh': 40000},\n",
       " 'oof_metrics': {'precision': 0.8571428571428571,\n",
       "  'recall': 0.07245283018867925,\n",
       "  'f1': 0.1336116910229645,\n",
       "  'confusion_matrix': [[2122770, 16], [1229, 96]],\n",
       "  'fpr': 7.5372647077943795e-06,\n",
       "  'pr_auc': 0.10935226487534482,\n",
       "  'threshold': 0.8790041208267212},\n",
       " 'fold_metrics': [{'precision': 0.8524590163934426,\n",
       "   'recall': 0.21138211382113822,\n",
       "   'f1': 0.33876221498371334,\n",
       "   'confusion_matrix': [[414203, 9], [194, 52]],\n",
       "   'fpr': 2.172800401726652e-05,\n",
       "   'pr_auc': 0.25084095388586086,\n",
       "   'threshold': 0.8528136610984802,\n",
       "   'fold': 1},\n",
       "  {'precision': 0.4318181818181818,\n",
       "   'recall': 0.07011070110701106,\n",
       "   'f1': 0.12063492063492064,\n",
       "   'confusion_matrix': [[416114, 25], [252, 19]],\n",
       "   'fpr': 6.00760803481529e-05,\n",
       "   'pr_auc': 0.05491901535741882,\n",
       "   'threshold': 0.7827397584915161,\n",
       "   'fold': 2},\n",
       "  {'precision': 0.0048337286845992035,\n",
       "   'recall': 0.4090909090909091,\n",
       "   'f1': 0.009554562746052108,\n",
       "   'confusion_matrix': [[419531, 22235], [156, 108]],\n",
       "   'fpr': 0.05033207625756622,\n",
       "   'pr_auc': 0.003341853902236372,\n",
       "   'threshold': 0.5291037559509277,\n",
       "   'fold': 3},\n",
       "  {'precision': 0.2688679245283019,\n",
       "   'recall': 0.2159090909090909,\n",
       "   'f1': 0.23949579831932774,\n",
       "   'confusion_matrix': [[431935, 155], [207, 57]],\n",
       "   'fpr': 0.0003587215626374135,\n",
       "   'pr_auc': 0.09926622883317372,\n",
       "   'threshold': 0.5794846415519714,\n",
       "   'fold': 4},\n",
       "  {'precision': 0.8627450980392157,\n",
       "   'recall': 0.15714285714285714,\n",
       "   'f1': 0.26586102719033233,\n",
       "   'confusion_matrix': [[418572, 7], [236, 44]],\n",
       "   'fpr': 1.6723246985634733e-05,\n",
       "   'pr_auc': 0.27413029961643665,\n",
       "   'threshold': 0.8881298899650574,\n",
       "   'fold': 5}],\n",
       " 'test_metrics': {'precision': 0.0004819443010086406,\n",
       "  'recall': 0.12334801762114538,\n",
       "  'f1': 0.0009601371624517788,\n",
       "  'confusion_matrix': [[1162448, 58070], [199, 28]],\n",
       "  'fpr': 0.04757815943722256,\n",
       "  'pr_auc': 0.00032596748768694566,\n",
       "  'threshold_used': 0.8790041208267212},\n",
       " 'feature_names': ['capacity_bytes',\n",
       "  'smart_5_raw',\n",
       "  'smart_187_raw',\n",
       "  'smart_194_raw',\n",
       "  'smart_231_raw',\n",
       "  'smart_233_raw',\n",
       "  'smart_241_raw',\n",
       "  'smart_242_raw',\n",
       "  'smart_9_raw',\n",
       "  'smart_173_raw',\n",
       "  'smart_174_raw',\n",
       "  'smart_184_raw',\n",
       "  'smart_199_raw',\n",
       "  'smart_230_raw',\n",
       "  '__subset__',\n",
       "  'delta_smart_5_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'delta_smart_194_raw',\n",
       "  'max_smart_194_raw',\n",
       "  'delta_smart_231_raw',\n",
       "  'max_smart_231_raw',\n",
       "  'delta_smart_233_raw',\n",
       "  'max_smart_233_raw',\n",
       "  'delta_smart_241_raw',\n",
       "  'max_smart_241_raw',\n",
       "  'delta_smart_242_raw',\n",
       "  'max_smart_242_raw',\n",
       "  'delta_smart_9_raw',\n",
       "  'max_smart_9_raw',\n",
       "  'delta_smart_173_raw',\n",
       "  'max_smart_173_raw',\n",
       "  'delta_smart_174_raw',\n",
       "  'max_smart_174_raw',\n",
       "  'delta_smart_184_raw',\n",
       "  'max_smart_184_raw',\n",
       "  'delta_smart_199_raw',\n",
       "  'max_smart_199_raw',\n",
       "  'delta_smart_230_raw',\n",
       "  'max_smart_230_raw',\n",
       "  'rmean7_smart_5_raw',\n",
       "  'rstd7_smart_5_raw',\n",
       "  'rmean7_smart_187_raw',\n",
       "  'rstd7_smart_187_raw',\n",
       "  'rmean7_smart_194_raw',\n",
       "  'rstd7_smart_194_raw',\n",
       "  'rmean7_smart_231_raw',\n",
       "  'rstd7_smart_231_raw',\n",
       "  'rmean7_smart_233_raw',\n",
       "  'rstd7_smart_233_raw',\n",
       "  'rmean7_smart_241_raw',\n",
       "  'rstd7_smart_241_raw',\n",
       "  'rmean7_smart_242_raw',\n",
       "  'rstd7_smart_242_raw',\n",
       "  'rmean7_smart_9_raw',\n",
       "  'rstd7_smart_9_raw',\n",
       "  'rmean7_smart_173_raw',\n",
       "  'rstd7_smart_173_raw',\n",
       "  'rmean7_smart_174_raw',\n",
       "  'rstd7_smart_174_raw',\n",
       "  'rmean7_smart_184_raw',\n",
       "  'rstd7_smart_184_raw',\n",
       "  'rmean7_smart_199_raw',\n",
       "  'rstd7_smart_199_raw',\n",
       "  'rmean7_smart_230_raw',\n",
       "  'rstd7_smart_230_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week',\n",
       "  'model_code',\n",
       "  'vendor_code']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_xgb_precision_v5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4e7661-7725-4dc7-994b-1c3de376fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "XGBOOST (GPU, PRECISION v72s4) - HDD\n",
      "Big-mode TRAIN=True | TEST streaming=True\n",
      "====================================================================================================\n",
      "Scanning earliest failure dates (streaming)...\n",
      "  Found 8,044 failed serials\n",
      "Loading BIG-FILTERED [2020, 2021, 2022, 2023] from ./Procesados/finales/HDD_FULL_CLEAN.parquet (keep_rate=0.0025)...\n",
      "  BIG-FILTERED kept 1,184,183/215,762,596 rows (~0.55%)\n",
      "TRAIN labels: 62,131 pos (5.2467%)\n",
      "\n",
      "Fold 1/5: train=948,389 (pos=49,699) | val=235,794 (pos=12,432)\n",
      "  After hard-neg sampling: 298,194 (pos=49,699, neg=248,495)\n",
      "  Fold 1 @thr=0.9993 | P=1.000 R=0.030 F1=0.058 PR-AUC=0.9008\n",
      "\n",
      "Fold 2/5: train=946,580 (pos=49,693) | val=237,603 (pos=12,438)\n",
      "  After hard-neg sampling: 298,158 (pos=49,693, neg=248,465)\n",
      "  Fold 2 @thr=0.9995 | P=0.995 R=0.030 F1=0.058 PR-AUC=0.9002\n",
      "\n",
      "Fold 3/5: train=948,183 (pos=49,752) | val=236,000 (pos=12,379)\n",
      "  After hard-neg sampling: 298,512 (pos=49,752, neg=248,760)\n",
      "  Fold 3 @thr=0.9981 | P=1.000 R=0.030 F1=0.058 PR-AUC=0.8970\n",
      "\n",
      "Fold 4/5: train=946,121 (pos=49,655) | val=238,062 (pos=12,476)\n",
      "  After hard-neg sampling: 297,930 (pos=49,655, neg=248,275)\n",
      "  Fold 4 @thr=0.9995 | P=1.000 R=0.030 F1=0.058 PR-AUC=0.9049\n",
      "\n",
      "Fold 5/5: train=947,459 (pos=49,725) | val=236,724 (pos=12,406)\n",
      "  After hard-neg sampling: 298,350 (pos=49,725, neg=248,625)\n",
      "  Fold 5 @thr=0.9982 | P=1.000 R=0.030 F1=0.058 PR-AUC=0.8997\n",
      "\n",
      "OOF (diag):\n",
      "{\n",
      "  \"precision\": 0.9989281886387996,\n",
      "  \"recall\": 0.030001126651751944,\n",
      "  \"f1\": 0.058252730596746724,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      1122050,\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      60267,\n",
      "      1864\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 1.7824485852705578e-06,\n",
      "  \"pr_auc\": 0.9002590602701519,\n",
      "  \"threshold\": 0.9992897510528564\n",
      "}\n",
      "\n",
      "✓ Final model saved: ./models_xgb_hdd_v72s4/HDD_xgb_v72s4_20251106_074841_model.pkl/.json\n",
      "\n",
      "TEST skipped (streaming test can be run later with evaluate_saved_model_2024_streaming_xgb).\n",
      "\n",
      "✓ Metadata saved: ./models_xgb_hdd_v72s4/HDD_xgb_v72s4_20251106_074841_metadata.json\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'HDD',\n",
       " 'train_years': [2020, 2021, 2022, 2023],\n",
       " 'dev_years': [],\n",
       " 'test_years': [2024],\n",
       " 'lookahead_days': 7,\n",
       " 'n_splits': 5,\n",
       " 'hard_negative_sampling': {'neg_pos_ratio': 5,\n",
       "  'hard_window': 90,\n",
       "  'hard_fraction': 0.7},\n",
       " 'xgb_params': {'tree_method': 'gpu_hist',\n",
       "  'predictor': 'gpu_predictor',\n",
       "  'objective': 'binary:logistic',\n",
       "  'eval_metric': 'aucpr',\n",
       "  'learning_rate': 0.06,\n",
       "  'max_depth': 8,\n",
       "  'min_child_weight': 10.0,\n",
       "  'subsample': 0.8,\n",
       "  'colsample_bytree': 0.6,\n",
       "  'reg_lambda': 1.0,\n",
       "  'reg_alpha': 0.0,\n",
       "  'max_bin': 256,\n",
       "  'enable_categorical': False,\n",
       "  'max_cat_to_onehot': 16,\n",
       "  'random_state': 42,\n",
       "  'nthread': 4,\n",
       "  'verbosity': 0,\n",
       "  'n_estimators': 1500},\n",
       " 'min_precision': 0.9,\n",
       " 'min_recall': 0.03,\n",
       " 'top_k_rate': 7.5e-05,\n",
       " 'min_alerts': 30,\n",
       " 'oof_metrics': {'precision': 0.9989281886387996,\n",
       "  'recall': 0.030001126651751944,\n",
       "  'f1': 0.058252730596746724,\n",
       "  'confusion_matrix': [[1122050, 2], [60267, 1864]],\n",
       "  'fpr': 1.7824485852705578e-06,\n",
       "  'pr_auc': 0.9002590602701519,\n",
       "  'threshold': 0.9992897510528564},\n",
       " 'dev_metrics': None,\n",
       " 'test_metrics': None,\n",
       " 'feature_names': ['smart_187_raw',\n",
       "  'smart_188_raw',\n",
       "  'smart_197_raw',\n",
       "  'smart_198_raw',\n",
       "  'smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'delta_smart_188_raw',\n",
       "  'max_smart_188_raw',\n",
       "  'delta_smart_197_raw',\n",
       "  'max_smart_197_raw',\n",
       "  'delta_smart_198_raw',\n",
       "  'max_smart_198_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week',\n",
       "  'model_code',\n",
       "  'vendor_code'],\n",
       " 'feature_style': 'codes_no_rolling',\n",
       " 'smart_cols': ['smart_187_raw',\n",
       "  'smart_188_raw',\n",
       "  'smart_197_raw',\n",
       "  'smart_198_raw',\n",
       "  'smart_5_raw']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hdd_precision_v72s4_xgb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514a616-85de-4afb-9922-8ef5ac112c83",
   "metadata": {},
   "source": [
    "# Demás Técnicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eede3dfe-5453-4a0d-ad55-373368aabe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "XGBOOST — PRECISION-CONTROLLED v7.2 (GPU) con estrategias de balanceo\n",
    "----------------------------------------------------------------------\n",
    "Refactor del pipeline CatBoost v7.2 hacia XGBoost manteniendo técnicas:\n",
    " - Join temporal TRAIN∪DEV∪TEST con features causales y normalización robusta por modelo\n",
    " - Folds estratificados por DISCO (Group K-Fold estratificado)\n",
    " - Muestreo de negativos \"duros\" (hard negatives) cercano al fallo\n",
    " - Balanceo: 'none' | 'under' | 'smote_knn' | 'smote_enn' (SMOTENC respeta categóricas model/vendor)\n",
    " - Umbral \"precision-first\" (min_precision/min_recall + fallback top-k/min_alerts)\n",
    " - Guardrails VRAM para RTX 2060 6GB (gpu_hist, single-precision, max_bin, subsample/colsample)\n",
    "\n",
    "Salida:\n",
    " - Métricas OOF + DEV + TEST\n",
    " - Modelos y metadata (JSON / PKL) con threshold para despliegue\n",
    "\n",
    "Requisitos:\n",
    " - pandas, numpy, pyarrow, scikit-learn, imblearn, xgboost>=2.0, joblib\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, json, warnings, pickle\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# XGBoost (GPU)\n",
    "import xgboost as xgb\n",
    "\n",
    "# Imbalance: under/over/combined (con soporte para categóricas)\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ===================== UTILS =====================\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"GC agresivo para liberar RAM/VRAM indirecta.\"\"\"\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# ===================== IO =====================\n",
    "\n",
    "def load_data(path: str, years: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"Carga .parquet (archivo o carpeta) y filtra por años en columna 'date'.\"\"\"\n",
    "    print(f\"Loading {years} from {path}...\")\n",
    "    chunks = []\n",
    "    if not path:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    files = []\n",
    "    if os.path.isdir(path):\n",
    "        files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.parquet')]\n",
    "    else:\n",
    "        files = [path]\n",
    "\n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for i in range(pf.num_row_groups):\n",
    "            df = pf.read_row_group(i).to_pandas()\n",
    "            if 'date' not in df.columns:\n",
    "                continue\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            df = df[df['date'].dt.year.isin(years)]\n",
    "            if len(df) > 0:\n",
    "                chunks.append(df)\n",
    "            if len(chunks) >= 20:\n",
    "                chunks = [pd.concat(chunks, ignore_index=True)]\n",
    "                cleanup()\n",
    "\n",
    "    result = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "    print(f\"Loaded {len(result):,} rows\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# ===================== PREP =====================\n",
    "\n",
    "def prepare_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Tipa columnas clave y ordena por (serial_number, date) de forma causal.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['serial_number'] = df['serial_number'].astype(str)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['serial_number', 'date'])\n",
    "    df = df.sort_values(['serial_number', 'date']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================== LABELS / DTF =====================\n",
    "\n",
    "def compute_days_to_failure(dfs: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Días hasta el primer fallo por disco (>=0 antes de fallo, <0 después/no fallo conocido).\n",
    "    \"\"\"\n",
    "    fail_map: Dict[str, pd.Timestamp] = {}\n",
    "    fails = dfs[dfs['failure'] == 1]\n",
    "    for sn, dt in zip(fails['serial_number'], fails['date']):\n",
    "        fail_map[sn] = min(dt, fail_map.get(sn, dt))\n",
    "\n",
    "    dtf = np.full(len(dfs), 1_000_000_000, dtype=np.int64)\n",
    "    for i, (sn, dt) in enumerate(zip(dfs['serial_number'], dfs['date'])):\n",
    "        if sn in fail_map:\n",
    "            dtf[i] = (fail_map[sn] - dt).days\n",
    "    return dtf\n",
    "\n",
    "\n",
    "def create_labels_from_dtf(dtf: np.ndarray, lookahead: int = 7) -> np.ndarray:\n",
    "    \"\"\"Etiqueta positiva si está dentro de [0, lookahead] días al fallo.\"\"\"\n",
    "    return ((dtf >= 0) & (dtf <= lookahead)).astype(np.int8)\n",
    "\n",
    "\n",
    "# ===================== CATEGORÍAS =====================\n",
    "\n",
    "def extract_vendor(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    return s.str.extract(r\"^([A-Za-z]+)\", expand=False).fillna(\"UNK\")\n",
    "\n",
    "\n",
    "# ===================== NORMALIZACIÓN POR MODELO =====================\n",
    "\n",
    "def fit_model_stats(df_train: pd.DataFrame, smart_cols: List[str]) -> Dict[str, Dict[str, Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Aprende mediana e IQR por (model, atributo) SOLO en TRAIN.\n",
    "    Devuelve: stats[model][col] = (median, iqr)\n",
    "    \"\"\"\n",
    "    stats: Dict[str, Dict[str, Tuple[float, float]]] = {}\n",
    "    if 'model' not in df_train.columns or df_train.empty:\n",
    "        return stats\n",
    "\n",
    "    for m, g in df_train.groupby(df_train['model'].astype(str), sort=False):\n",
    "        d = {}\n",
    "        for c in smart_cols:\n",
    "            x = pd.to_numeric(g[c], errors='coerce')\n",
    "            x = x[np.isfinite(x)]\n",
    "            if x.empty:\n",
    "                continue\n",
    "            med = float(np.median(x))\n",
    "            q1, q3 = np.percentile(x, [25, 75])\n",
    "            iqr = float(max(q3 - q1, 1e-6))\n",
    "            d[c] = (med, iqr)\n",
    "        stats[str(m)] = d\n",
    "    return stats\n",
    "\n",
    "\n",
    "def apply_model_normalization(df_all: pd.DataFrame, smart_cols: List[str], stats: Dict[str, Dict[str, Tuple[float, float]]]):\n",
    "    \"\"\"\n",
    "    Agrega features:\n",
    "    - z_{col}  = (col - median(model,col)) / (IQR(model,col)+eps)\n",
    "    - log1p_{col}\n",
    "    Vectorizado y robusto a modelos no vistos (fallback global).\n",
    "    \"\"\"\n",
    "    # Global fallback\n",
    "    global_median = {}\n",
    "    global_iqr = {}\n",
    "    for c in smart_cols:\n",
    "        x = pd.to_numeric(df_all[c], errors='coerce').fillna(0.0).values\n",
    "        global_median[c] = float(np.median(x))\n",
    "        q1, q3 = np.percentile(x, [25, 75])\n",
    "        global_iqr[c] = float(max(q3 - q1, 1e-6))\n",
    "\n",
    "    model_series = df_all['model'].astype(str)\n",
    "\n",
    "    for c in smart_cols:\n",
    "        med_map = {m: v[c][0] for m, v in stats.items() if c in v}\n",
    "        iqr_map = {m: v[c][1] for m, v in stats.items() if c in v}\n",
    "\n",
    "        med_s = model_series.map(med_map).fillna(global_median[c]).astype(np.float32)\n",
    "        iqr_s = model_series.map(iqr_map).fillna(global_iqr[c]).astype(np.float32)\n",
    "\n",
    "        col = pd.to_numeric(df_all[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "        z = (col - med_s) / (iqr_s + 1e-6)\n",
    "        df_all[f'z_{c}'] = z.values\n",
    "        df_all[f'log1p_{c}'] = np.log1p(np.maximum(col.values, 0.0)).astype(np.float32)\n",
    "\n",
    "\n",
    "# ===================== FEATURES (JOINED TRAIN∪DEV∪TEST) =====================\n",
    "\n",
    "def create_features_joined_xgb_v72(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_dev: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    dataset_type: str,\n",
    "    add_rolling: bool,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Une TRAIN∪DEV∪TEST, crea features causales por disco y marca 'model'/'vendor' como categóricas nativas.\n",
    "    Devuelve X_train, X_dev, X_test y timestamps, más nombres de features y lista de columnas categóricas.\n",
    "    Nota: XGBoost no acepta 'timestamp' como CatBoost; lo usamos sólo para info/diagnóstico.\n",
    "    \"\"\"\n",
    "    print(f\"Creating features (temporal join) for {dataset_type} with TRAIN∪DEV∪TEST...\")\n",
    "\n",
    "    df_train = df_train.copy(); df_train['__subset__'] = 'train'\n",
    "    df_dev   = df_dev.copy();   df_dev['__subset__']   = 'dev'\n",
    "    df_test  = df_test.copy();  df_test['__subset__']  = 'test'\n",
    "    df_all = pd.concat([df_train, df_dev, df_test], ignore_index=True)\n",
    "    df_all.sort_values(['serial_number','date'], inplace=True)\n",
    "\n",
    "    all_cols = df_all.columns.tolist()\n",
    "    smart_cols = [c for c in all_cols if ('smart' in c.lower()) and ('_raw' in c.lower())]\n",
    "    print(f\"  Found {len(smart_cols)} SMART attributes (raw)\")\n",
    "\n",
    "    for c in smart_cols:\n",
    "        df_all[c] = pd.to_numeric(df_all[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # Deltas / cummax por disco (causal)\n",
    "    for c in smart_cols:\n",
    "        g = df_all.groupby('serial_number', sort=False)[c]\n",
    "        df_all[f'delta_{c}'] = g.diff().fillna(0.0)\n",
    "        df_all[f'max_{c}']   = g.cummax()\n",
    "\n",
    "    # Rolling 7d\n",
    "    if add_rolling:\n",
    "        for c in smart_cols:\n",
    "            r = df_all.groupby('serial_number', sort=False)[c]\n",
    "            df_all[f'rmean7_{c}'] = r.rolling(window=7, min_periods=2).mean().reset_index(level=0, drop=True).fillna(0.0)\n",
    "            df_all[f'rstd7_{c}']  = r.rolling(window=7, min_periods=2).std().reset_index(level=0, drop=True).fillna(0.0)\n",
    "\n",
    "    # Edad y calendario\n",
    "    df_all['age_days'] = df_all.groupby('serial_number', sort=False).cumcount()\n",
    "    d = df_all['date']\n",
    "    df_all['month'] = d.dt.month\n",
    "    df_all['day_of_week'] = d.dt.dayofweek\n",
    "\n",
    "    # Categóricas nativas + vendor\n",
    "    if 'model' not in df_all.columns:\n",
    "        df_all['model'] = 'UNK'\n",
    "    df_all['model'] = df_all['model'].astype(str).fillna('UNK')\n",
    "    df_all['vendor'] = extract_vendor(df_all['model']).astype(str).fillna('UNK')\n",
    "\n",
    "    # Timestamp (solo para referencia; XGBoost no lo usa como parámetro)\n",
    "    df_all['ts_sec'] = (df_all['date'].astype('int64') // 10**9).astype(np.int64)\n",
    "\n",
    "    # Normalización por modelo aprendida en TRAIN\n",
    "    stats = fit_model_stats(df_train, smart_cols)\n",
    "    apply_model_normalization(df_all, smart_cols, stats)\n",
    "\n",
    "    # Drop no predictoras para X\n",
    "    drop_cols = ['serial_number', 'date', 'failure']\n",
    "    X_all = df_all.drop(columns=[c for c in drop_cols if c in df_all.columns], errors='ignore')\n",
    "\n",
    "    # Definición de categóricas nativas (XGBoost soporta dtype 'category' con enable_categorical=True)\n",
    "    cat_cols = [c for c in ['model','vendor'] if c in X_all.columns]\n",
    "    num_cols = [c for c in X_all.columns if c not in cat_cols + ['ts_sec']]\n",
    "\n",
    "    # Tipado numérico\n",
    "    for c in num_cols:\n",
    "        X_all[c] = pd.to_numeric(X_all[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "\n",
    "    # Elimina constantes\n",
    "    var = X_all[num_cols].var()\n",
    "    keep_num = var[var > 0].index.tolist()\n",
    "    X_all = pd.concat([X_all[cat_cols], X_all[keep_num], df_all[['ts_sec']]], axis=1)\n",
    "\n",
    "    # Asegura dtype categórico para XGBoost\n",
    "    for c in cat_cols:\n",
    "        X_all[c] = X_all[c].astype('category')\n",
    "\n",
    "    feature_names = list(X_all.columns)\n",
    "\n",
    "    # Split back\n",
    "    tr_mask  = (df_all['__subset__']=='train').values\n",
    "    dev_mask = (df_all['__subset__']=='dev').values\n",
    "    te_mask  = (df_all['__subset__']=='test').values\n",
    "\n",
    "    X_train = X_all.loc[tr_mask].reset_index(drop=True)\n",
    "    X_dev   = X_all.loc[dev_mask].reset_index(drop=True)\n",
    "    X_test  = X_all.loc[te_mask].reset_index(drop=True)\n",
    "\n",
    "    ts_train = X_train['ts_sec'].values.astype(np.int64)\n",
    "    ts_dev   = X_dev['ts_sec'].values.astype(np.int64)\n",
    "    ts_test  = X_test['ts_sec'].values.astype(np.int64)\n",
    "\n",
    "    # Quitar ts_sec de features (no se usa como predictor por defecto)\n",
    "    X_train = X_train.drop(columns=['ts_sec'])\n",
    "    X_dev   = X_dev.drop(columns=['ts_sec'])\n",
    "    X_test  = X_test.drop(columns=['ts_sec'])\n",
    "    feature_names.remove('ts_sec')\n",
    "\n",
    "    print(f\"  Final features: {X_train.shape[1]} (cat={len(cat_cols)}, num≈{len(keep_num)})\")\n",
    "    return X_train, X_dev, X_test, ts_train, ts_dev, ts_test, feature_names, cat_cols\n",
    "\n",
    "\n",
    "# ===================== GROUPED CV (por DISCO) =====================\n",
    "\n",
    "def make_group_folds(serials: pd.Series, y: np.ndarray, n_splits: int = 5, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Folds estratificados por disco (cada fold recibe discos con/ sin fallo).\n",
    "    \"\"\"\n",
    "    serials = serials.astype(str).values\n",
    "    uniq_serials, inverse = np.unique(serials, return_inverse=True)\n",
    "\n",
    "    y_disk = np.zeros(len(uniq_serials), dtype=np.int8)\n",
    "    for idx_row, disk_idx in enumerate(inverse):\n",
    "        if y[idx_row] == 1:\n",
    "            y_disk[disk_idx] = 1\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for tr_d, va_d in skf.split(uniq_serials, y_disk):\n",
    "        tr_mask = np.isin(inverse, tr_d)\n",
    "        va_mask = np.isin(inverse, va_d)\n",
    "        yield np.where(tr_mask)[0], np.where(va_mask)[0]\n",
    "\n",
    "\n",
    "# ===================== HARD NEGATIVES =====================\n",
    "\n",
    "def sample_negatives_hard(\n",
    "    X: pd.DataFrame, y: np.ndarray, dtf: np.ndarray, lookahead: int,\n",
    "    neg_pos_ratio: int = 3, hard_window: int = 60, hard_fraction: float = 0.7,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Selección de negativos con énfasis en los cercanos al fallo (dtf in (lookahead, hard_window]).\n",
    "    Devuelve subset (Xb, yb) + índices seleccionados (sel_idx) para diagnóstico.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    if len(pos_idx) == 0:\n",
    "        raise ValueError(\"No positives in training fold for hard-negative sampling\")\n",
    "\n",
    "    n_pos = len(pos_idx)\n",
    "    n_neg_needed = max(n_pos * neg_pos_ratio, 1)\n",
    "\n",
    "    hard_mask = (dtf > lookahead) & (dtf <= hard_window)\n",
    "    hard_idx = np.where((y == 0) & hard_mask)[0]\n",
    "    easy_idx = np.where((y == 0) & (~hard_mask))[0]\n",
    "\n",
    "    n_hard = min(int(n_neg_needed * hard_fraction), len(hard_idx))\n",
    "    n_easy = min(n_neg_needed - n_hard, len(easy_idx))\n",
    "\n",
    "    chosen_hard = rng.choice(hard_idx, size=n_hard, replace=False) if n_hard > 0 else np.empty(0, dtype=int)\n",
    "    chosen_easy = rng.choice(easy_idx, size=n_easy, replace=False) if n_easy > 0 else np.empty(0, dtype=int)\n",
    "\n",
    "    sel_idx = np.sort(np.concatenate([pos_idx, chosen_hard, chosen_easy]))\n",
    "    Xb = X.iloc[sel_idx].reset_index(drop=True)\n",
    "    yb = y[sel_idx]\n",
    "    return Xb, yb, sel_idx\n",
    "\n",
    "\n",
    "# ===================== BALANCING =====================\n",
    "\n",
    "def _encode_categoricals_for_smote(X: pd.DataFrame, cat_cols: List[str]):\n",
    "    \"\"\"\n",
    "    Codifica categóricas a códigos enteros por columna (mapas por col).\n",
    "    Devuelve X_num (DataFrame), mapas enc/dec y lista de índices categóricos.\n",
    "    \"\"\"\n",
    "    Xc = X.copy()\n",
    "    enc_maps = {}\n",
    "    dec_maps = {}\n",
    "    for c in cat_cols:\n",
    "        cat = pd.Categorical(Xc[c].astype(str).fillna(\"UNK\"))\n",
    "        codes = cat.codes.astype(np.int64)\n",
    "        codes = np.where(codes < 0, cat.categories.size, codes)  # remapea -1 a \"__UNK__\"\n",
    "        Xc[c] = codes\n",
    "        enc_maps[c] = {k: i for i, k in enumerate(list(cat.categories) + [\"__UNK__\"])}\n",
    "        dec_maps[c] = {i: k for k, i in enc_maps[c].items()}\n",
    "    cat_indices = [Xc.columns.get_loc(c) for c in cat_cols]\n",
    "    return Xc, enc_maps, dec_maps, cat_indices\n",
    "\n",
    "\n",
    "def _decode_categoricals_after_smote(X_res: pd.DataFrame, dec_maps: Dict[str, Dict[int, str]], cat_cols: List[str]):\n",
    "    \"\"\"Convierte códigos enteros devueltos por SMOTENC a strings originales.\"\"\"\n",
    "    Xd = X_res.copy()\n",
    "    for c in cat_cols:\n",
    "        inv = dec_maps[c]\n",
    "        vals = Xd[c].astype(int).values\n",
    "        Xd[c] = [inv.get(int(v), \"__UNK__\") for v in vals]\n",
    "    return Xd\n",
    "\n",
    "\n",
    "def balance_with_strategy(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    cat_cols: List[str],\n",
    "    strategy: str = \"under\",\n",
    "    neg_pos_ratio: int = 3,        # objetivo final NEG:POS\n",
    "    smote_k_neighbors: int = 5,\n",
    "    enn_k_neighbors: int = 3,\n",
    "    random_state: int = 42,\n",
    "    max_total_samples: Optional[int] = None,\n",
    "):\n",
    "    if strategy not in {\"none\", \"under\", \"smote_knn\", \"smote_enn\"}:\n",
    "        raise ValueError(f\"Unknown balancing strategy: {strategy}\")\n",
    "\n",
    "    if strategy == \"none\":\n",
    "        return X.reset_index(drop=True), y.copy()\n",
    "\n",
    "    n_pos = int((y == 1).sum())\n",
    "    n_neg = int((y == 0).sum())\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return X.reset_index(drop=True), y.copy()\n",
    "\n",
    "    desired_neg_pos = max(1, int(neg_pos_ratio))\n",
    "    desired_pos_over_neg = 1.0 / float(desired_neg_pos)  # pos/neg deseado\n",
    "    cur_pos_over_neg = n_pos / float(n_neg)\n",
    "    eps = 1e-6\n",
    "\n",
    "    # ---------------- UNDER-SAMPLING ----------------\n",
    "    if strategy == \"under\":\n",
    "        target_neg = min(n_neg, int(round(n_pos * desired_neg_pos)))\n",
    "        target_pos = n_pos\n",
    "        if max_total_samples:\n",
    "            per_pos = max(1, max_total_samples // (1 + desired_neg_pos))\n",
    "            target_pos = min(n_pos, per_pos)\n",
    "            target_neg = min(n_neg, per_pos * desired_neg_pos)\n",
    "        rus = RandomUnderSampler(\n",
    "            sampling_strategy={0: int(target_neg), 1: int(target_pos)},\n",
    "            random_state=random_state\n",
    "        )\n",
    "        X_res, y_res = rus.fit_resample(X, y)\n",
    "        return pd.DataFrame(X_res, columns=X.columns).reset_index(drop=True), y_res.astype(np.int8)\n",
    "\n",
    "    # ---------------- SMOTENC / SMOTEENN ----------------\n",
    "    # Guardia 1: si ya estamos en (o por encima de) el ratio objetivo, NO oversamplear.\n",
    "    if cur_pos_over_neg + eps >= desired_pos_over_neg:\n",
    "        # Opciones: (a) no tocar, (b) ajustar por under a ratio exacto. Tomamos (b) para consistencia.\n",
    "        target_neg = min(n_neg, int(round(n_pos * desired_neg_pos)))\n",
    "        rus = RandomUnderSampler(\n",
    "            sampling_strategy={0: int(target_neg), 1: int(n_pos)},\n",
    "            random_state=random_state\n",
    "        )\n",
    "        X_res, y_res = rus.fit_resample(X, y)\n",
    "        X_res = pd.DataFrame(X_res, columns=X.columns)\n",
    "        # Asegura dtypes tras el re-sample\n",
    "        for c in X.columns:\n",
    "            if c in cat_cols:\n",
    "                X_res[c] = pd.Categorical(X_res[c]).astype('category')\n",
    "            else:\n",
    "                X_res[c] = pd.to_numeric(X_res[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "        return X_res.reset_index(drop=True), y_res.astype(np.int8)\n",
    "\n",
    "    # Guardia 2: asegurar que SMOTE realmente genere al menos 1 muestra\n",
    "    target_min = max(n_pos + 1, int(np.ceil(desired_pos_over_neg * n_neg)))\n",
    "    sampling_strategy = min(1.0, target_min / float(n_neg))\n",
    "\n",
    "    # Encodifica categóricas a enteros para SMOTENC\n",
    "    X_enc = X.copy()\n",
    "    enc_maps = {}; dec_maps = {}; cat_idx = []\n",
    "    for c in cat_cols:\n",
    "        cat = pd.Categorical(X_enc[c].astype(str).fillna(\"UNK\"))\n",
    "        codes = cat.codes.astype(np.int64)\n",
    "        codes = np.where(codes < 0, cat.categories.size, codes)  # -1 -> \"__UNK__\"\n",
    "        X_enc[c] = codes\n",
    "        enc_maps[c] = {k: i for i, k in enumerate(list(cat.categories) + [\"__UNK__\"])}\n",
    "        dec_maps[c] = {i: k for k, i in enc_maps[c].items()}\n",
    "        cat_idx.append(X_enc.columns.get_loc(c))\n",
    "\n",
    "    # Clamping de k_neighbors a n_pos-1 para evitar ValueError\n",
    "    k_smote = max(1, min(smote_k_neighbors, n_pos - 1))\n",
    "    smote = SMOTENC(\n",
    "        categorical_features=cat_idx,\n",
    "        sampling_strategy=float(sampling_strategy),\n",
    "        k_neighbors=k_smote,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    if strategy == \"smote_knn\":\n",
    "        X_res, y_res = smote.fit_resample(X_enc.values, y)\n",
    "    else:  # smote_enn\n",
    "        enn = EditedNearestNeighbours(n_neighbors=enn_k_neighbors)\n",
    "        comb = SMOTEENN(smote=smote, enn=enn)\n",
    "        X_res, y_res = comb.fit_resample(X_enc.values, y)\n",
    "\n",
    "    # Tope de tamaño si aplica\n",
    "    if max_total_samples and len(y_res) > max_total_samples:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        pos_idx = np.where(y_res == 1)[0]; neg_idx = np.where(y_res == 0)[0]\n",
    "        keep_pos = min(len(pos_idx), max_total_samples // (1 + desired_neg_pos))\n",
    "        keep_neg = min(len(neg_idx), keep_pos * desired_neg_pos)\n",
    "        sel_pos = rng.choice(pos_idx, size=keep_pos, replace=False)\n",
    "        sel_neg = rng.choice(neg_idx, size=keep_neg, replace=False)\n",
    "        sel = np.sort(np.concatenate([sel_pos, sel_neg]))\n",
    "        X_res = X_res[sel]; y_res = y_res[sel]\n",
    "\n",
    "    # Decodifica categóricas de vuelta y fija dtypes\n",
    "    X_res_df = pd.DataFrame(X_res, columns=X_enc.columns)\n",
    "    for c in cat_cols:\n",
    "        inv = dec_maps[c]; X_res_df[c] = [inv.get(int(v), \"__UNK__\") for v in X_res_df[c].astype(int).values]\n",
    "        X_res_df[c] = pd.Categorical(X_res_df[c]).astype('category')\n",
    "    for c in X.columns:\n",
    "        if c not in cat_cols:\n",
    "            X_res_df[c] = pd.to_numeric(X_res_df[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "\n",
    "    return X_res_df.reset_index(drop=True), y_res.astype(np.int8)\n",
    "\n",
    "\n",
    "\n",
    "# ===================== XGBOOST (GPU) =====================\n",
    "\n",
    "def get_xgb_default_params(\n",
    "    random_state: int = 42,\n",
    "    enable_categorical: bool = True,\n",
    "    max_depth: int = 8,\n",
    "    n_estimators: int = 1500,\n",
    "    learning_rate: float = 0.06,\n",
    "    min_child_weight: float = 8.0,\n",
    "    subsample: float = 0.8,\n",
    "    colsample_bytree: float = 0.6,\n",
    "    reg_lambda: float = 1.0,\n",
    "    reg_alpha: float = 0.0,\n",
    "    max_bin: int = 256,\n",
    "    max_cat_to_onehot: int = 16,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parámetros por defecto seguros para RTX 2060 6GB:\n",
    "    - gpu_hist + single precision -> menor VRAM\n",
    "    - max_bin moderado, subsamples para reducir memoria\n",
    "    - enable_categorical para tratar model/vendor como categóricas nativas\n",
    "    \"\"\"\n",
    "    params = dict(\n",
    "        # Core\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_lambda=reg_lambda,\n",
    "        reg_alpha=reg_alpha,\n",
    "        max_bin=max_bin,\n",
    "        # Categóricas\n",
    "        enable_categorical=enable_categorical,\n",
    "        max_cat_to_onehot=max_cat_to_onehot,\n",
    "        # Miscelánea\n",
    "        random_state=random_state,\n",
    "        nthread=4,\n",
    "        verbosity=0,\n",
    "    )\n",
    "    return params, int(n_estimators)\n",
    "\n",
    "\n",
    "class XGBBoosterWrapper:\n",
    "    \"\"\"Wrapper sobre xgb.Booster con predict_proba robusto a versiones/early stopping.\"\"\"\n",
    "    def __init__(self, booster: xgb.Booster):\n",
    "        self.booster = booster\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame | np.ndarray) -> np.ndarray:\n",
    "        # Asegura dtype category en columnas categóricas si vienen como object\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X2 = X.copy()\n",
    "            for c in X2.columns:\n",
    "                if str(X2[c].dtype) == 'object' and X2[c].nunique() < 10_000:\n",
    "                    X2[c] = X2[c].astype('category')\n",
    "            d = xgb.DMatrix(X2, enable_categorical=True)\n",
    "        else:\n",
    "            d = xgb.DMatrix(X)\n",
    "\n",
    "        best_it = getattr(self.booster, \"best_iteration\", None)\n",
    "        if best_it is not None:\n",
    "            try:\n",
    "                pred = self.booster.predict(d, iteration_range=(0, int(best_it) + 1))\n",
    "            except TypeError:\n",
    "                ntree_limit = getattr(self.booster, \"best_ntree_limit\", int(best_it) + 1)\n",
    "                pred = self.booster.predict(d, ntree_limit=int(ntree_limit))\n",
    "        else:\n",
    "            pred = self.booster.predict(d)\n",
    "        return np.vstack([1 - pred, pred]).T\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {'raw': self.booster.save_raw()}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        bst = xgb.Booster()\n",
    "        bst.load_model(bytearray(state['raw']))\n",
    "        self.booster = bst\n",
    "\n",
    "\n",
    "def train_xgb_gpu(\n",
    "    X_tr: pd.DataFrame, y_tr: np.ndarray,\n",
    "    X_va: Optional[pd.DataFrame] = None, y_va: Optional[np.ndarray] = None,\n",
    "    params: Optional[Dict] = None, n_estimators: Optional[int] = None,\n",
    "    early_stopping_rounds: int = 200, scale_pos_weight: Optional[float] = None\n",
    ") -> XGBBoosterWrapper:\n",
    "    \"\"\"\n",
    "    Entrena XGBoost GPU con soporte de categóricas nativas (pandas.Categorical).\n",
    "    - scale_pos_weight se usa si el set está desbalanceado (p.ej. strategy='none').\n",
    "    \"\"\"\n",
    "    base_params, base_n = get_xgb_default_params()\n",
    "    if params:\n",
    "        base_params.update(params)\n",
    "    if n_estimators is None:\n",
    "        n_estimators = base_n\n",
    "    if scale_pos_weight is not None:\n",
    "        base_params['scale_pos_weight'] = float(scale_pos_weight)\n",
    "\n",
    "    # Asegura dtype category en columnas categóricas\n",
    "    X_tr2 = X_tr.copy()\n",
    "    if isinstance(X_tr2, pd.DataFrame):\n",
    "        for c in X_tr2.columns:\n",
    "            if str(X_tr2[c].dtype) == 'object':\n",
    "                X_tr2[c] = X_tr2[c].astype('category')\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr2, label=y_tr, enable_categorical=True)\n",
    "    evals = []\n",
    "    if X_va is not None and y_va is not None:\n",
    "        X_va2 = X_va.copy()\n",
    "        for c in X_va2.columns:\n",
    "            if str(X_va2[c].dtype) == 'object':\n",
    "                X_va2[c] = X_va2[c].astype('category')\n",
    "        dvalid = xgb.DMatrix(X_va2, label=y_va, enable_categorical=True)\n",
    "        evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=base_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=int(n_estimators),\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=early_stopping_rounds if evals else 0,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    return XGBBoosterWrapper(booster)\n",
    "\n",
    "\n",
    "# ===================== THRESHOLDING & EVAL =====================\n",
    "\n",
    "def pick_threshold_precision_first(\n",
    "    y_true: np.ndarray, proba: np.ndarray,\n",
    "    min_precision: float = 0.90, min_recall: float = 0.03,\n",
    "    top_k_rate: float = 1e-4, min_alerts: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Selección de umbral:\n",
    "      1) prioridad a cumplir precisión y recall mínimos (elige el umbral mayor que cumpla)\n",
    "      2) fallback a top-k por tasa (o mínimo de alertas)\n",
    "    \"\"\"\n",
    "    precision, recall, thr = precision_recall_curve(y_true, proba)\n",
    "    pr_auc = average_precision_score(y_true, proba)\n",
    "\n",
    "    valid = (precision >= min_precision) & (recall >= min_recall)\n",
    "    if valid.any():\n",
    "        idxs = np.where(valid)[0]\n",
    "        idx = idxs[-1] - 1 if idxs[-1] >= len(thr) else idxs[-1]\n",
    "        idx = max(0, min(idx, len(thr)-1))\n",
    "        chosen = thr[idx]\n",
    "    else:\n",
    "        k = max(max(1, min_alerts), int(len(proba) * max(top_k_rate, 1e-6)))\n",
    "        chosen = float(np.partition(proba, -k)[-k])\n",
    "\n",
    "    return float(chosen), float(pr_auc)\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true: np.ndarray, proba: np.ndarray, thr: float) -> Dict:\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        'confusion_matrix': [[int(tn), int(fp)], [int(fn), int(tp)]],\n",
    "        'fpr': float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "# ===================== MAIN (TRAIN 20–22, DEV 23, TEST 24/25) =====================\n",
    "\n",
    "def train_xgb_precision_pipeline_v72(\n",
    "    train_parquet: str,\n",
    "    test_parquet: str | None,\n",
    "    dataset_type: str,\n",
    "    train_years: List[int] = [2020, 2021, 2022, 2023],\n",
    "    dev_years: List[int]   = [2024],\n",
    "    test_years: List[int]  = [2025],\n",
    "    lookahead_days: int = 7,\n",
    "    n_splits: int = 5,\n",
    "\n",
    "    # HARD-NEG sampling\n",
    "    neg_pos_ratio: int = 3,        # SSD=3, HDD=5 recomendado\n",
    "    hard_window: int = 60,         # HDD=90\n",
    "    hard_fraction: float = 0.7,\n",
    "\n",
    "    # BALANCING (tras hard-neg subset)\n",
    "    balancing: str = \"under\",      # 'none' | 'under' | 'smote_knn' | 'smote_enn'\n",
    "    balancing_neg_pos_ratio: int = 3,   # objetivo final neg:pos\n",
    "    smote_k_neighbors: int = 5,\n",
    "    enn_k_neighbors: int = 3,\n",
    "    max_balanced_samples: Optional[int] = None,  # p.ej., 40_000\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_params: Optional[Dict] = None,  # override opcional\n",
    "    xgb_n_estimators: int = 1500,\n",
    "    early_stopping_rounds: int = 200,\n",
    "\n",
    "    # Objetivos de despliegue (umbral)\n",
    "    min_precision: float = 0.90,\n",
    "    min_recall: float = 0.03,\n",
    "    top_k_rate: float = 1e-4,\n",
    "    min_alerts: int = 20,\n",
    "\n",
    "    output_dir: str = './models_xgb_precision_v72',\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena XGBoost (GPU) con control de precisión y estrategias de balanceo configurables.\n",
    "    \"\"\"\n",
    "    print(\"=\"*92)\n",
    "    print(f\"XGBOOST (GPU, PRECISION-CONTROLLED v7.2) - {dataset_type.upper()} \"\n",
    "          f\"(Train={train_years} | Dev={dev_years} | Test={test_years})\")\n",
    "    print(f\"Balancing: {balancing} (target neg:pos={balancing_neg_pos_ratio})\")\n",
    "    print(\"=\"*92)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df_tr_raw = load_data(train_parquet, train_years)\n",
    "    df_dev_raw = load_data(train_parquet, dev_years) if dev_years else pd.DataFrame()\n",
    "    df_te_raw  = load_data(test_parquet, test_years) if (test_parquet and test_years) else pd.DataFrame()\n",
    "    if df_tr_raw.empty:\n",
    "        raise ValueError(\"Training data is empty!\")\n",
    "\n",
    "    # ---------- Prepare ----------\n",
    "    df_tr  = prepare_df(df_tr_raw)\n",
    "    df_dev = prepare_df(df_dev_raw) if not df_dev_raw.empty else pd.DataFrame()\n",
    "    df_te  = prepare_df(df_te_raw) if not df_te_raw.empty else pd.DataFrame()\n",
    "\n",
    "    # ---------- Labels ----------\n",
    "    dtf_tr  = compute_days_to_failure(df_tr)\n",
    "    y_tr    = create_labels_from_dtf(dtf_tr, lookahead_days)\n",
    "    print(f\"TRAIN labels: {int(y_tr.sum()):,} pos ({100*y_tr.mean():.4f}%)\")\n",
    "    if y_tr.sum() < 50:\n",
    "        raise ValueError(f\"Insufficient positive samples in TRAIN: {y_tr.sum()}\")\n",
    "\n",
    "    dtf_dev = compute_days_to_failure(df_dev) if not df_dev.empty else np.array([], dtype=np.int64)\n",
    "    y_dev   = create_labels_from_dtf(dtf_dev, lookahead_days) if not df_dev.empty else np.array([], dtype=np.int8)\n",
    "\n",
    "    # ---------- Features (join temporal) ----------\n",
    "    X_tr, X_dev, X_te, ts_tr, ts_dev, ts_te, feature_names, cat_cols = create_features_joined_xgb_v72(\n",
    "        df_tr, df_dev, df_te, dataset_type, add_rolling=True\n",
    "    )\n",
    "\n",
    "    # ---------- CV por disco en TRAIN (OOF diagnóstico) ----------\n",
    "    serials_tr = df_tr['serial_number']\n",
    "    oof_proba = np.zeros(len(y_tr), dtype=np.float32)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(make_group_folds(serials_tr, y_tr, n_splits=n_splits, random_state=random_state), start=1):\n",
    "        X_tr_fold, y_tr_fold = X_tr.iloc[tr_idx].reset_index(drop=True), y_tr[tr_idx]\n",
    "        dtf_tr_fold = dtf_tr[tr_idx]\n",
    "\n",
    "        X_va_fold, y_va_fold = X_tr.iloc[va_idx].reset_index(drop=True), y_tr[va_idx]\n",
    "\n",
    "        print(f\"\\nFold {fold}/{n_splits}: train={len(y_tr_fold):,} (pos={int(y_tr_fold.sum()):,}) | \"\n",
    "              f\"val={len(y_va_fold):,} (pos={int(y_va_fold.sum()):,})\")\n",
    "\n",
    "        # 1) Hard-negatives para controlar tamaño y acercar al borde\n",
    "        Xb, yb, sel_idx = sample_negatives_hard(\n",
    "            X_tr_fold, y_tr_fold, dtf_tr_fold,\n",
    "            lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio,\n",
    "            hard_window=hard_window if dataset_type.upper()=='SSD' else max(hard_window, 90),\n",
    "            hard_fraction=hard_fraction,\n",
    "            seed=random_state,\n",
    "        )\n",
    "        print(f\"  After hard-neg sampling: {len(yb):,} (pos={int(yb.sum()):,}, neg={len(yb)-int(yb.sum()):,})\")\n",
    "\n",
    "        # 2) Balanceo elegido (UNDER/SMOTENC/SMOTEENN/none)\n",
    "        X_bal, y_bal = balance_with_strategy(\n",
    "            Xb, yb, cat_cols=cat_cols,\n",
    "            strategy=balancing,\n",
    "            neg_pos_ratio=balancing_neg_pos_ratio,\n",
    "            smote_k_neighbors=smote_k_neighbors,\n",
    "            enn_k_neighbors=enn_k_neighbors,\n",
    "            random_state=random_state,\n",
    "            max_total_samples=max_balanced_samples,\n",
    "        )\n",
    "        # scale_pos_weight si no balanceamos (mantener info de skew para XGB)\n",
    "        spw = None\n",
    "        if balancing == 'none':\n",
    "            n_pos = max(1, int((y_bal == 1).sum()))\n",
    "            n_neg = max(1, int((y_bal == 0).sum()))\n",
    "            spw = float(n_neg / n_pos)\n",
    "\n",
    "        print(f\"  After balancing [{balancing}]: {len(y_bal):,} (pos={int(y_bal.sum()):,}, neg={len(y_bal)-int(y_bal.sum()):,})\")\n",
    "\n",
    "        # 3) XGBoost\n",
    "        model = train_xgb_gpu(\n",
    "            X_bal, y_bal, X_va_fold, y_va_fold,\n",
    "            params=xgb_params, n_estimators=xgb_n_estimators,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            scale_pos_weight=spw\n",
    "        )\n",
    "\n",
    "        proba_va = model.predict_proba(X_va_fold)[:, 1]\n",
    "        oof_proba[va_idx] = proba_va\n",
    "\n",
    "        thr_oof, pr_auc = pick_threshold_precision_first(\n",
    "            y_va_fold, proba_va, min_precision=min_precision, min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "        )\n",
    "        m = metrics_at_threshold(y_va_fold, proba_va, thr_oof)\n",
    "        m.update({'pr_auc': float(pr_auc), 'threshold': float(thr_oof), 'fold': int(fold)})\n",
    "        fold_metrics.append(m)\n",
    "        print(f\"  Fold {fold} @thr={thr_oof:.4f} | P={m['precision']:.3f} R={m['recall']:.3f} F1={m['f1']:.3f} PR-AUC={pr_auc:.4f}\")\n",
    "\n",
    "        del X_tr_fold, y_tr_fold, X_va_fold, y_va_fold, Xb, yb, X_bal, y_bal, model\n",
    "        cleanup()\n",
    "\n",
    "    # ---------- OOF (diag, no se usa para prod) ----------\n",
    "    thr_oof_global, pr_auc_oof = pick_threshold_precision_first(\n",
    "        y_tr, oof_proba, min_precision=min_precision, min_recall=min_recall,\n",
    "        top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "    )\n",
    "    agg_metrics = metrics_at_threshold(y_tr, oof_proba, thr_oof_global)\n",
    "    agg_metrics.update({'pr_auc': float(pr_auc_oof), 'threshold': float(thr_oof_global)})\n",
    "    print(\"\\nOOF (diag, not used for prod):\")\n",
    "    print(json.dumps(agg_metrics, indent=2))\n",
    "\n",
    "    # ---------- Calibración UMBRAL en DEV ----------\n",
    "    thr_prod = None\n",
    "    dev_metrics = None\n",
    "    if not df_dev.empty:\n",
    "        Xb_full, yb_full, _ = sample_negatives_hard(\n",
    "            X_tr, y_tr, dtf_tr, lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio, hard_window=hard_window if dataset_type.upper()=='SSD' else max(hard_window,90),\n",
    "            hard_fraction=hard_fraction, seed=random_state\n",
    "        )\n",
    "        X_bal_full, y_bal_full = balance_with_strategy(\n",
    "            Xb_full, yb_full, cat_cols=cat_cols, strategy=balancing,\n",
    "            neg_pos_ratio=balancing_neg_pos_ratio,\n",
    "            smote_k_neighbors=smote_k_neighbors,\n",
    "            enn_k_neighbors=enn_k_neighbors,\n",
    "            random_state=random_state,\n",
    "            max_total_samples=max_balanced_samples,\n",
    "        )\n",
    "        spw = None\n",
    "        if balancing == 'none':\n",
    "            n_pos = max(1, int((y_bal_full == 1).sum()))\n",
    "            n_neg = max(1, int((y_bal_full == 0).sum()))\n",
    "            spw = float(n_neg / n_pos)\n",
    "\n",
    "        final_model_dev = train_xgb_gpu(\n",
    "            X_bal_full, y_bal_full, params=xgb_params, n_estimators=xgb_n_estimators,\n",
    "            early_stopping_rounds=0, scale_pos_weight=spw\n",
    "        )\n",
    "\n",
    "        proba_dev = final_model_dev.predict_proba(X_dev)[:, 1]\n",
    "        thr_prod, pr_auc_dev = pick_threshold_precision_first(\n",
    "            y_dev, proba_dev, min_precision=min_precision, min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "        )\n",
    "        dev_metrics = metrics_at_threshold(y_dev, proba_dev, thr_prod)\n",
    "        dev_metrics.update({'pr_auc': float(pr_auc_dev), 'threshold': float(thr_prod)})\n",
    "        print(\"\\nDEV calibration metrics (used for PROD threshold):\")\n",
    "        print(json.dumps(dev_metrics, indent=2))\n",
    "\n",
    "        del final_model_dev\n",
    "        cleanup()\n",
    "    else:\n",
    "        thr_prod = float(thr_oof_global)\n",
    "        dev_metrics = None\n",
    "\n",
    "    # ---------- TEST ----------\n",
    "    test_metrics = None\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_prefix = os.path.join(output_dir, f\"{dataset_type}_xgb_precv72_{balancing}_{timestamp}\")\n",
    "\n",
    "    if not df_te.empty:\n",
    "        Xb_full, yb_full, _ = sample_negatives_hard(\n",
    "            X_tr, y_tr, dtf_tr, lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio, hard_window=hard_window if dataset_type.upper()=='SSD' else max(hard_window,90),\n",
    "            hard_fraction=hard_fraction, seed=random_state\n",
    "        )\n",
    "        X_bal_full, y_bal_full = balance_with_strategy(\n",
    "            Xb_full, yb_full, cat_cols=cat_cols, strategy=balancing,\n",
    "            neg_pos_ratio=balancing_neg_pos_ratio,\n",
    "            smote_k_neighbors=smote_k_neighbors,\n",
    "            enn_k_neighbors=enn_k_neighbors,\n",
    "            random_state=random_state,\n",
    "            max_total_samples=max_balanced_samples,\n",
    "        )\n",
    "        spw = None\n",
    "        if balancing == 'none':\n",
    "            n_pos = max(1, int((y_bal_full == 1).sum()))\n",
    "            n_neg = max(1, int((y_bal_full == 0).sum()))\n",
    "            spw = float(n_neg / n_pos)\n",
    "\n",
    "        final_model = train_xgb_gpu(\n",
    "            X_bal_full, y_bal_full, params=xgb_params, n_estimators=xgb_n_estimators,\n",
    "            early_stopping_rounds=0, scale_pos_weight=spw\n",
    "        )\n",
    "\n",
    "        y_test = create_labels_from_dtf(compute_days_to_failure(df_te), lookahead_days)\n",
    "        proba_test = final_model.predict_proba(X_te)[:, 1]\n",
    "\n",
    "        thr_used = float(thr_prod)\n",
    "        test_metrics = metrics_at_threshold(y_test, proba_test, thr_used)\n",
    "        test_metrics.update({'pr_auc': float(average_precision_score(y_test, proba_test)), 'threshold_used': thr_used})\n",
    "\n",
    "        # Persistencia\n",
    "        joblib.dump(final_model, f\"{model_prefix}_model.pkl\")\n",
    "        final_model.booster.save_model(f\"{model_prefix}_model.json\")\n",
    "        with open(f\"{model_prefix}_features.json\", 'w') as f:\n",
    "            json.dump({\n",
    "                'feature_names': feature_names,\n",
    "                'cat_cols': cat_cols,\n",
    "                'threshold': float(thr_used),\n",
    "                'calibration': 'dev_years',\n",
    "                'balancing': {\n",
    "                    'strategy': balancing,\n",
    "                    'neg_pos_ratio': balancing_neg_pos_ratio,\n",
    "                    'smote_k_neighbors': smote_k_neighbors,\n",
    "                    'enn_k_neighbors': enn_k_neighbors,\n",
    "                    'max_balanced_samples': max_balanced_samples\n",
    "                },\n",
    "                'xgb_params': (xgb_params or get_xgb_default_params()[0])\n",
    "            }, f, indent=2)\n",
    "        print(f\"\\n✓ Final model saved: {model_prefix}_model.pkl/.json\")\n",
    "\n",
    "        del final_model\n",
    "        cleanup()\n",
    "    else:\n",
    "        # Train-only artefacts\n",
    "        Xb_full, yb_full, _ = sample_negatives_hard(\n",
    "            X_tr, y_tr, dtf_tr, lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio, hard_window=hard_window if dataset_type.upper()=='SSD' else max(hard_window,90),\n",
    "            hard_fraction=hard_fraction, seed=random_state\n",
    "        )\n",
    "        X_bal_full, y_bal_full = balance_with_strategy(\n",
    "            Xb_full, yb_full, cat_cols=cat_cols, strategy=balancing,\n",
    "            neg_pos_ratio=balancing_neg_pos_ratio,\n",
    "            smote_k_neighbors=smote_k_neighbors,\n",
    "            enn_k_neighbors=enn_k_neighbors,\n",
    "            random_state=random_state,\n",
    "            max_total_samples=max_balanced_samples,\n",
    "        )\n",
    "        spw = None\n",
    "        if balancing == 'none':\n",
    "            n_pos = max(1, int((y_bal_full == 1).sum()))\n",
    "            n_neg = max(1, int((y_bal_full == 0).sum()))\n",
    "            spw = float(n_neg / n_pos)\n",
    "\n",
    "        final_model = train_xgb_gpu(\n",
    "            X_bal_full, y_bal_full, params=xgb_params, n_estimators=xgb_n_estimators,\n",
    "            early_stopping_rounds=0, scale_pos_weight=spw\n",
    "        )\n",
    "        joblib.dump(final_model, f\"{model_prefix}_model.pkl\")\n",
    "        final_model.booster.save_model(f\"{model_prefix}_model.json\")\n",
    "        with open(f\"{model_prefix}_features.json\", 'w') as f:\n",
    "            json.dump({\n",
    "                'feature_names': feature_names,\n",
    "                'cat_cols': cat_cols,\n",
    "                'threshold': float(thr_oof_global),\n",
    "                'calibration': 'oof_global',\n",
    "                'balancing': {\n",
    "                    'strategy': balancing,\n",
    "                    'neg_pos_ratio': balancing_neg_pos_ratio,\n",
    "                    'smote_k_neighbors': smote_k_neighbors,\n",
    "                    'enn_k_neighbors': enn_k_neighbors,\n",
    "                    'max_balanced_samples': max_balanced_samples\n",
    "                },\n",
    "                'xgb_params': (xgb_params or get_xgb_default_params()[0])\n",
    "            }, f, indent=2)\n",
    "        print(f\"\\n✓ Final model saved (train-only): {model_prefix}_model.pkl/.json\")\n",
    "\n",
    "    # ---------- Metadata ----------\n",
    "    metadata = {\n",
    "        'dataset_type': dataset_type,\n",
    "        'train_years': train_years,\n",
    "        'dev_years': dev_years,\n",
    "        'test_years': test_years,\n",
    "        'lookahead_days': lookahead_days,\n",
    "        'n_splits': n_splits,\n",
    "        'hard_negative_sampling': {\n",
    "            'neg_pos_ratio': neg_pos_ratio,\n",
    "            'hard_window': hard_window,\n",
    "            'hard_fraction': hard_fraction\n",
    "        },\n",
    "        'balancing': {\n",
    "            'strategy': balancing,\n",
    "            'neg_pos_ratio': balancing_neg_pos_ratio,\n",
    "            'smote_k_neighbors': smote_k_neighbors,\n",
    "            'enn_k_neighbors': enn_k_neighbors,\n",
    "            'max_balanced_samples': max_balanced_samples\n",
    "        },\n",
    "        'xgb_params': (xgb_params or get_xgb_default_params()[0]) | {'n_estimators': xgb_n_estimators},\n",
    "        'min_precision': min_precision,\n",
    "        'min_recall': min_recall,\n",
    "        'top_k_rate': top_k_rate,\n",
    "        'min_alerts': min_alerts,\n",
    "        'oof_metrics': {\n",
    "            'precision': agg_metrics['precision'],\n",
    "            'recall': agg_metrics['recall'],\n",
    "            'f1': agg_metrics['f1'],\n",
    "            'confusion_matrix': agg_metrics['confusion_matrix'],\n",
    "            'fpr': agg_metrics['fpr'],\n",
    "            'pr_auc': agg_metrics['pr_auc'],\n",
    "            'threshold': agg_metrics['threshold']\n",
    "        },\n",
    "        'dev_metrics': dev_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'feature_names': feature_names,\n",
    "        'cat_cols': cat_cols,\n",
    "        'normalization': 'per-model robust z + log1p on *_raw'\n",
    "    }\n",
    "\n",
    "    meta_path = os.path.join(output_dir, f\"{dataset_type}_xgb_precv72_{balancing}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_metadata.json\")\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"\\n✓ Metadata saved: {meta_path}\")\n",
    "    print(\"=\"*92)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# ===================== WRAPPERS =====================\n",
    "\n",
    "def train_ssd_precision_v72_xgb():\n",
    "    \"\"\"\n",
    "    SSD: ratios moderados, hard-window 60d, SMOTE opcional si hay muy pocos positivos.\n",
    "    \"\"\"\n",
    "    xgb_params = dict(\n",
    "        # Guardrails VRAM + rendimiento\n",
    "        learning_rate=0.06,\n",
    "        max_depth=8,\n",
    "        min_child_weight=8.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_lambda=1.2,\n",
    "        reg_alpha=0.0,\n",
    "        max_bin=256,\n",
    "        enable_categorical=True,\n",
    "        max_cat_to_onehot=16,\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "    return train_xgb_precision_pipeline_v72(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        train_years=[2020, 2021, 2022, 2023],\n",
    "        dev_years=[2024],\n",
    "        test_years=[2025],\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "\n",
    "        neg_pos_ratio=3,\n",
    "        hard_window=60,\n",
    "        hard_fraction=0.7,\n",
    "\n",
    "        balancing='smote_knn',                 # cambiar a 'smote_enn' si la clase positiva es < ~0.1%\n",
    "        balancing_neg_pos_ratio=3,\n",
    "        smote_k_neighbors=5,\n",
    "        enn_k_neighbors=3,\n",
    "        max_balanced_samples=40_000,\n",
    "\n",
    "        xgb_params=xgb_params,\n",
    "        xgb_n_estimators=1500,\n",
    "        early_stopping_rounds=200,\n",
    "\n",
    "        min_precision=0.90,\n",
    "        min_recall=0.03,\n",
    "        top_k_rate=1e-4,\n",
    "        min_alerts=20,\n",
    "        output_dir='./models_xgb_ssd_precv72',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "def train_hdd_precision_v72_xgb():\n",
    "    \"\"\"\n",
    "    HDD: dataset grande → más negativos, ventana larga, y a veces conviene 'under' puro para controlar VRAM.\n",
    "    \"\"\"\n",
    "    xgb_params = dict(\n",
    "        learning_rate=0.06,\n",
    "        max_depth=8,\n",
    "        min_child_weight=10.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.0,\n",
    "        max_bin=256,         # 256–512; subir a 512 si mejoras estables en PR-AUC y hay VRAM\n",
    "        enable_categorical=True,\n",
    "        max_cat_to_onehot=16,\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "    return train_xgb_precision_pipeline_v72(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        dataset_type='HDD',\n",
    "        train_years=[2020, 2021, 2022, 2023],\n",
    "        dev_years=[2024],\n",
    "        test_years=[2025],\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "\n",
    "        neg_pos_ratio=5,\n",
    "        hard_window=90,\n",
    "        hard_fraction=0.7,\n",
    "\n",
    "        balancing='under',                 # en HDD empezar con 'under'; probar SMOTENC en subsets\n",
    "        balancing_neg_pos_ratio=5,\n",
    "        smote_k_neighbors=7,\n",
    "        enn_k_neighbors=3,\n",
    "        max_balanced_samples=60_000,\n",
    "\n",
    "        xgb_params=xgb_params,\n",
    "        xgb_n_estimators=1500,\n",
    "        early_stopping_rounds=200,\n",
    "\n",
    "        min_precision=0.90,\n",
    "        min_recall=0.03,\n",
    "        top_k_rate=7.5e-5,\n",
    "        min_alerts=30,\n",
    "        output_dir='./models_xgb_hdd_precv72',\n",
    "        random_state=42\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f017dd92-cac9-419d-a0af-a7ea96f2bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "XGBOOST (GPU, PRECISION-CONTROLLED v7.2) - SSD (Train=[2020, 2021, 2022, 2023] | Dev=[2024] | Test=[2025])\n",
      "Balancing: smote_knn (target neg:pos=3)\n",
      "============================================================================================\n",
      "Loading [2020, 2021, 2022, 2023] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 2,124,111 rows\n",
      "Loading [2024] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 1,220,745 rows\n",
      "Loading [2025] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 0 rows\n",
      "TRAIN labels: 1,325 pos (0.0624%)\n",
      "Creating features (temporal join) for SSD with TRAIN∪DEV∪TEST...\n",
      "  Found 13 SMART attributes (raw)\n",
      "  Final features: 97 (cat=2, num≈95)\n",
      "\n",
      "Fold 1/5: train=1,709,653 (pos=1,079) | val=414,458 (pos=246)\n",
      "  After hard-neg sampling: 4,316 (pos=1,079, neg=3,237)\n",
      "  After balancing [smote_knn]: 4,316 (pos=1,079, neg=3,237)\n",
      "  Fold 1 @thr=0.7276 | P=1.000 R=0.045 F1=0.086 PR-AUC=0.2743\n",
      "\n",
      "Fold 2/5: train=1,707,701 (pos=1,054) | val=416,410 (pos=271)\n",
      "  After hard-neg sampling: 4,216 (pos=1,054, neg=3,162)\n",
      "  After balancing [smote_knn]: 4,216 (pos=1,054, neg=3,162)\n",
      "  Fold 2 @thr=0.6097 | P=1.000 R=0.037 F1=0.071 PR-AUC=0.2516\n",
      "\n",
      "Fold 3/5: train=1,682,081 (pos=1,061) | val=442,030 (pos=264)\n",
      "  After hard-neg sampling: 4,244 (pos=1,061, neg=3,183)\n",
      "  After balancing [smote_knn]: 4,244 (pos=1,061, neg=3,183)\n",
      "  Fold 3 @thr=0.8517 | P=0.818 R=0.136 F1=0.234 PR-AUC=0.3352\n",
      "\n",
      "Fold 4/5: train=1,691,757 (pos=1,061) | val=432,354 (pos=264)\n",
      "  After hard-neg sampling: 4,244 (pos=1,061, neg=3,183)\n",
      "  After balancing [smote_knn]: 4,244 (pos=1,061, neg=3,183)\n",
      "  Fold 4 @thr=0.9896 | P=1.000 R=0.030 F1=0.059 PR-AUC=0.3009\n",
      "\n",
      "Fold 5/5: train=1,705,252 (pos=1,045) | val=418,859 (pos=280)\n",
      "  After hard-neg sampling: 4,180 (pos=1,045, neg=3,135)\n",
      "  After balancing [smote_knn]: 4,180 (pos=1,045, neg=3,135)\n",
      "  Fold 5 @thr=0.9651 | P=1.000 R=0.032 F1=0.062 PR-AUC=0.2221\n",
      "\n",
      "OOF (diag, not used for prod):\n",
      "{\n",
      "  \"precision\": 0.9302325581395349,\n",
      "  \"recall\": 0.03018867924528302,\n",
      "  \"f1\": 0.05847953216374269,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      2122783,\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      1285,\n",
      "      40\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 1.4132371327114462e-06,\n",
      "  \"pr_auc\": 0.17678622904595673,\n",
      "  \"threshold\": 0.9736286401748657\n",
      "}\n",
      "\n",
      "DEV calibration metrics (used for PROD threshold):\n",
      "{\n",
      "  \"precision\": 0.0,\n",
      "  \"recall\": 0.0,\n",
      "  \"f1\": 0.0,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      1220396,\n",
      "      122\n",
      "    ],\n",
      "    [\n",
      "      227,\n",
      "      0\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 9.995755900363616e-05,\n",
      "  \"pr_auc\": 0.00045536336656633054,\n",
      "  \"threshold\": 0.9977745413780212\n",
      "}\n",
      "\n",
      "✓ Final model saved (train-only): ./models_xgb_ssd_precv72/SSD_xgb_precv72_smote_knn_20251106_001819_model.pkl/.json\n",
      "\n",
      "✓ Metadata saved: ./models_xgb_ssd_precv72/SSD_xgb_precv72_smote_knn_20251106_001831_metadata.json\n",
      "============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'SSD',\n",
       " 'train_years': [2020, 2021, 2022, 2023],\n",
       " 'dev_years': [2024],\n",
       " 'test_years': [2025],\n",
       " 'lookahead_days': 7,\n",
       " 'n_splits': 5,\n",
       " 'hard_negative_sampling': {'neg_pos_ratio': 3,\n",
       "  'hard_window': 60,\n",
       "  'hard_fraction': 0.7},\n",
       " 'balancing': {'strategy': 'smote_knn',\n",
       "  'neg_pos_ratio': 3,\n",
       "  'smote_k_neighbors': 5,\n",
       "  'enn_k_neighbors': 3,\n",
       "  'max_balanced_samples': 40000},\n",
       " 'xgb_params': {'learning_rate': 0.06,\n",
       "  'max_depth': 8,\n",
       "  'min_child_weight': 8.0,\n",
       "  'subsample': 0.8,\n",
       "  'colsample_bytree': 0.6,\n",
       "  'reg_lambda': 1.2,\n",
       "  'reg_alpha': 0.0,\n",
       "  'max_bin': 256,\n",
       "  'enable_categorical': True,\n",
       "  'max_cat_to_onehot': 16,\n",
       "  'tree_method': 'gpu_hist',\n",
       "  'predictor': 'gpu_predictor',\n",
       "  'eval_metric': 'aucpr',\n",
       "  'random_state': 42,\n",
       "  'verbosity': 0,\n",
       "  'n_estimators': 1500},\n",
       " 'min_precision': 0.9,\n",
       " 'min_recall': 0.03,\n",
       " 'top_k_rate': 0.0001,\n",
       " 'min_alerts': 20,\n",
       " 'oof_metrics': {'precision': 0.9302325581395349,\n",
       "  'recall': 0.03018867924528302,\n",
       "  'f1': 0.05847953216374269,\n",
       "  'confusion_matrix': [[2122783, 3], [1285, 40]],\n",
       "  'fpr': 1.4132371327114462e-06,\n",
       "  'pr_auc': 0.17678622904595673,\n",
       "  'threshold': 0.9736286401748657},\n",
       " 'dev_metrics': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'confusion_matrix': [[1220396, 122], [227, 0]],\n",
       "  'fpr': 9.995755900363616e-05,\n",
       "  'pr_auc': 0.00045536336656633054,\n",
       "  'threshold': 0.9977745413780212},\n",
       " 'test_metrics': None,\n",
       " 'feature_names': ['model',\n",
       "  'vendor',\n",
       "  'capacity_bytes',\n",
       "  'smart_5_raw',\n",
       "  'smart_187_raw',\n",
       "  'smart_194_raw',\n",
       "  'smart_231_raw',\n",
       "  'smart_233_raw',\n",
       "  'smart_241_raw',\n",
       "  'smart_242_raw',\n",
       "  'smart_9_raw',\n",
       "  'smart_173_raw',\n",
       "  'smart_174_raw',\n",
       "  'smart_184_raw',\n",
       "  'smart_199_raw',\n",
       "  'smart_230_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'delta_smart_194_raw',\n",
       "  'max_smart_194_raw',\n",
       "  'delta_smart_231_raw',\n",
       "  'max_smart_231_raw',\n",
       "  'delta_smart_233_raw',\n",
       "  'max_smart_233_raw',\n",
       "  'delta_smart_241_raw',\n",
       "  'max_smart_241_raw',\n",
       "  'delta_smart_242_raw',\n",
       "  'max_smart_242_raw',\n",
       "  'delta_smart_9_raw',\n",
       "  'max_smart_9_raw',\n",
       "  'delta_smart_173_raw',\n",
       "  'max_smart_173_raw',\n",
       "  'delta_smart_174_raw',\n",
       "  'max_smart_174_raw',\n",
       "  'delta_smart_184_raw',\n",
       "  'max_smart_184_raw',\n",
       "  'delta_smart_199_raw',\n",
       "  'max_smart_199_raw',\n",
       "  'delta_smart_230_raw',\n",
       "  'max_smart_230_raw',\n",
       "  'rmean7_smart_5_raw',\n",
       "  'rstd7_smart_5_raw',\n",
       "  'rmean7_smart_187_raw',\n",
       "  'rstd7_smart_187_raw',\n",
       "  'rmean7_smart_194_raw',\n",
       "  'rstd7_smart_194_raw',\n",
       "  'rmean7_smart_231_raw',\n",
       "  'rstd7_smart_231_raw',\n",
       "  'rmean7_smart_233_raw',\n",
       "  'rstd7_smart_233_raw',\n",
       "  'rmean7_smart_241_raw',\n",
       "  'rstd7_smart_241_raw',\n",
       "  'rmean7_smart_242_raw',\n",
       "  'rstd7_smart_242_raw',\n",
       "  'rmean7_smart_9_raw',\n",
       "  'rstd7_smart_9_raw',\n",
       "  'rmean7_smart_173_raw',\n",
       "  'rstd7_smart_173_raw',\n",
       "  'rmean7_smart_174_raw',\n",
       "  'rstd7_smart_174_raw',\n",
       "  'rmean7_smart_184_raw',\n",
       "  'rstd7_smart_184_raw',\n",
       "  'rmean7_smart_199_raw',\n",
       "  'rstd7_smart_199_raw',\n",
       "  'rmean7_smart_230_raw',\n",
       "  'rstd7_smart_230_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week',\n",
       "  'z_smart_5_raw',\n",
       "  'log1p_smart_5_raw',\n",
       "  'z_smart_187_raw',\n",
       "  'log1p_smart_187_raw',\n",
       "  'z_smart_194_raw',\n",
       "  'log1p_smart_194_raw',\n",
       "  'z_smart_231_raw',\n",
       "  'log1p_smart_231_raw',\n",
       "  'z_smart_233_raw',\n",
       "  'log1p_smart_233_raw',\n",
       "  'z_smart_241_raw',\n",
       "  'log1p_smart_241_raw',\n",
       "  'z_smart_242_raw',\n",
       "  'log1p_smart_242_raw',\n",
       "  'z_smart_9_raw',\n",
       "  'log1p_smart_9_raw',\n",
       "  'z_smart_173_raw',\n",
       "  'log1p_smart_173_raw',\n",
       "  'z_smart_174_raw',\n",
       "  'log1p_smart_174_raw',\n",
       "  'z_smart_184_raw',\n",
       "  'log1p_smart_184_raw',\n",
       "  'z_smart_199_raw',\n",
       "  'log1p_smart_199_raw',\n",
       "  'z_smart_230_raw',\n",
       "  'log1p_smart_230_raw'],\n",
       " 'cat_cols': ['model', 'vendor'],\n",
       " 'normalization': 'per-model robust z + log1p on *_raw'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_precision_v72_xgb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
