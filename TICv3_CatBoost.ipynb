{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c5e045-f9e3-4108-957a-1fb73f99a687",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b2a683-626e-452b-8633-eb0eda6aa6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CATBOOST — PRECISION-CONTROLLED v7.2\n",
    "# (SSD/HDD) — Big-mode en TRAIN y TEST (streaming)\n",
    "#   • TRAIN big: escanea fallos y carga subset (pos/near/ sample far)\n",
    "#   • TEST 2024 streaming (sin cargar todo a RAM)\n",
    "#   • HDD big-mode: sin rolling; VRAM guardrails (RTX 2060 6GB)\n",
    "# ============================================\n",
    "\n",
    "import os, gc, json, warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "# ===================== UTILS =====================\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "\n",
    "def downcast_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        elif pd.api.types.is_integer_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================== IO Helpers (streaming) =====================\n",
    "\n",
    "def _list_parquet_files(path: str) -> List[str]:\n",
    "    if os.path.isdir(path):\n",
    "        return [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".parquet\")]\n",
    "    return [path]\n",
    "\n",
    "def _peek_row_group(path: str) -> pd.DataFrame:\n",
    "    files = _list_parquet_files(path)\n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        if pf.num_row_groups > 0:\n",
    "            tb = pf.read_row_group(0, columns=None)\n",
    "            return tb.to_pandas().head(100)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def discover_smart_columns(path: str) -> List[str]:\n",
    "    peek = _peek_row_group(path)\n",
    "    if peek.empty:\n",
    "        return []\n",
    "    cols = peek.columns.tolist()\n",
    "    return sorted([c for c in cols if (\"smart\" in c.lower()) and c.endswith(\"_raw\")])\n",
    "\n",
    "\n",
    "def _iter_parquet_row_groups(path: str, years: List[int], columns: Optional[List[str]] = None):\n",
    "    files = _list_parquet_files(path)\n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for i in range(pf.num_row_groups):\n",
    "            tb = pf.read_row_group(i, columns=columns) if columns else pf.read_row_group(i)\n",
    "            df = tb.to_pandas()\n",
    "            if \"date\" not in df.columns:\n",
    "                continue\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            mask = df[\"date\"].dt.year.isin(years)\n",
    "            if mask.any():\n",
    "                yield downcast_df(df.loc[mask].reset_index(drop=True))\n",
    "            del df, tb\n",
    "            cleanup()\n",
    "\n",
    "\n",
    "def load_data(path: str, years: List[int], columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    print(f\"Loading {years} from {path}...\")\n",
    "    chunks = []\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=columns):\n",
    "        chunks.append(df)\n",
    "        if len(chunks) >= 16:\n",
    "            chunks = [pd.concat(chunks, ignore_index=True)]\n",
    "            cleanup()\n",
    "    out = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "    print(f\"Loaded {len(out):,} rows\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===== BIG MODE: pasada 1 – mapa de fallos =====\n",
    "\n",
    "def scan_fail_dates(path: str, years: List[int]) -> Dict[str, pd.Timestamp]:\n",
    "    print(\"Scanning earliest failure dates (streaming)...\")\n",
    "    fail_map: Dict[str, pd.Timestamp] = {}\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=[\"serial_number\", \"date\", \"failure\"]):\n",
    "        df = df[df[\"failure\"] == 1]\n",
    "        if df.empty:\n",
    "            continue\n",
    "        sns = df[\"serial_number\"].astype(str).values\n",
    "        dts = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        for sn, dt in zip(sns, dts):\n",
    "            if pd.isna(dt):\n",
    "                continue\n",
    "            if sn not in fail_map or dt < fail_map[sn]:\n",
    "                fail_map[sn] = dt\n",
    "    print(f\"  Found {len(fail_map):,} failed serials\")\n",
    "    return fail_map\n",
    "\n",
    "\n",
    "# ===== BIG MODE: pasada 2 – filtrado de filas =====\n",
    "\n",
    "def load_data_big_filtered(\n",
    "    path: str,\n",
    "    years: List[int],\n",
    "    fail_map: Dict[str, pd.Timestamp],\n",
    "    lookahead_days: int,\n",
    "    hard_window: int,\n",
    "    neg_random_keep_rate: float = 0.0025,\n",
    "    columns: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TRAIN big: conserva todos positivos, negativos cercanos y una muestra de lejanos.\n",
    "    NaT-safe y causal por disco (pero sin rolling), como en tu RF v5s4.\n",
    "    \"\"\"\n",
    "    print(f\"Loading BIG-FILTERED {years} from {path} (keep_rate={neg_random_keep_rate})...\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    kept = []\n",
    "    total_rows = 0\n",
    "    kept_rows = 0\n",
    "\n",
    "    for df in _iter_parquet_row_groups(path, years, columns=columns):\n",
    "        total_rows += len(df)\n",
    "        sn_ser = df[\"serial_number\"].astype(str)\n",
    "        dt = pd.to_datetime(df[\"date\"], errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "        fail_series = sn_ser.map(fail_map)\n",
    "        fdt = pd.to_datetime(fail_series, errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "\n",
    "        dtf = np.full(len(df), 10**9, dtype=np.int64)\n",
    "        valid = ~np.isnat(fdt)\n",
    "        if valid.any():\n",
    "            dd = (fdt[valid] - dt[valid]).astype(\"timedelta64[D]\").astype(\"int64\")\n",
    "            dtf[valid] = dd\n",
    "\n",
    "        failure = (df[\"failure\"].values == 1)\n",
    "        pos_mask = failure | ((dtf >= 0) & (dtf <= lookahead_days))\n",
    "        near_mask = (dtf > lookahead_days) & (dtf <= hard_window)\n",
    "\n",
    "        keep = pos_mask | near_mask\n",
    "        far_neg = (~keep) & (~failure)\n",
    "        if far_neg.any():\n",
    "            sample = rng.random(far_neg.sum()) < neg_random_keep_rate\n",
    "            sel = np.zeros_like(far_neg, dtype=bool)\n",
    "            sel[np.where(far_neg)[0]] = sample\n",
    "            keep = keep | sel\n",
    "\n",
    "        kept.append(df.loc[keep].reset_index(drop=True))\n",
    "        kept_rows += int(keep.sum())\n",
    "\n",
    "        if sum(len(x) for x in kept) > 3_000_000:\n",
    "            kept = [pd.concat(kept, ignore_index=True)]\n",
    "            cleanup()\n",
    "\n",
    "    out = pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n",
    "    rate = 100 * kept_rows / max(1, total_rows)\n",
    "    print(f\"  BIG-FILTERED kept {kept_rows:,}/{total_rows:,} rows (~{rate:.2f}%)\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== PREP =====================\n",
    "\n",
    "def prepare_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"serial_number\"] = df[\"serial_number\"].astype(str)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"serial_number\", \"date\"])\n",
    "    df = df.sort_values([\"serial_number\", \"date\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================== LABELS =====================\n",
    "\n",
    "def compute_days_to_failure(dfs: pd.DataFrame) -> np.ndarray:\n",
    "    fail_map: Dict[str, pd.Timestamp] = {}\n",
    "    fails = dfs[dfs[\"failure\"] == 1]\n",
    "    for sn, dt in zip(fails[\"serial_number\"], fails[\"date\"]):\n",
    "        fail_map[sn] = min(dt, fail_map.get(sn, dt))\n",
    "    dtf = np.full(len(dfs), 10**9, dtype=np.int64)\n",
    "    for i, (sn, dt) in enumerate(zip(dfs[\"serial_number\"], dfs[\"date\"])):\n",
    "        if sn in fail_map:\n",
    "            dtf[i] = (fail_map[sn] - dt).days\n",
    "    return dtf\n",
    "\n",
    "def create_labels_from_dtf(dtf: np.ndarray, lookahead: int = 7) -> np.ndarray:\n",
    "    return ((dtf >= 0) & (dtf <= lookahead)).astype(np.int8)\n",
    "\n",
    "\n",
    "# ===================== CATEGÓRICOS & NORMALIZACIÓN =====================\n",
    "\n",
    "def extract_vendor(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    return s.str.extract(r\"^([A-Za-z]+)\", expand=False).fillna(\"UNK\")\n",
    "\n",
    "def fit_model_stats(df_train: pd.DataFrame, smart_cols: List[str]) -> Tuple[Dict[str, Dict[str, Tuple[float, float]]], Dict[str, Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Stats robustas SOLO en TRAIN:\n",
    "      - Por modelo: median e IQR (p50, q3-q1) => stats_model[model][col] = (med, iqr)\n",
    "      - Fallback global: median e IQR global por col => stats_global[col] = (med, iqr)\n",
    "    \"\"\"\n",
    "    stats_model: Dict[str, Dict[str, Tuple[float, float]]] = {}\n",
    "    stats_global: Dict[str, Tuple[float, float]] = {}\n",
    "\n",
    "    if 'model' in df_train.columns and not df_train.empty:\n",
    "        for c in smart_cols:\n",
    "            xg = pd.to_numeric(df_train[c], errors='coerce').dropna().values\n",
    "            if xg.size:\n",
    "                med = float(np.median(xg))\n",
    "                q1, q3 = np.percentile(xg, [25, 75])\n",
    "                stats_global[c] = (med, float(max(q3 - q1, 1e-6)))\n",
    "            else:\n",
    "                stats_global[c] = (0.0, 1.0)\n",
    "\n",
    "        for m, g in df_train.groupby(df_train['model'].astype(str)):\n",
    "            d = {}\n",
    "            for c in smart_cols:\n",
    "                x = pd.to_numeric(g[c], errors='coerce').dropna().values\n",
    "                if x.size:\n",
    "                    med = float(np.median(x)); q1, q3 = np.percentile(x, [25, 75])\n",
    "                    d[c] = (med, float(max(q3 - q1, 1e-6)))\n",
    "            stats_model[str(m)] = d\n",
    "    return stats_model, stats_global\n",
    "\n",
    "def apply_model_normalization_inplace(df_all: pd.DataFrame, smart_cols: List[str],\n",
    "                                     stats_model: Dict[str, Dict[str, Tuple[float, float]]],\n",
    "                                     stats_global: Dict[str, Tuple[float, float]]):\n",
    "    model_series = df_all['model'].astype(str)\n",
    "    for c in smart_cols:\n",
    "        med_map = {m: v[c][0] for m, v in stats_model.items() if c in v}\n",
    "        iqr_map = {m: v[c][1] for m, v in stats_model.items() if c in v}\n",
    "        g_med, g_iqr = stats_global.get(c, (0.0, 1.0))\n",
    "        med_s = model_series.map(med_map).fillna(g_med).astype(np.float32)\n",
    "        iqr_s = model_series.map(iqr_map).fillna(g_iqr).astype(np.float32)\n",
    "\n",
    "        col = pd.to_numeric(df_all[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "        z = (col - med_s) / (iqr_s + 1e-6)\n",
    "        df_all[f'z_{c}'] = z.values\n",
    "        df_all[f'log1p_{c}'] = np.log1p(np.maximum(col.values, 0.0)).astype(np.float32)\n",
    "\n",
    "\n",
    "# ===================== FEATURES (JOIN TRAIN∪DEV∪TEST, RAM) =====================\n",
    "\n",
    "def create_features_joined_cat_v7(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_dev: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    dataset_type: str,\n",
    "    add_rolling: bool,\n",
    "    smart_cols: Optional[List[str]] = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, List[str], List[int], Dict, Dict]:\n",
    "    print(f\"Creating features (temporal join) for {dataset_type} with TRAIN∪DEV∪TEST...\")\n",
    "\n",
    "    df_train = df_train.copy(); df_train['__subset__'] = 'train'\n",
    "    df_dev   = df_dev.copy();   df_dev['__subset__']   = 'dev'\n",
    "    df_test  = df_test.copy();  df_test['__subset__']  = 'test'\n",
    "    df_all = pd.concat([df_train, df_dev, df_test], ignore_index=True)\n",
    "    df_all.sort_values(['serial_number','date'], inplace=True)\n",
    "\n",
    "    if smart_cols is None:\n",
    "        all_cols = df_all.columns.tolist()\n",
    "        smart_cols = [c for c in all_cols if ('smart' in c.lower()) and ('_raw' in c.lower())]\n",
    "    print(f\"  Found {len(smart_cols)} SMART attributes (raw)\")\n",
    "\n",
    "    for c in smart_cols:\n",
    "        df_all[c] = pd.to_numeric(df_all[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # Delta / cummax\n",
    "    for c in smart_cols:\n",
    "        g = df_all.groupby('serial_number', sort=False)[c]\n",
    "        df_all[f'delta_{c}'] = g.diff().fillna(0.0)\n",
    "        df_all[f'max_{c}']   = g.cummax()\n",
    "\n",
    "    # Rolling 7d (evitar en HDD big)\n",
    "    if add_rolling:\n",
    "        for c in smart_cols:\n",
    "            r = df_all.groupby('serial_number', sort=False)[c]\n",
    "            df_all[f'rmean7_{c}'] = r.rolling(window=7, min_periods=2).mean().reset_index(level=0, drop=True).fillna(0.0)\n",
    "            df_all[f'rstd7_{c}']  = r.rolling(window=7, min_periods=2).std().reset_index(level=0, drop=True).fillna(0.0)\n",
    "\n",
    "    # Edad / calendario\n",
    "    df_all['age_days'] = df_all.groupby('serial_number', sort=False).cumcount()\n",
    "    df_all['month'] = df_all['date'].dt.month\n",
    "    df_all['day_of_week'] = df_all['date'].dt.dayofweek\n",
    "\n",
    "    # Categóricas nativas\n",
    "    if 'model' not in df_all.columns:\n",
    "        df_all['model'] = 'UNK'\n",
    "    df_all['model'] = df_all['model'].astype(str).fillna('UNK')\n",
    "    df_all['vendor'] = extract_vendor(df_all['model']).astype(str).fillna('UNK')\n",
    "\n",
    "    # Normalización por modelo (aprendida en TRAIN)\n",
    "    stats_model, stats_global = fit_model_stats(df_train, smart_cols)\n",
    "    apply_model_normalization_inplace(df_all, smart_cols, stats_model, stats_global)\n",
    "\n",
    "    # Construcción de X\n",
    "    drop_cols = ['serial_number', 'date', 'failure']\n",
    "    X_all = df_all.drop(columns=[c for c in drop_cols if c in df_all.columns], errors='ignore')\n",
    "\n",
    "    cat_cols = [c for c in ['model','vendor'] if c in X_all.columns]\n",
    "    num_cols = [c for c in X_all.columns if c not in cat_cols]\n",
    "    for c in num_cols:\n",
    "        X_all[c] = pd.to_numeric(X_all[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "\n",
    "    # elimina constantes\n",
    "    var = X_all[num_cols].var()\n",
    "    keep_num = var[var > 0].index.tolist()\n",
    "    X_all = pd.concat([X_all[cat_cols], X_all[keep_num]], axis=1)\n",
    "\n",
    "    feature_names = list(X_all.columns)\n",
    "    cat_indices = [X_all.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "    # Split\n",
    "    tr_mask  = (df_all['__subset__']=='train').values\n",
    "    dev_mask = (df_all['__subset__']=='dev').values\n",
    "    te_mask  = (df_all['__subset__']=='test').values\n",
    "\n",
    "    X_train = X_all.loc[tr_mask].reset_index(drop=True)\n",
    "    X_dev   = X_all.loc[dev_mask].reset_index(drop=True)\n",
    "    X_test  = X_all.loc[te_mask].reset_index(drop=True)\n",
    "\n",
    "    print(f\"  Final features: {X_train.shape[1]} (cat={len(cat_indices)}, num≈{len(keep_num)})\")\n",
    "    return X_train, X_dev, X_test, None, None, None, feature_names, cat_indices, {'stats_model': stats_model}, {'stats_global': stats_global, 'smart_cols': smart_cols}\n",
    "\n",
    "\n",
    "# ===================== GROUPED CV =====================\n",
    "\n",
    "def make_group_folds(serials: pd.Series, y: np.ndarray, n_splits: int = 5, random_state: int = 42):\n",
    "    serials = serials.astype(str).values\n",
    "    uniq_serials, inverse = np.unique(serials, return_inverse=True)\n",
    "    y_disk = np.zeros(len(uniq_serials), dtype=np.int8)\n",
    "    for idx_row, disk_idx in enumerate(inverse):\n",
    "        if y[idx_row] == 1:\n",
    "            y_disk[disk_idx] = 1\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for tr_d, va_d in skf.split(uniq_serials, y_disk):\n",
    "        tr_mask = np.isin(inverse, tr_d)\n",
    "        va_mask = np.isin(inverse, va_d)\n",
    "        yield np.where(tr_mask)[0], np.where(va_mask)[0]\n",
    "\n",
    "\n",
    "# ===================== NEGATIVE SAMPLING =====================\n",
    "\n",
    "def sample_negatives_hard(\n",
    "    X: pd.DataFrame, y: np.ndarray, dtf: np.ndarray, lookahead: int,\n",
    "    neg_pos_ratio: int = 3, hard_window: int = 60, hard_fraction: float = 0.7,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    if len(pos_idx) == 0:\n",
    "        raise ValueError(\"No positives in training fold for hard-negative sampling\")\n",
    "\n",
    "    n_pos = len(pos_idx)\n",
    "    n_neg_needed = max(n_pos * neg_pos_ratio, 1)\n",
    "\n",
    "    hard_mask = (dtf > lookahead) & (dtf <= hard_window)\n",
    "    hard_idx = np.where((y == 0) & hard_mask)[0]\n",
    "    easy_idx = np.where((y == 0) & (~hard_mask))[0]\n",
    "\n",
    "    n_hard = min(int(n_neg_needed * hard_fraction), len(hard_idx))\n",
    "    n_easy = min(n_neg_needed - n_hard, len(easy_idx))\n",
    "\n",
    "    chosen_hard = rng.choice(hard_idx, size=n_hard, replace=False) if n_hard > 0 else np.empty(0, dtype=int)\n",
    "    chosen_easy = rng.choice(easy_idx, size=n_easy, replace=False) if n_easy > 0 else np.empty(0, dtype=int)\n",
    "\n",
    "    sel_idx = np.sort(np.concatenate([pos_idx, chosen_hard, chosen_easy]))\n",
    "    Xb = X.iloc[sel_idx].reset_index(drop=True)\n",
    "    yb = y[sel_idx]\n",
    "    return Xb, yb, sel_idx\n",
    "\n",
    "\n",
    "# ===================== CATBOOST (GPU) =====================\n",
    "\n",
    "def get_catboost(\n",
    "    depth: int = 8,\n",
    "    iterations: int = 1500,\n",
    "    learning_rate: float = 0.06,\n",
    "    l2_leaf_reg: float = 4.0,\n",
    "    border_count: int = 64,\n",
    "    random_seed: int = 42,\n",
    "    gpu_ram_part: float = 0.70,\n",
    "    bootstrap_type: str = 'Bernoulli',\n",
    "    subsample: float = 0.8,\n",
    "):\n",
    "    return CatBoostClassifier(\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        iterations=iterations,\n",
    "        depth=depth,\n",
    "        learning_rate=learning_rate,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        border_count=border_count,\n",
    "        random_seed=random_seed,\n",
    "        task_type='GPU',\n",
    "        devices='0',\n",
    "        gpu_ram_part=gpu_ram_part,\n",
    "        auto_class_weights=None,\n",
    "        bootstrap_type=bootstrap_type,\n",
    "        subsample=subsample,\n",
    "        logging_level='Silent',\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "\n",
    "\n",
    "# ===================== THRESHOLDING & EVAL =====================\n",
    "\n",
    "def pick_threshold_precision_first(\n",
    "    y_true: np.ndarray, proba: np.ndarray,\n",
    "    min_precision: float = 0.90, min_recall: float = 0.03,\n",
    "    top_k_rate: float = 1e-4, min_alerts: int = 5\n",
    "):\n",
    "    precision, recall, thr = precision_recall_curve(y_true, proba)\n",
    "    pr_auc = average_precision_score(y_true, proba)\n",
    "\n",
    "    valid = (precision >= min_precision) & (recall >= min_recall)\n",
    "    if valid.any():\n",
    "        idxs = np.where(valid)[0]\n",
    "        idx = idxs[-1] - 1 if idxs[-1] >= len(thr) else idxs[-1]\n",
    "        idx = max(0, min(idx, len(thr)-1))\n",
    "        chosen = thr[idx]\n",
    "    else:\n",
    "        k = max(max(1, min_alerts), int(len(proba) * max(top_k_rate, 1e-6)))\n",
    "        chosen = float(np.partition(proba, -k)[-k])\n",
    "\n",
    "    return float(chosen), float(pr_auc)\n",
    "\n",
    "def metrics_at_threshold(y_true: np.ndarray, proba: np.ndarray, thr: float) -> Dict:\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        'confusion_matrix': [[int(tn), int(fp)], [int(fn), int(tp)]],\n",
    "        'fpr': float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "# ===================== MAIN PIPELINE (v7.2 con BIG switches) =====================\n",
    "\n",
    "def train_catboost_precision_pipeline_v72(\n",
    "    train_parquet: str,\n",
    "    test_parquet: str | None,\n",
    "    dataset_type: str,\n",
    "    train_years: List[int] = [2020, 2021, 2022],\n",
    "    dev_years: List[int]   = [2023],\n",
    "    test_years: List[int]  = [2024],\n",
    "    lookahead_days: int = 7,\n",
    "    n_splits: int = 5,\n",
    "    neg_pos_ratio: int = 3,        # SSD=3, HDD=5\n",
    "    hard_window: int = 60,         # HDD=90\n",
    "    hard_fraction: float = 0.7,\n",
    "    cb_depth: int = 8,\n",
    "    cb_iterations: int = 1500,\n",
    "    cb_learning_rate: float = 0.06,\n",
    "    cb_l2_leaf_reg: float = 4.0,\n",
    "    cb_border_count: int = 64,\n",
    "    cb_bootstrap_type: str = 'Bernoulli',\n",
    "    cb_subsample: float = 0.8,\n",
    "    cb_gpu_ram_part: float = 0.70,\n",
    "    min_precision: float = 0.90,\n",
    "    min_recall: float = 0.03,\n",
    "    top_k_rate: float = 1e-4,\n",
    "    min_alerts: int = 20,\n",
    "    output_dir: str = './models_cb_precision_v72',\n",
    "    random_state: int = 42,\n",
    "    # BIG mode\n",
    "    big_mode: bool = False,                 # TRAIN big\n",
    "    dev_big_mode: bool = False,             # DEV big (útil en HDD)\n",
    "    test_big_mode: bool = False,            # TEST streaming\n",
    "    neg_random_keep_rate_train: float = 0.0025,\n",
    "    neg_random_keep_rate_dev: float = 0.01,\n",
    "    add_rolling: bool = True,               # desactiva rolling en HDD big\n",
    "):\n",
    "    print(\"=\"*100)\n",
    "    print(f\"CATBOOST (GPU, PRECISION-CONTROLLED v7.2) - {dataset_type.upper()}\")\n",
    "    print(\"=\"*100)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Descubrir SMART para leer solo lo necesario en BIG mode\n",
    "    smart_cols_scan = discover_smart_columns(train_parquet)\n",
    "    base_cols = ['serial_number', 'date', 'failure', 'model', 'capacity_bytes']\n",
    "    read_cols = base_cols + smart_cols_scan\n",
    "\n",
    "    # ---------- Load TRAIN ----------\n",
    "    if big_mode:\n",
    "        fail_map_train = scan_fail_dates(train_parquet, train_years)\n",
    "        df_tr_raw = load_data_big_filtered(\n",
    "            train_parquet, train_years, fail_map_train,\n",
    "            lookahead_days=lookahead_days, hard_window=hard_window,\n",
    "            neg_random_keep_rate=neg_random_keep_rate_train, columns=read_cols\n",
    "        )\n",
    "    else:\n",
    "        df_tr_raw = load_data(train_parquet, train_years, columns=read_cols)\n",
    "\n",
    "    # ---------- Load DEV ----------\n",
    "    if dev_years:\n",
    "        if dev_big_mode:\n",
    "            fail_map_dev = scan_fail_dates(train_parquet, dev_years)\n",
    "            df_dev_raw = load_data_big_filtered(\n",
    "                train_parquet, dev_years, fail_map_dev,\n",
    "                lookahead_days=lookahead_days, hard_window=hard_window,\n",
    "                neg_random_keep_rate=neg_random_keep_rate_dev, columns=read_cols\n",
    "            )\n",
    "        else:\n",
    "            df_dev_raw = load_data(train_parquet, dev_years, columns=read_cols)\n",
    "    else:\n",
    "        df_dev_raw = pd.DataFrame()\n",
    "\n",
    "    # ---------- Load TEST (si no streaming) ----------\n",
    "    if test_parquet and test_years and (not test_big_mode):\n",
    "        df_te_raw = load_data(test_parquet, test_years, columns=read_cols)\n",
    "    else:\n",
    "        df_te_raw = pd.DataFrame()\n",
    "\n",
    "    if df_tr_raw.empty:\n",
    "        raise ValueError(\"Training data is empty!\")\n",
    "\n",
    "    # ---------- Prepare ----------\n",
    "    df_tr  = prepare_df(df_tr_raw)\n",
    "    df_dev = prepare_df(df_dev_raw) if not df_dev_raw.empty else pd.DataFrame()\n",
    "    df_te  = prepare_df(df_te_raw) if not df_te_raw.empty else pd.DataFrame()\n",
    "\n",
    "    # ---------- Labels ----------\n",
    "    dtf_tr  = compute_days_to_failure(df_tr)\n",
    "    y_tr    = create_labels_from_dtf(dtf_tr, lookahead_days)\n",
    "    print(f\"TRAIN labels: {int(y_tr.sum()):,} pos ({100*y_tr.mean():.4f}%)\")\n",
    "    if y_tr.sum() < 50:\n",
    "        raise ValueError(f\"Insufficient positive samples in TRAIN: {y_tr.sum()}\")\n",
    "\n",
    "    dtf_dev = compute_days_to_failure(df_dev) if not df_dev.empty else np.array([], dtype=np.int64)\n",
    "    y_dev   = create_labels_from_dtf(dtf_dev, lookahead_days) if not df_dev.empty else np.array([], dtype=np.int8)\n",
    "\n",
    "    # ---------- Features (RAM, sin TEST si streaming) ----------\n",
    "    X_tr, X_dev, X_te, _, _, _, feature_names, cat_indices, stats_model_wrap, stats_global_wrap = create_features_joined_cat_v7(\n",
    "        df_tr, df_dev, df_te, dataset_type, add_rolling=add_rolling, smart_cols=smart_cols_scan\n",
    "    )\n",
    "    stats_model = stats_model_wrap['stats_model']\n",
    "    stats_global = stats_global_wrap['stats_global']\n",
    "    smart_cols = stats_global_wrap['smart_cols']\n",
    "\n",
    "    # ---------- CV por disco (OOF diagnóstico) ----------\n",
    "    serials_tr = df_tr['serial_number']\n",
    "    oof_proba = np.zeros(len(y_tr), dtype=np.float32)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(make_group_folds(serials_tr, y_tr, n_splits=n_splits, random_state=random_state), start=1):\n",
    "        X_tr_fold, y_tr_fold = X_tr.iloc[tr_idx].reset_index(drop=True), y_tr[tr_idx]\n",
    "        dtf_tr_fold = dtf_tr[tr_idx]\n",
    "\n",
    "        X_va_fold, y_va_fold = X_tr.iloc[va_idx].reset_index(drop=True), y_tr[va_idx]\n",
    "\n",
    "        print(f\"\\nFold {fold}/{n_splits}: train={len(y_tr_fold):,} (pos={int(y_tr_fold.sum()):,}) | val={len(y_va_fold):,} (pos={int(y_va_fold.sum()):,})\")\n",
    "\n",
    "        Xb, yb, _ = sample_negatives_hard(\n",
    "            X_tr_fold, y_tr_fold, dtf_tr_fold,\n",
    "            lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio,\n",
    "            hard_window=hard_window,\n",
    "            hard_fraction=hard_fraction,\n",
    "            seed=random_state,\n",
    "        )\n",
    "        print(f\"  After hard-neg sampling: {len(yb):,} (pos={int(yb.sum()):,}, neg={len(yb)-int(yb.sum()):,})\")\n",
    "\n",
    "        model = get_catboost(\n",
    "            depth=cb_depth, iterations=cb_iterations, learning_rate=cb_learning_rate,\n",
    "            l2_leaf_reg=cb_l2_leaf_reg, border_count=cb_border_count,\n",
    "            bootstrap_type=cb_bootstrap_type, subsample=cb_subsample,\n",
    "            gpu_ram_part=cb_gpu_ram_part, random_seed=random_state\n",
    "        )\n",
    "        train_pool = Pool(Xb, label=yb, cat_features=cat_indices)\n",
    "        valid_pool = Pool(X_va_fold, label=y_va_fold, cat_features=cat_indices)\n",
    "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True, early_stopping_rounds=200)\n",
    "\n",
    "        proba_va = model.predict_proba(valid_pool)[:, 1]\n",
    "        thr_oof, pr_auc = pick_threshold_precision_first(\n",
    "            y_va_fold, proba_va, min_precision=min_precision, min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "        )\n",
    "        oof_proba[va_idx] = proba_va\n",
    "        m = metrics_at_threshold(y_va_fold, proba_va, thr_oof)\n",
    "        m.update({'pr_auc': float(pr_auc), 'threshold': float(thr_oof), 'fold': int(fold),\n",
    "                  'best_iteration': int(model.get_best_iteration())})\n",
    "        fold_metrics.append(m)\n",
    "\n",
    "        del X_tr_fold, y_tr_fold, X_va_fold, y_va_fold, Xb, yb, model, train_pool, valid_pool\n",
    "        cleanup()\n",
    "\n",
    "    # ---------- OOF (diag, no prod) ----------\n",
    "    thr_oof_global, pr_auc_oof = pick_threshold_precision_first(\n",
    "        y_tr, oof_proba, min_precision=min_precision, min_recall=min_recall,\n",
    "        top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "    )\n",
    "    agg_metrics = metrics_at_threshold(y_tr, oof_proba, thr_oof_global)\n",
    "    agg_metrics.update({'pr_auc': float(pr_auc_oof), 'threshold': float(thr_oof_global)})\n",
    "    print(\"\\nOOF (diag, not used for prod):\")\n",
    "    print(json.dumps(agg_metrics, indent=2))\n",
    "\n",
    "    # ---------- Calibración UMBRAL en DEV ----------\n",
    "    thr_prod = float(thr_oof_global)\n",
    "    dev_metrics = None\n",
    "    if not df_dev.empty:\n",
    "        Xb_full, yb_full, _ = sample_negatives_hard(\n",
    "            X_tr, y_tr, dtf_tr, lookahead=lookahead_days,\n",
    "            neg_pos_ratio=neg_pos_ratio, hard_window=hard_window,\n",
    "            hard_fraction=hard_fraction, seed=random_state\n",
    "        )\n",
    "        final_model_dev = get_catboost(\n",
    "            depth=cb_depth, iterations=cb_iterations, learning_rate=cb_learning_rate,\n",
    "            l2_leaf_reg=cb_l2_leaf_reg, border_count=cb_border_count,\n",
    "            bootstrap_type=cb_bootstrap_type, subsample=cb_subsample,\n",
    "            gpu_ram_part=cb_gpu_ram_part, random_seed=random_state\n",
    "        )\n",
    "        train_pool_full = Pool(Xb_full, label=yb_full, cat_features=cat_indices)\n",
    "        final_model_dev.fit(train_pool_full, use_best_model=False)\n",
    "\n",
    "        dev_pool = Pool(X_dev, label=y_dev, cat_features=cat_indices)\n",
    "        proba_dev = final_model_dev.predict_proba(dev_pool)[:, 1]\n",
    "        thr_prod, pr_auc_dev = pick_threshold_precision_first(\n",
    "            y_dev, proba_dev, min_precision=min_precision, min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "        )\n",
    "        dev_metrics = metrics_at_threshold(y_dev, proba_dev, thr_prod)\n",
    "        dev_metrics.update({'pr_auc': float(pr_auc_dev), 'threshold': float(thr_prod)})\n",
    "        print(\"\\nDEV calibration metrics (used for PROD threshold):\")\n",
    "        print(json.dumps(dev_metrics, indent=2))\n",
    "\n",
    "        del final_model_dev, train_pool_full, dev_pool\n",
    "        cleanup()\n",
    "\n",
    "    # ---------- FINAL model ----------\n",
    "    Xb_full, yb_full, _ = sample_negatives_hard(\n",
    "        X_tr, y_tr, dtf_tr, lookahead=lookahead_days,\n",
    "        neg_pos_ratio=neg_pos_ratio, hard_window=hard_window,\n",
    "        hard_fraction=hard_fraction, seed=random_state\n",
    "    )\n",
    "    final_model = get_catboost(\n",
    "        depth=cb_depth, iterations=cb_iterations, learning_rate=cb_learning_rate,\n",
    "        l2_leaf_reg=cb_l2_leaf_reg, border_count=cb_border_count,\n",
    "        bootstrap_type=cb_bootstrap_type, subsample=cb_subsample,\n",
    "        gpu_ram_part=cb_gpu_ram_part, random_seed=random_state\n",
    "    )\n",
    "    train_pool_full = Pool(Xb_full, label=yb_full, cat_features=cat_indices)\n",
    "    final_model.fit(train_pool_full, use_best_model=False)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_prefix = os.path.join(output_dir, f\"{dataset_type}_cb_precv72_{timestamp}\")\n",
    "    final_model.save_model(f\"{model_prefix}.cbm\")\n",
    "    features_meta = {\n",
    "        'feature_names': feature_names,\n",
    "        'cat_indices': cat_indices,\n",
    "        'threshold': float(thr_prod),\n",
    "        'calibration': 'dev_years' if not df_dev.empty else 'oof_global',\n",
    "        'catboost_params': {\n",
    "            'depth': cb_depth, 'iterations': cb_iterations, 'learning_rate': cb_learning_rate,\n",
    "            'l2_leaf_reg': cb_l2_leaf_reg, 'border_count': cb_border_count,\n",
    "            'bootstrap_type': cb_bootstrap_type, 'subsample': cb_subsample,\n",
    "            'gpu_ram_part': cb_gpu_ram_part\n",
    "        },\n",
    "        'smart_cols': smart_cols,\n",
    "        'stats_model': stats_model,   # para normalización streaming\n",
    "        'stats_global': stats_global  # fallback global\n",
    "    }\n",
    "    with open(f\"{model_prefix}_features.json\", 'w') as f:\n",
    "        json.dump(features_meta, f, indent=2)\n",
    "    print(f\"\\n✓ Final model saved: {model_prefix}.cbm\")\n",
    "\n",
    "    # ---------- TEST (RAM normal) opcional ----------\n",
    "    test_metrics = None\n",
    "    if not df_te.empty and (not test_big_mode):\n",
    "        y_test = create_labels_from_dtf(compute_days_to_failure(df_te), lookahead_days)\n",
    "        test_pool = Pool(X_te, label=y_test, cat_features=cat_indices)\n",
    "        proba_test = final_model.predict_proba(test_pool)[:, 1]\n",
    "        test_metrics = metrics_at_threshold(y_test, proba_test, float(thr_prod))\n",
    "        test_metrics.update({'pr_auc': float(average_precision_score(y_test, proba_test)), 'threshold_used': float(thr_prod)})\n",
    "\n",
    "    # ---------- Metadata ----------\n",
    "    metadata = {\n",
    "        'dataset_type': dataset_type,\n",
    "        'train_years': train_years, 'dev_years': dev_years, 'test_years': test_years,\n",
    "        'lookahead_days': lookahead_days,\n",
    "        'n_splits': n_splits,\n",
    "        'neg_pos_ratio': neg_pos_ratio, 'hard_window': hard_window, 'hard_fraction': hard_fraction,\n",
    "        'catboost_params': {\n",
    "            'depth': cb_depth, 'iterations': cb_iterations, 'learning_rate': cb_learning_rate,\n",
    "            'l2_leaf_reg': cb_l2_leaf_reg, 'border_count': cb_border_count,\n",
    "            'bootstrap_type': cb_bootstrap_type, 'subsample': cb_subsample,\n",
    "            'task_type': 'GPU', 'gpu_ram_part': cb_gpu_ram_part, 'logging_level': 'Silent'\n",
    "        },\n",
    "        'min_precision': min_precision, 'min_recall': min_recall,\n",
    "        'top_k_rate': top_k_rate, 'min_alerts': min_alerts,\n",
    "        'oof_metrics': {\n",
    "            'precision': agg_metrics['precision'], 'recall': agg_metrics['recall'],\n",
    "            'f1': agg_metrics['f1'], 'confusion_matrix': agg_metrics['confusion_matrix'],\n",
    "            'fpr': agg_metrics['fpr'], 'pr_auc': agg_metrics['pr_auc'],\n",
    "            'threshold': agg_metrics['threshold']\n",
    "        },\n",
    "        'dev_metrics': dev_metrics, 'test_metrics': test_metrics,\n",
    "        'feature_names': feature_names, 'cat_indices': cat_indices,\n",
    "        'smart_cols': smart_cols\n",
    "    }\n",
    "    meta_path = os.path.join(output_dir, f\"{dataset_type}_cb_precv72_{timestamp}_metadata.json\")\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"\\n✓ Metadata saved: {meta_path}\")\n",
    "    print(\"=\"*100)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# ===================== TEST-ONLY 2024 (STREAMING, BIG) =====================\n",
    "\n",
    "class StreamCatFeaturesBuilder:\n",
    "    \"\"\"\n",
    "    Construye features de TEST en streaming (sin rolling) con:\n",
    "      • delta / cummax / age_days por serial\n",
    "      • calendario\n",
    "      • model & vendor categóricas (strings)\n",
    "      • normalización por modelo (z_*, log1p_*) usando stats_model + stats_global\n",
    "    \"\"\"\n",
    "    def __init__(self, smart_cols: List[str], stats_model: Dict, stats_global: Dict):\n",
    "        self.smart_cols = smart_cols\n",
    "        self.stats_model = stats_model\n",
    "        self.stats_global = stats_global\n",
    "        self.last_vals: Dict[str, Dict[str, float]] = {}\n",
    "        self.cummax_vals: Dict[str, Dict[str, float]] = {}\n",
    "        self.age: Dict[str, int] = {}\n",
    "\n",
    "    def _ensure_serial(self, sn: str):\n",
    "        if sn not in self.last_vals:\n",
    "            self.last_vals[sn] = {c: 0.0 for c in self.smart_cols}\n",
    "            self.cummax_vals[sn] = {c: 0.0 for c in self.smart_cols}\n",
    "            self.age[sn] = 0\n",
    "\n",
    "    def transform_chunk(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.sort_values([\"serial_number\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "        # Base types\n",
    "        df[\"model\"] = df.get(\"model\", \"UNK\").astype(str).fillna(\"UNK\")\n",
    "        df[\"vendor\"] = extract_vendor(df[\"model\"]).astype(str).fillna(\"UNK\")\n",
    "        df[\"month\"] = df[\"date\"].dt.month\n",
    "        df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n",
    "\n",
    "        for c in self.smart_cols:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        # delta/cummax/age\n",
    "        deltas = {f\"delta_{c}\": [] for c in self.smart_cols}\n",
    "        cummaxs = {f\"max_{c}\": [] for c in self.smart_cols}\n",
    "        ages = []\n",
    "        serials = df[\"serial_number\"].astype(str).values\n",
    "\n",
    "        for idx, sn in enumerate(serials):\n",
    "            self._ensure_serial(sn)\n",
    "            ages.append(self.age[sn])\n",
    "            self.age[sn] += 1\n",
    "            for c in self.smart_cols:\n",
    "                v = float(df.at[idx, c])\n",
    "                d = v - self.last_vals[sn][c]\n",
    "                self.last_vals[sn][c] = v\n",
    "                self.cummax_vals[sn][c] = max(self.cummax_vals[sn][c], v)\n",
    "                deltas[f\"delta_{c}\"].append(d)\n",
    "                cummaxs[f\"max_{c}\"].append(self.cummax_vals[sn][c])\n",
    "\n",
    "        for k, arr in deltas.items():\n",
    "            df[k] = arr\n",
    "        for k, arr in cummaxs.items():\n",
    "            df[k] = arr\n",
    "        df[\"age_days\"] = ages\n",
    "\n",
    "        # normalización por modelo (vectorizada por chunk)\n",
    "        model_series = df[\"model\"].astype(str)\n",
    "        for c in self.smart_cols:\n",
    "            med_map = {m: v[c][0] for m, v in self.stats_model.items() if c in v}\n",
    "            iqr_map = {m: v[c][1] for m, v in self.stats_model.items() if c in v}\n",
    "            g_med, g_iqr = self.stats_global.get(c, (0.0, 1.0))\n",
    "            med_s = model_series.map(med_map).fillna(g_med).astype(np.float32)\n",
    "            iqr_s = model_series.map(iqr_map).fillna(g_iqr).astype(np.float32)\n",
    "            col = pd.to_numeric(df[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "            z = (col - med_s) / (iqr_s + 1e-6)\n",
    "            df[f'z_{c}'] = z.values\n",
    "            df[f'log1p_{c}'] = np.log1p(np.maximum(col.values, 0.0)).astype(np.float32)\n",
    "\n",
    "        # posibles features\n",
    "        feat_cols = (\n",
    "            [\"model\", \"vendor\", \"capacity_bytes\"]\n",
    "            + self.smart_cols\n",
    "            + [f\"delta_{c}\" for c in self.smart_cols]\n",
    "            + [f\"max_{c}\" for c in self.smart_cols]\n",
    "            + [f\"z_{c}\" for c in self.smart_cols]\n",
    "            + [f\"log1p_{c}\" for c in self.smart_cols]\n",
    "            + [\"age_days\", \"month\", \"day_of_week\"]\n",
    "        )\n",
    "        return df[feat_cols]\n",
    "\n",
    "\n",
    "def evaluate_saved_catboost_2024_streaming(\n",
    "    model_path: str,\n",
    "    features_meta_path: str,  # *_features.json (feature_names, thr, cat_indices, stats)\n",
    "    parquet_path: str,\n",
    "    test_years: List[int] = [2024],\n",
    "    lookahead_days: int = 7,\n",
    "    chunk_limit_groups: int = 0  # 0 = todos los row-groups\n",
    "):\n",
    "    \"\"\"\n",
    "    Evalúa 2024 en streaming:\n",
    "      • NO carga todo 2024 a RAM.\n",
    "      • Labels con fail_map del TEST (lookahead).\n",
    "      • Features sin rolling (delta, cummax, age_days) + normalización por modelo (stats entrenadas).\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"EVALUATE SAVED CATBOOST — TEST 2024 (STREAMING)\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    model = CatBoostClassifier()\n",
    "    model.load_model(model_path)\n",
    "    with open(features_meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    feature_names = meta[\"feature_names\"]\n",
    "    thr = float(meta.get(\"threshold\", 0.5))\n",
    "    cat_indices = meta.get(\"cat_indices\", [])\n",
    "    smart_cols = meta.get(\"smart_cols\", [])\n",
    "    stats_model = meta.get(\"stats_model\", {})\n",
    "    stats_global = meta.get(\"stats_global\", {})\n",
    "\n",
    "    base_cols = ['serial_number', 'date', 'failure', 'model', 'capacity_bytes']\n",
    "    read_cols = base_cols + smart_cols\n",
    "\n",
    "    fail_map_test = scan_fail_dates(parquet_path, test_years)\n",
    "    builder = StreamCatFeaturesBuilder(smart_cols=smart_cols, stats_model=stats_model, stats_global=stats_global)\n",
    "\n",
    "    tn=fp=fn=tp=0\n",
    "    disk_stat: Dict[str, Tuple[float, int]] = {}\n",
    "    processed_groups = 0\n",
    "\n",
    "    for df in _iter_parquet_row_groups(parquet_path, test_years, columns=read_cols):\n",
    "        if chunk_limit_groups and processed_groups >= chunk_limit_groups:\n",
    "            break\n",
    "\n",
    "        # Labels\n",
    "        sn_ser = df[\"serial_number\"].astype(str)\n",
    "        dt = pd.to_datetime(df[\"date\"], errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "        fdt = pd.to_datetime(sn_ser.map(fail_map_test), errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "        dtf = np.full(len(df), 10**9, dtype=np.int64)\n",
    "        valid = ~np.isnat(fdt)\n",
    "        if valid.any():\n",
    "            dd = (fdt[valid] - dt[valid]).astype(\"timedelta64[D]\").astype(\"int64\")\n",
    "            dtf[valid] = dd\n",
    "        y_chunk = ((dtf >= 0) & (dtf <= lookahead_days)).astype(np.int8)\n",
    "\n",
    "        # Features streaming\n",
    "        X_chunk = builder.transform_chunk(df)\n",
    "\n",
    "        # Alinear columnas al orden del modelo (completar faltantes)\n",
    "        for col in feature_names:\n",
    "            if col not in X_chunk.columns:\n",
    "                # categóricas por si acaso\n",
    "                if col in [\"model\", \"vendor\"]:\n",
    "                    X_chunk[col] = \"UNK\"\n",
    "                else:\n",
    "                    X_chunk[col] = 0.0\n",
    "        X_chunk = X_chunk[feature_names]\n",
    "        # Pool con cat_features\n",
    "        pool = Pool(X_chunk, label=y_chunk, cat_features=cat_indices)\n",
    "        proba = model.predict_proba(pool)[:, 1]\n",
    "        y_pred = (proba >= thr).astype(np.int8)\n",
    "\n",
    "        cm = confusion_matrix(y_chunk, y_pred, labels=[0,1])\n",
    "        tn += int(cm[0,0]); fp += int(cm[0,1]); fn += int(cm[1,0]); tp += int(cm[1,1])\n",
    "\n",
    "        for s, p, y in zip(sn_ser.values, proba, y_chunk):\n",
    "            if s not in disk_stat:\n",
    "                disk_stat[s] = (p, int(y))\n",
    "            else:\n",
    "                mp, my = disk_stat[s]\n",
    "                disk_stat[s] = (max(mp, p), max(my, int(y)))\n",
    "\n",
    "        processed_groups += 1\n",
    "        cleanup()\n",
    "\n",
    "    row_metrics = {\n",
    "        'precision': float(tp / max(1, tp + fp)),\n",
    "        'recall': float(tp / max(1, tp + fn)),\n",
    "        'f1': float((2*tp) / max(1, 2*tp + fp + fn)),\n",
    "        'confusion_matrix': [[tn, fp], [fn, tp]],\n",
    "        'fpr': float(fp / max(1, fp + tn))\n",
    "    }\n",
    "\n",
    "    y_disk = np.array([v[1] for v in disk_stat.values()], dtype=np.int8)\n",
    "    yhat_disk = np.array([1 if v[0] >= thr else 0 for v in disk_stat.values()], dtype=np.int8)\n",
    "    cm_d = confusion_matrix(y_disk, yhat_disk, labels=[0,1])\n",
    "    tn_d, fp_d, fn_d, tp_d = int(cm_d[0,0]), int(cm_d[0,1]), int(cm_d[1,0]), int(cm_d[1,1])\n",
    "    disk_metrics = {\n",
    "        'precision': float(tp_d / max(1, tp_d + fp_d)),\n",
    "        'recall': float(tp_d / max(1, tp_d + fn_d)),\n",
    "        'f1': float((2*tp_d) / max(1, 2*tp_d + fp_d + fn_d)),\n",
    "        'confusion_matrix': [[tn_d, fp_d], [fn_d, tp_d]],\n",
    "        'n_disks': int(len(disk_stat))\n",
    "    }\n",
    "\n",
    "    print(\"\\nRow-level TEST metrics (streaming 2024):\")\n",
    "    print(json.dumps(row_metrics, indent=2))\n",
    "    print(\"\\nDisk-level TEST metrics (streaming 2024):\")\n",
    "    print(json.dumps(disk_metrics, indent=2))\n",
    "    print(\"=\"*100)\n",
    "    return {\"row\": row_metrics, \"disk\": disk_metrics}\n",
    "\n",
    "\n",
    "# ===================== WRAPPERS =====================\n",
    "\n",
    "def train_ssd_precision_v72_catboost():\n",
    "    # SSD cabe en RAM → rolling ON; test normal; dev normal\n",
    "    return train_catboost_precision_pipeline_v72(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        train_years=[2020, 2021, 2022],\n",
    "        dev_years=[2023],\n",
    "        test_years=[2024],\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "        neg_pos_ratio=3,\n",
    "        hard_window=60,\n",
    "        hard_fraction=0.7,\n",
    "        cb_depth=8,\n",
    "        cb_iterations=1500,\n",
    "        cb_learning_rate=0.06,\n",
    "        cb_l2_leaf_reg=4.0,\n",
    "        cb_border_count=64,\n",
    "        cb_bootstrap_type='Bernoulli',\n",
    "        cb_subsample=0.8,\n",
    "        cb_gpu_ram_part=0.70,\n",
    "        min_precision=0.90,\n",
    "        min_recall=0.03,\n",
    "        top_k_rate=1e-4,\n",
    "        min_alerts=20,\n",
    "        output_dir='./models_cb_ssd_precv72',\n",
    "        big_mode=False,\n",
    "        dev_big_mode=False,\n",
    "        test_big_mode=False,\n",
    "        neg_random_keep_rate_train=0.0,\n",
    "        neg_random_keep_rate_dev=0.0,\n",
    "        add_rolling=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def train_hdd_precision_v72_catboost_big(neg_random_keep_rate_train: float = 0.0025):\n",
    "    # HDD enorme → TRAIN big + DEV big + TEST streaming; rolling OFF\n",
    "    return train_catboost_precision_pipeline_v72(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        dataset_type='HDD',\n",
    "        train_years=[2020, 2021, 2022],\n",
    "        dev_years=[2023],\n",
    "        test_years=[2024],\n",
    "        lookahead_days=7,\n",
    "        n_splits=3,               # menos folds para RAM/tiempo\n",
    "        neg_pos_ratio=5,\n",
    "        hard_window=90,\n",
    "        hard_fraction=0.7,\n",
    "        cb_depth=6,\n",
    "        cb_iterations=1000,\n",
    "        cb_learning_rate=0.06,\n",
    "        cb_l2_leaf_reg=4.0,\n",
    "        cb_border_count=32,\n",
    "        cb_bootstrap_type='Bernoulli',\n",
    "        cb_subsample=0.7,\n",
    "        cb_gpu_ram_part=0.60,\n",
    "        min_precision=0.90,\n",
    "        min_recall=0.03,\n",
    "        top_k_rate=7.5e-5,\n",
    "        min_alerts=30,\n",
    "        output_dir='./models_cb_hdd_precv72',\n",
    "        big_mode=True,\n",
    "        dev_big_mode=True,\n",
    "        test_big_mode=True,      # no evaluamos test aquí; se hace con la función streaming aparte\n",
    "        neg_random_keep_rate_train=neg_random_keep_rate_train,\n",
    "        neg_random_keep_rate_dev=0.01,\n",
    "        add_rolling=False,\n",
    "        random_state=42\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e21ed5-6a20-45e8-9863-9a1eb0b55449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "CATBOOST (GPU, PRECISION-CONTROLLED v7.1) - SSD (Train=[2020, 2021, 2022] | Dev=[2023] | Test=[2024])\n",
      "============================================================================================\n",
      "Loading [2020, 2021, 2022] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 987,010 rows\n",
      "Loading [2023] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 1,137,101 rows\n",
      "Loading [2024] from ./Procesados/finales/SSD_FULL_CLEAN.parquet...\n",
      "Loaded 1,220,745 rows\n",
      "TRAIN labels: 1,070 pos (0.1084%)\n",
      "Creating features (temporal join) for SSD with TRAIN∪DEV∪TEST...\n",
      "  Found 13 SMART attributes (raw)\n",
      "  Final features: 97 (cat=2, num≈95)\n",
      "\n",
      "Fold 1/5: train=791,918 (pos=854) | val=195,092 (pos=216)\n",
      "  After hard-neg sampling: 3,416 (pos=854, neg=2,562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2/5: train=786,233 (pos=859) | val=200,777 (pos=211)\n",
      "  After hard-neg sampling: 3,436 (pos=859, neg=2,577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3/5: train=800,508 (pos=855) | val=186,502 (pos=215)\n",
      "  After hard-neg sampling: 3,420 (pos=855, neg=2,565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4/5: train=779,562 (pos=858) | val=207,448 (pos=212)\n",
      "  After hard-neg sampling: 3,432 (pos=858, neg=2,574)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5/5: train=789,819 (pos=854) | val=197,191 (pos=216)\n",
      "  After hard-neg sampling: 3,416 (pos=854, neg=2,562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF (diag, not used for prod):\n",
      "{\n",
      "  \"precision\": 1.0,\n",
      "  \"recall\": 0.0308411214953271,\n",
      "  \"f1\": 0.05983680870353581,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      985940,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      1037,\n",
      "      33\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 0.0,\n",
      "  \"pr_auc\": 0.3743214108139969,\n",
      "  \"threshold\": 0.8701784014701843\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEV calibration metrics (used for PROD threshold):\n",
      "{\n",
      "  \"precision\": 0.0,\n",
      "  \"recall\": 0.0,\n",
      "  \"f1\": 0.0,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      1136733,\n",
      "      113\n",
      "    ],\n",
      "    [\n",
      "      255,\n",
      "      0\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 9.939780761862204e-05,\n",
      "  \"pr_auc\": 0.00039457193290159346,\n",
      "  \"threshold\": 0.8728576110992888\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Final model saved: ./models_cb_ssd_precv7/SSD_cb_precv7_20251105_204900.cbm\n",
      "\n",
      "✓ Metadata saved: ./models_cb_ssd_precv7/SSD_cb_precv7_20251105_204900_metadata.json\n",
      "============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'SSD',\n",
       " 'train_years': [2020, 2021, 2022],\n",
       " 'dev_years': [2023],\n",
       " 'test_years': [2024],\n",
       " 'lookahead_days': 7,\n",
       " 'n_splits': 5,\n",
       " 'neg_pos_ratio': 3,\n",
       " 'hard_window': 60,\n",
       " 'hard_fraction': 0.7,\n",
       " 'catboost_params': {'depth': 8,\n",
       "  'iterations': 1500,\n",
       "  'learning_rate': 0.06,\n",
       "  'l2_leaf_reg': 4.0,\n",
       "  'border_count': 64,\n",
       "  'bootstrap_type': 'Bernoulli',\n",
       "  'subsample': 0.8,\n",
       "  'task_type': 'GPU',\n",
       "  'logging_level': 'Silent'},\n",
       " 'min_precision': 0.9,\n",
       " 'min_recall': 0.03,\n",
       " 'top_k_rate': 0.0001,\n",
       " 'min_alerts': 20,\n",
       " 'oof_metrics': {'precision': 1.0,\n",
       "  'recall': 0.0308411214953271,\n",
       "  'f1': 0.05983680870353581,\n",
       "  'confusion_matrix': [[985940, 0], [1037, 33]],\n",
       "  'fpr': 0.0,\n",
       "  'pr_auc': 0.3743214108139969,\n",
       "  'threshold': 0.8701784014701843},\n",
       " 'dev_metrics': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'confusion_matrix': [[1136733, 113], [255, 0]],\n",
       "  'fpr': 9.939780761862204e-05,\n",
       "  'pr_auc': 0.00039457193290159346,\n",
       "  'threshold': 0.8728576110992888},\n",
       " 'test_metrics': {'precision': 0.010062893081761006,\n",
       "  'recall': 0.03524229074889868,\n",
       "  'f1': 0.015655577299412915,\n",
       "  'confusion_matrix': [[1219731, 787], [219, 8]],\n",
       "  'fpr': 0.000644808187998866,\n",
       "  'pr_auc': 0.0006077847191286913,\n",
       "  'threshold_used': 0.8728576110992888},\n",
       " 'feature_names': ['model',\n",
       "  'vendor',\n",
       "  'capacity_bytes',\n",
       "  'smart_5_raw',\n",
       "  'smart_187_raw',\n",
       "  'smart_194_raw',\n",
       "  'smart_231_raw',\n",
       "  'smart_233_raw',\n",
       "  'smart_241_raw',\n",
       "  'smart_242_raw',\n",
       "  'smart_9_raw',\n",
       "  'smart_173_raw',\n",
       "  'smart_174_raw',\n",
       "  'smart_184_raw',\n",
       "  'smart_199_raw',\n",
       "  'smart_230_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'delta_smart_194_raw',\n",
       "  'max_smart_194_raw',\n",
       "  'delta_smart_231_raw',\n",
       "  'max_smart_231_raw',\n",
       "  'delta_smart_233_raw',\n",
       "  'max_smart_233_raw',\n",
       "  'delta_smart_241_raw',\n",
       "  'max_smart_241_raw',\n",
       "  'delta_smart_242_raw',\n",
       "  'max_smart_242_raw',\n",
       "  'delta_smart_9_raw',\n",
       "  'max_smart_9_raw',\n",
       "  'delta_smart_173_raw',\n",
       "  'max_smart_173_raw',\n",
       "  'delta_smart_174_raw',\n",
       "  'max_smart_174_raw',\n",
       "  'delta_smart_184_raw',\n",
       "  'max_smart_184_raw',\n",
       "  'delta_smart_199_raw',\n",
       "  'max_smart_199_raw',\n",
       "  'delta_smart_230_raw',\n",
       "  'max_smart_230_raw',\n",
       "  'rmean7_smart_5_raw',\n",
       "  'rstd7_smart_5_raw',\n",
       "  'rmean7_smart_187_raw',\n",
       "  'rstd7_smart_187_raw',\n",
       "  'rmean7_smart_194_raw',\n",
       "  'rstd7_smart_194_raw',\n",
       "  'rmean7_smart_231_raw',\n",
       "  'rstd7_smart_231_raw',\n",
       "  'rmean7_smart_233_raw',\n",
       "  'rstd7_smart_233_raw',\n",
       "  'rmean7_smart_241_raw',\n",
       "  'rstd7_smart_241_raw',\n",
       "  'rmean7_smart_242_raw',\n",
       "  'rstd7_smart_242_raw',\n",
       "  'rmean7_smart_9_raw',\n",
       "  'rstd7_smart_9_raw',\n",
       "  'rmean7_smart_173_raw',\n",
       "  'rstd7_smart_173_raw',\n",
       "  'rmean7_smart_174_raw',\n",
       "  'rstd7_smart_174_raw',\n",
       "  'rmean7_smart_184_raw',\n",
       "  'rstd7_smart_184_raw',\n",
       "  'rmean7_smart_199_raw',\n",
       "  'rstd7_smart_199_raw',\n",
       "  'rmean7_smart_230_raw',\n",
       "  'rstd7_smart_230_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week',\n",
       "  'z_smart_5_raw',\n",
       "  'log1p_smart_5_raw',\n",
       "  'z_smart_187_raw',\n",
       "  'log1p_smart_187_raw',\n",
       "  'z_smart_194_raw',\n",
       "  'log1p_smart_194_raw',\n",
       "  'z_smart_231_raw',\n",
       "  'log1p_smart_231_raw',\n",
       "  'z_smart_233_raw',\n",
       "  'log1p_smart_233_raw',\n",
       "  'z_smart_241_raw',\n",
       "  'log1p_smart_241_raw',\n",
       "  'z_smart_242_raw',\n",
       "  'log1p_smart_242_raw',\n",
       "  'z_smart_9_raw',\n",
       "  'log1p_smart_9_raw',\n",
       "  'z_smart_173_raw',\n",
       "  'log1p_smart_173_raw',\n",
       "  'z_smart_174_raw',\n",
       "  'log1p_smart_174_raw',\n",
       "  'z_smart_184_raw',\n",
       "  'log1p_smart_184_raw',\n",
       "  'z_smart_199_raw',\n",
       "  'log1p_smart_199_raw',\n",
       "  'z_smart_230_raw',\n",
       "  'log1p_smart_230_raw'],\n",
       " 'cat_indices': [0, 1],\n",
       " 'normalization': 'per-model robust z + log1p on *_raw'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_precision_v7_catboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27351330-c796-4eb7-92d4-53a7fa990883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "CATBOOST (GPU, PRECISION-CONTROLLED v7.2) - HDD\n",
      "====================================================================================================\n",
      "Scanning earliest failure dates (streaming)...\n",
      "  Found 3,725 failed serials\n",
      "Loading BIG-FILTERED [2020, 2021, 2022] from ./Procesados/finales/HDD_FULL_CLEAN.parquet (keep_rate=0.0025)...\n",
      "  BIG-FILTERED kept 613,954/125,216,564 rows (~0.49%)\n",
      "Scanning earliest failure dates (streaming)...\n",
      "  Found 4,321 failed serials\n",
      "Loading BIG-FILTERED [2023] from ./Procesados/finales/HDD_FULL_CLEAN.parquet (keep_rate=0.01)...\n",
      "  BIG-FILTERED kept 1,246,686/90,546,032 rows (~1.38%)\n",
      "TRAIN labels: 29,043 pos (4.7305%)\n",
      "Creating features (temporal join) for HDD with TRAIN∪DEV∪TEST...\n",
      "  Found 5 SMART attributes (raw)\n",
      "  Final features: 31 (cat=2, num≈29)\n",
      "\n",
      "Fold 1/3: train=408,717 (pos=19,314) | val=205,237 (pos=9,729)\n",
      "  After hard-neg sampling: 115,884 (pos=19,314, neg=96,570)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2/3: train=410,379 (pos=19,398) | val=203,575 (pos=9,645)\n",
      "  After hard-neg sampling: 116,388 (pos=19,398, neg=96,990)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3/3: train=408,812 (pos=19,374) | val=205,142 (pos=9,669)\n",
      "  After hard-neg sampling: 116,244 (pos=19,374, neg=96,870)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF (diag, not used for prod):\n",
      "{\n",
      "  \"precision\": 0.9988545246277205,\n",
      "  \"recall\": 0.030024446510346726,\n",
      "  \"f1\": 0.05829656371172617,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      584910,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      28171,\n",
      "      872\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 1.7096618117970084e-06,\n",
      "  \"pr_auc\": 0.9128897872007613,\n",
      "  \"threshold\": 0.9994227886199951\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEV calibration metrics (used for PROD threshold):\n",
      "{\n",
      "  \"precision\": 0.9726027397260274,\n",
      "  \"recall\": 0.03002658289028516,\n",
      "  \"f1\": 0.05825470315888179,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      1213554,\n",
      "      28\n",
      "    ],\n",
      "    [\n",
      "      32110,\n",
      "      994\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 2.30721945447444e-05,\n",
      "  \"pr_auc\": 0.7992707484853779,\n",
      "  \"threshold\": 0.9997780998603982\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Final model saved: ./models_cb_hdd_precv72/HDD_cb_precv72_20251106_083714.cbm\n",
      "\n",
      "✓ Metadata saved: ./models_cb_hdd_precv72/HDD_cb_precv72_20251106_083714_metadata.json\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'HDD',\n",
       " 'train_years': [2020, 2021, 2022],\n",
       " 'dev_years': [2023],\n",
       " 'test_years': [2024],\n",
       " 'lookahead_days': 7,\n",
       " 'n_splits': 3,\n",
       " 'neg_pos_ratio': 5,\n",
       " 'hard_window': 90,\n",
       " 'hard_fraction': 0.7,\n",
       " 'catboost_params': {'depth': 6,\n",
       "  'iterations': 1000,\n",
       "  'learning_rate': 0.06,\n",
       "  'l2_leaf_reg': 4.0,\n",
       "  'border_count': 32,\n",
       "  'bootstrap_type': 'Bernoulli',\n",
       "  'subsample': 0.7,\n",
       "  'task_type': 'GPU',\n",
       "  'gpu_ram_part': 0.6,\n",
       "  'logging_level': 'Silent'},\n",
       " 'min_precision': 0.9,\n",
       " 'min_recall': 0.03,\n",
       " 'top_k_rate': 7.5e-05,\n",
       " 'min_alerts': 30,\n",
       " 'oof_metrics': {'precision': 0.9988545246277205,\n",
       "  'recall': 0.030024446510346726,\n",
       "  'f1': 0.05829656371172617,\n",
       "  'confusion_matrix': [[584910, 1], [28171, 872]],\n",
       "  'fpr': 1.7096618117970084e-06,\n",
       "  'pr_auc': 0.9128897872007613,\n",
       "  'threshold': 0.9994227886199951},\n",
       " 'dev_metrics': {'precision': 0.9726027397260274,\n",
       "  'recall': 0.03002658289028516,\n",
       "  'f1': 0.05825470315888179,\n",
       "  'confusion_matrix': [[1213554, 28], [32110, 994]],\n",
       "  'fpr': 2.30721945447444e-05,\n",
       "  'pr_auc': 0.7992707484853779,\n",
       "  'threshold': 0.9997780998603982},\n",
       " 'test_metrics': None,\n",
       " 'feature_names': ['model',\n",
       "  'vendor',\n",
       "  'capacity_bytes',\n",
       "  'smart_187_raw',\n",
       "  'smart_188_raw',\n",
       "  'smart_197_raw',\n",
       "  'smart_198_raw',\n",
       "  'smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'delta_smart_188_raw',\n",
       "  'max_smart_188_raw',\n",
       "  'delta_smart_197_raw',\n",
       "  'max_smart_197_raw',\n",
       "  'delta_smart_198_raw',\n",
       "  'max_smart_198_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week',\n",
       "  'z_smart_187_raw',\n",
       "  'log1p_smart_187_raw',\n",
       "  'z_smart_188_raw',\n",
       "  'log1p_smart_188_raw',\n",
       "  'z_smart_197_raw',\n",
       "  'log1p_smart_197_raw',\n",
       "  'z_smart_198_raw',\n",
       "  'log1p_smart_198_raw',\n",
       "  'z_smart_5_raw',\n",
       "  'log1p_smart_5_raw'],\n",
       " 'cat_indices': [0, 1],\n",
       " 'smart_cols': ['smart_187_raw',\n",
       "  'smart_188_raw',\n",
       "  'smart_197_raw',\n",
       "  'smart_198_raw',\n",
       "  'smart_5_raw']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hdd_precision_v72_catboost_big()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2fe7c-ea75-4a46-b405-20ab73e2164a",
   "metadata": {},
   "source": [
    "# Demás Técnicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664da0ef-8382-488a-8f42-befe00572dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CATBOOST — PRECISION CONTROLLED (GPU, v8) — Sólo técnicas compatibles CatBoost\n",
    "-------------------------------------------------------------------------------\n",
    "- Categóricas nativas + timestamp (ordered boosting)\n",
    "- Ingeniería causal y normalización robusta por modelo (mediana/IQR)\n",
    "- Hard-negative sampling (+ undersampling opcional)\n",
    "- class_weights (auto o manual) — nunca mezclados con undersampling\n",
    "- Umbral calibrado en DEV (precision-first con recall mínimo)\n",
    "- Guardrails VRAM para RTX 2060 (6GB)\n",
    "\n",
    "Requisitos: pandas, numpy, pyarrow, scikit-learn, imbalanced-learn, catboost\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, json, warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================== Utils =====================\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"GC agresivo para liberar RAM.\"\"\"\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "# ===================== IO =====================\n",
    "\n",
    "def load_data(path: str, years: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"Carga parquet (archivo o carpeta) y filtra por años en 'date'.\"\"\"\n",
    "    print(f\"Loading years={years} from {path} ...\")\n",
    "    if not path:\n",
    "        return pd.DataFrame()\n",
    "    files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.parquet')] if os.path.isdir(path) else [path]\n",
    "    chunks = []\n",
    "    for f in files:\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            df = pf.read_row_group(rg).to_pandas()\n",
    "            if 'date' not in df.columns:\n",
    "                continue\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            df = df[df['date'].dt.year.isin(years)]\n",
    "            if not df.empty:\n",
    "                chunks.append(df)\n",
    "            if len(chunks) >= 20:\n",
    "                chunks = [pd.concat(chunks, ignore_index=True)]\n",
    "                cleanup()\n",
    "    out = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "    print(f\"Loaded {len(out):,} rows\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== Prep =====================\n",
    "\n",
    "def prepare_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Tipa y ordena de forma causal por (serial_number, date).\"\"\"\n",
    "    df = df.copy()\n",
    "    df['serial_number'] = df['serial_number'].astype(str)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['serial_number', 'date']).sort_values(['serial_number', 'date']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================== Labels (Days-To-Failure) =====================\n",
    "\n",
    "def compute_days_to_failure(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Días hasta el primer fallo por disco (>=0 antes del fallo).\"\"\"\n",
    "    fail_map: Dict[str, pd.Timestamp] = {}\n",
    "    fails = df[df['failure'] == 1]\n",
    "    for sn, dt in zip(fails['serial_number'], fails['date']):\n",
    "        fail_map[sn] = min(dt, fail_map.get(sn, dt))\n",
    "    dtf = np.full(len(df), 1_000_000_000, dtype=np.int64)\n",
    "    for i, (sn, dt) in enumerate(zip(df['serial_number'], df['date'])):\n",
    "        if sn in fail_map:\n",
    "            dtf[i] = (fail_map[sn] - dt).days\n",
    "    return dtf\n",
    "\n",
    "\n",
    "def create_labels_from_dtf(dtf: np.ndarray, lookahead: int = 7) -> np.ndarray:\n",
    "    \"\"\"Etiqueta positiva si está a [0, lookahead] días del fallo.\"\"\"\n",
    "    return ((dtf >= 0) & (dtf <= lookahead)).astype(np.int8)\n",
    "\n",
    "\n",
    "# ===================== Categóricas =====================\n",
    "\n",
    "def extract_vendor(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    return s.str.extract(r\"^([A-Za-z]+)\", expand=False).fillna(\"UNK\")\n",
    "\n",
    "\n",
    "# ===================== Normalización por modelo =====================\n",
    "\n",
    "def fit_model_stats(df_train: pd.DataFrame, smart_cols: List[str]) -> Dict[str, Dict[str, Tuple[float, float]]]:\n",
    "    \"\"\"Mediana e IQR por (model, atributo) aprendidos en TRAIN.\"\"\"\n",
    "    stats: Dict[str, Dict[str, Tuple[float, float]]] = {}\n",
    "    if 'model' not in df_train.columns or df_train.empty:\n",
    "        return stats\n",
    "    for m, g in df_train.groupby(df_train['model'].astype(str), sort=False):\n",
    "        d = {}\n",
    "        for c in smart_cols:\n",
    "            x = pd.to_numeric(g[c], errors='coerce')\n",
    "            x = x[np.isfinite(x)]\n",
    "            if x.empty:\n",
    "                continue\n",
    "            med = float(np.median(x))\n",
    "            q1, q3 = np.percentile(x, [25, 75])\n",
    "            iqr = float(max(q3 - q1, 1e-6))\n",
    "            d[c] = (med, iqr)\n",
    "        stats[str(m)] = d\n",
    "    return stats\n",
    "\n",
    "\n",
    "def apply_model_normalization(df_all: pd.DataFrame, smart_cols: List[str], stats: Dict[str, Dict[str, Tuple[float, float]]]):\n",
    "    \"\"\"z-score robusto por modelo + log1p para atributos SMART *_raw.\"\"\"\n",
    "    global_median, global_iqr = {}, {}\n",
    "    for c in smart_cols:\n",
    "        x = pd.to_numeric(df_all[c], errors='coerce').fillna(0.0).values\n",
    "        global_median[c] = float(np.median(x))\n",
    "        q1, q3 = np.percentile(x, [25, 75])\n",
    "        global_iqr[c] = float(max(q3 - q1, 1e-6))\n",
    "    model_series = df_all['model'].astype(str)\n",
    "    for c in smart_cols:\n",
    "        med_map = {m: v[c][0] for m, v in stats.items() if c in v}\n",
    "        iqr_map = {m: v[c][1] for m, v in stats.items() if c in v}\n",
    "        med_s = model_series.map(med_map).fillna(global_median[c]).astype(np.float32)\n",
    "        iqr_s = model_series.map(iqr_map).fillna(global_iqr[c]).astype(np.float32)\n",
    "        col = pd.to_numeric(df_all[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "        z = (col - med_s) / (iqr_s + 1e-6)\n",
    "        df_all[f'z_{c}'] = z.values\n",
    "        df_all[f'log1p_{c}'] = np.log1p(np.maximum(col.values, 0.0)).astype(np.float32)\n",
    "\n",
    "\n",
    "# ===================== Features (join temporal) =====================\n",
    "\n",
    "def create_features_joined_cat(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_dev: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    dataset_type: str,\n",
    "    add_rolling: bool,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, List[str], List[int]]:\n",
    "    \"\"\"Une TRAIN∪DEV∪TEST, crea features causales y marca categóricas nativas.\"\"\"\n",
    "    print(f\"Creating features (temporal join) for {dataset_type} with TRAIN∪DEV∪TEST...\")\n",
    "\n",
    "    df_train = df_train.copy(); df_train['__subset__'] = 'train'\n",
    "    df_dev   = df_dev.copy();   df_dev['__subset__']   = 'dev'\n",
    "    df_test  = df_test.copy();  df_test['__subset__']  = 'test'\n",
    "    df_all = pd.concat([df_train, df_dev, df_test], ignore_index=True)\n",
    "    df_all.sort_values(['serial_number','date'], inplace=True)\n",
    "\n",
    "    all_cols = df_all.columns.tolist()\n",
    "    smart_cols = [c for c in all_cols if ('smart' in c.lower()) and ('_raw' in c.lower())]\n",
    "    print(f\"  Found {len(smart_cols)} SMART attributes (raw)\")\n",
    "\n",
    "    # Limpia numéricas\n",
    "    for c in smart_cols:\n",
    "        df_all[c] = pd.to_numeric(df_all[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # Deltas y cummax por disco (causal)\n",
    "    for c in smart_cols:\n",
    "        g = df_all.groupby('serial_number', sort=False)[c]\n",
    "        df_all[f'delta_{c}'] = g.diff().fillna(0.0)\n",
    "        df_all[f'max_{c}']   = g.cummax()\n",
    "\n",
    "    # Rolling 7d\n",
    "    if add_rolling:\n",
    "        for c in smart_cols:\n",
    "            r = df_all.groupby('serial_number', sort=False)[c]\n",
    "            df_all[f'rmean7_{c}'] = r.rolling(window=7, min_periods=2).mean().reset_index(level=0, drop=True).fillna(0.0)\n",
    "            df_all[f'rstd7_{c}']  = r.rolling(window=7, min_periods=2).std().reset_index(level=0, drop=True).fillna(0.0)\n",
    "\n",
    "    # Edad y calendario\n",
    "    df_all['age_days'] = df_all.groupby('serial_number', sort=False).cumcount()\n",
    "    d = df_all['date']\n",
    "    df_all['month'] = d.dt.month\n",
    "    df_all['day_of_week'] = d.dt.dayofweek\n",
    "\n",
    "    # Categóricas nativas (+ vendor)\n",
    "    if 'model' not in df_all.columns:\n",
    "        df_all['model'] = 'UNK'\n",
    "    df_all['model'] = df_all['model'].astype(str).fillna('UNK')\n",
    "    df_all['vendor'] = extract_vendor(df_all['model']).astype(str).fillna('UNK')\n",
    "\n",
    "    # Timestamp (CTR temporal)\n",
    "    df_all['ts_sec'] = (df_all['date'].astype('int64') // 10**9).astype(np.int64)\n",
    "\n",
    "    # Normalización por modelo aprendida en TRAIN\n",
    "    stats = fit_model_stats(df_train, smart_cols)\n",
    "    apply_model_normalization(df_all, smart_cols, stats)\n",
    "\n",
    "    # Drop no predictoras\n",
    "    drop_cols = ['serial_number', 'date', 'failure']\n",
    "    X_all = df_all.drop(columns=[c for c in drop_cols if c in df_all.columns], errors='ignore')\n",
    "\n",
    "    # Categóricas nativas\n",
    "    cat_cols = [c for c in ['model','vendor'] if c in X_all.columns]\n",
    "    num_cols = [c for c in X_all.columns if c not in cat_cols + ['ts_sec']]\n",
    "    for c in num_cols:\n",
    "        X_all[c] = pd.to_numeric(X_all[c], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "\n",
    "    # Elimina constantes\n",
    "    var = X_all[num_cols].var()\n",
    "    keep_num = var[var > 0].index.tolist()\n",
    "    X_all = pd.concat([X_all[cat_cols], X_all[keep_num], df_all[['ts_sec']]], axis=1)\n",
    "\n",
    "    feature_names = list(X_all.columns)\n",
    "    cat_indices = [X_all.columns.get_loc(c) for c in cat_cols]\n",
    "    ts_idx = X_all.columns.get_loc('ts_sec')\n",
    "\n",
    "    # Split back\n",
    "    tr_mask  = (df_all['__subset__']=='train').values\n",
    "    dev_mask = (df_all['__subset__']=='dev').values\n",
    "    te_mask  = (df_all['__subset__']=='test').values\n",
    "\n",
    "    X_train = X_all.loc[tr_mask].reset_index(drop=True)\n",
    "    X_dev   = X_all.loc[dev_mask].reset_index(drop=True)\n",
    "    X_test  = X_all.loc[te_mask].reset_index(drop=True)\n",
    "\n",
    "    ts_train = X_train.iloc[:, ts_idx].values.astype(np.int64)\n",
    "    ts_dev   = X_dev.iloc[:, ts_idx].values.astype(np.int64)\n",
    "    ts_test  = X_test.iloc[:, ts_idx].values.astype(np.int64)\n",
    "\n",
    "    # Quitar ts_sec de features\n",
    "    X_train = X_train.drop(columns=['ts_sec'])\n",
    "    X_dev   = X_dev.drop(columns=['ts_sec'])\n",
    "    X_test  = X_test.drop(columns=['ts_sec'])\n",
    "    feature_names.remove('ts_sec')\n",
    "\n",
    "    print(f\"  Final features: {X_train.shape[1]} (cat={len(cat_indices)}, num≈{len(keep_num)})\")\n",
    "    return X_train, X_dev, X_test, ts_train, ts_dev, ts_test, feature_names, cat_indices\n",
    "\n",
    "\n",
    "# ===================== Grouped CV (por disco) =====================\n",
    "\n",
    "def make_group_folds(serials: pd.Series, y: np.ndarray, n_splits: int = 5, random_state: int = 42):\n",
    "    \"\"\"Folds estratificados por disco (cada fold recibe discos con/sin fallo).\"\"\"\n",
    "    serials = serials.astype(str).values\n",
    "    uniq_serials, inverse = np.unique(serials, return_inverse=True)\n",
    "    y_disk = np.zeros(len(uniq_serials), dtype=np.int8)\n",
    "    for i, d_idx in enumerate(inverse):\n",
    "        if y[i] == 1:\n",
    "            y_disk[d_idx] = 1\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for tr_d, va_d in skf.split(uniq_serials, y_disk):\n",
    "        tr_mask = np.isin(inverse, tr_d)\n",
    "        va_mask = np.isin(inverse, va_d)\n",
    "        yield np.where(tr_mask)[0], np.where(va_mask)[0]\n",
    "\n",
    "\n",
    "# ===================== Hard negatives =====================\n",
    "\n",
    "def sample_negatives_hard(\n",
    "    X: pd.DataFrame, y: np.ndarray, dtf: np.ndarray, lookahead: int,\n",
    "    neg_pos_ratio: int = 4, hard_window: int = 60, hard_fraction: float = 0.7,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Selecciona negativos cercanos al fallo (dtf ∈ (lookahead, hard_window]) y completa con fáciles.\n",
    "    Devuelve subset (Xb, yb) + indices (sel_idx) para alinear timestamps.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    if len(pos_idx) == 0:\n",
    "        raise ValueError(\"No positives in training fold for hard-negative sampling\")\n",
    "    n_pos = len(pos_idx)\n",
    "    n_neg_needed = max(n_pos * neg_pos_ratio, 1)\n",
    "\n",
    "    hard_mask = (dtf > lookahead) & (dtf <= hard_window)\n",
    "    hard_idx = np.where((y == 0) & hard_mask)[0]\n",
    "    easy_idx = np.where((y == 0) & (~hard_mask))[0]\n",
    "\n",
    "    n_hard = min(int(n_neg_needed * hard_fraction), len(hard_idx))\n",
    "    n_easy = min(n_neg_needed - n_hard, len(easy_idx))\n",
    "\n",
    "    chosen_hard = rng.choice(hard_idx, size=n_hard, replace=False) if n_hard > 0 else np.empty(0, dtype=int)\n",
    "    chosen_easy = rng.choice(easy_idx, size=n_easy, replace=False) if n_easy > 0 else np.empty(0, dtype=int)\n",
    "\n",
    "    sel_idx = np.sort(np.concatenate([pos_idx, chosen_hard, chosen_easy]))\n",
    "    Xb = X.iloc[sel_idx].reset_index(drop=True)\n",
    "    yb = y[sel_idx]\n",
    "    return Xb, yb, sel_idx\n",
    "\n",
    "\n",
    "# ===================== Balancing (CatBoost-compatible) =====================\n",
    "\n",
    "def normalize_balancing_name(name: str) -> str:\n",
    "    \"\"\"Normaliza alias de estrategia.\"\"\"\n",
    "    n = (name or 'none').strip().lower()\n",
    "    aliases = {\n",
    "        'none': 'none', 'no': 'none',\n",
    "        'under': 'under', 'undersample': 'under',\n",
    "        'class_auto': 'class_auto', 'auto_class_weights': 'class_auto', 'balanced': 'class_auto',\n",
    "        'class_manual': 'class_manual', 'class_weight': 'class_manual'\n",
    "    }\n",
    "    return aliases.get(n, n)\n",
    "\n",
    "\n",
    "def undersample_with_timestamps(X: pd.DataFrame, y: np.ndarray, timestamps: Optional[np.ndarray],\n",
    "                                target_neg_pos: int, max_total: Optional[int], seed: int):\n",
    "    \"\"\"UNDER sample preservando timestamps exactos via _rowid_.\"\"\"\n",
    "    Xu = X.copy()\n",
    "    Xu[\"_rowid_\"] = np.arange(len(Xu), dtype=np.int64)\n",
    "    n_pos = int((y == 1).sum()); n_neg = int((y == 0).sum())\n",
    "    target_pos = n_pos\n",
    "    target_neg = min(n_neg, target_pos * max(1, int(target_neg_pos)))\n",
    "    if max_total:\n",
    "        per_pos = max(1, max_total // (1 + target_neg_pos))\n",
    "        target_pos = min(target_pos, per_pos)\n",
    "        target_neg = min(n_neg, per_pos * target_neg_pos)\n",
    "\n",
    "    rus = RandomUnderSampler(sampling_strategy={0:int(target_neg), 1:int(target_pos)}, random_state=seed)\n",
    "    X_res, y_res = rus.fit_resample(Xu, y)\n",
    "    ts_res = None\n",
    "    if timestamps is not None:\n",
    "        rowid = X_res.pop(\"_rowid_\").astype(int).values\n",
    "        ts_res = timestamps[rowid]\n",
    "    else:\n",
    "        X_res = X_res.drop(columns=[\"_rowid_\"], errors='ignore')\n",
    "    return X_res.reset_index(drop=True), y_res.astype(np.int8), ts_res\n",
    "\n",
    "\n",
    "# ===================== CatBoost (GPU) =====================\n",
    "\n",
    "def get_catboost(\n",
    "    depth: int = 8,\n",
    "    iterations: int = 1500,\n",
    "    learning_rate: float = 0.06,\n",
    "    l2_leaf_reg: float = 4.0,\n",
    "    border_count: int = 64,\n",
    "    rsm: float | None = None,          # <- dejamos el arg para compatibilidad, pero NO se pasa al modelo\n",
    "    random_seed: int = 42,\n",
    "    gpu_ram_part: float = 0.85,\n",
    "    bootstrap_type: str = 'Bernoulli',\n",
    "    subsample: float = 0.8,\n",
    "    balancing_mode: str = 'none',      # 'none'|'class_auto'|'class_manual'\n",
    "    class_weight_ratio: float | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    CatBoost GPU (clasificación). NOTA: 'rsm' no está soportado en GPU salvo pairwise -> no se envía.\n",
    "    \"\"\"\n",
    "    params = dict(\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        iterations=iterations,\n",
    "        depth=depth,\n",
    "        learning_rate=learning_rate,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        border_count=border_count,\n",
    "        # rsm   <-- NO AGREGAR AQUÍ EN GPU CLASIFICACIÓN\n",
    "        random_seed=random_seed,\n",
    "        task_type='GPU',\n",
    "        devices='0',\n",
    "        gpu_ram_part=gpu_ram_part,\n",
    "        bootstrap_type=bootstrap_type,\n",
    "        subsample=subsample,\n",
    "        logging_level='Silent',\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "    if balancing_mode == 'class_auto':\n",
    "        params['auto_class_weights'] = 'Balanced'\n",
    "    elif balancing_mode == 'class_manual' and class_weight_ratio is not None:\n",
    "        params['class_weights'] = [1.0, float(class_weight_ratio)]\n",
    "\n",
    "    return CatBoostClassifier(**params)\n",
    "\n",
    "\n",
    "# ===================== Thresholding & Eval =====================\n",
    "\n",
    "def pick_threshold_precision_first(\n",
    "    y_true: np.ndarray, proba: np.ndarray,\n",
    "    min_precision: float = 0.90, min_recall: float = 0.03,\n",
    "    top_k_rate: float = 1e-4, min_alerts: int = 5\n",
    "):\n",
    "    \"\"\"Elige umbral que cumpla precisión y recall; fallback a top-k.\"\"\"\n",
    "    precision, recall, thr = precision_recall_curve(y_true, proba)\n",
    "    pr_auc = average_precision_score(y_true, proba)\n",
    "\n",
    "    valid = (precision >= min_precision) & (recall >= min_recall)\n",
    "    if valid.any():\n",
    "        idxs = np.where(valid)[0]\n",
    "        idx = idxs[-1] - 1 if idxs[-1] >= len(thr) else idxs[-1]\n",
    "        idx = max(0, min(idx, len(thr)-1))\n",
    "        chosen = thr[idx]\n",
    "    else:\n",
    "        k = max(max(1, min_alerts), int(len(proba) * max(top_k_rate, 1e-6)))\n",
    "        chosen = float(np.partition(proba, -k)[-k])\n",
    "\n",
    "    return float(chosen), float(pr_auc)\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true: np.ndarray, proba: np.ndarray, thr: float) -> Dict:\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        'confusion_matrix': [[int(tn), int(fp)], [int(fn), int(tp)]],\n",
    "        'fpr': float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "# ===================== MAIN PIPELINE =====================\n",
    "\n",
    "def train_catboost_precision_pipeline(\n",
    "    train_parquet: str,\n",
    "    test_parquet: Optional[str],\n",
    "    dataset_type: str,\n",
    "    train_years: List[int] = [2020, 2021, 2022, 2023],\n",
    "    dev_years:   List[int] = [2024],\n",
    "    test_years:  List[int] = [2025],\n",
    "    lookahead_days: int = 7,\n",
    "    n_splits: int = 5,\n",
    "\n",
    "    # Hard-negatives\n",
    "    hard_neg_ratio: int = 4,     # neg:pos en subset\n",
    "    hard_window: int = 60,       # HDD=90\n",
    "    hard_fraction: float = 0.7,\n",
    "\n",
    "    # Balancing (sólo compatibles CatBoost)\n",
    "    balancing: str = 'under',    # 'none'|'under'|'class_auto'|'class_manual'\n",
    "    balancing_neg_pos_ratio: int = 4,  # para 'under'\n",
    "    max_under_samples: Optional[int] = None,\n",
    "    manual_pos_weight: Optional[float] = None,  # n_neg/n_pos si 'class_manual'\n",
    "\n",
    "    # CatBoost\n",
    "    cb_depth: int = 8,\n",
    "    cb_iterations: int = 1500,\n",
    "    cb_learning_rate: float = 0.06,\n",
    "    cb_l2_leaf_reg: float = 4.0,\n",
    "    cb_border_count: int = 64,\n",
    "    cb_rsm: float = 0.9,\n",
    "    cb_bootstrap_type: str = 'Bernoulli',\n",
    "    cb_subsample: float = 0.8,\n",
    "\n",
    "    # Umbral\n",
    "    min_precision: float = 0.90,\n",
    "    min_recall: float = 0.03,\n",
    "    top_k_rate: float = 1e-4,\n",
    "    min_alerts: int = 20,\n",
    "\n",
    "    output_dir: str = './models_cb_prec_v8',\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    print(\"=\"*92)\n",
    "    print(f\"CATBOOST (GPU, PRECISION v8) - {dataset_type.upper()}  Train={train_years} | Dev={dev_years} | Test={test_years}\")\n",
    "    balancing = normalize_balancing_name(balancing)\n",
    "    print(f\"Balancing: {balancing}\")\n",
    "    print(\"=\"*92)\n",
    "    ensure_dir(output_dir)\n",
    "\n",
    "    # --------- Load ---------\n",
    "    df_tr_raw = load_data(train_parquet, train_years)\n",
    "    df_dev_raw = load_data(train_parquet, dev_years) if dev_years else pd.DataFrame()\n",
    "    df_te_raw  = load_data(test_parquet, test_years) if (test_parquet and test_years) else pd.DataFrame()\n",
    "    if df_tr_raw.empty:\n",
    "        raise ValueError(\"Training data is empty!\")\n",
    "\n",
    "    # --------- Prepare ---------\n",
    "    df_tr  = prepare_df(df_tr_raw)\n",
    "    df_dev = prepare_df(df_dev_raw) if not df_dev_raw.empty else pd.DataFrame()\n",
    "    df_te  = prepare_df(df_te_raw) if not df_te_raw.empty else pd.DataFrame()\n",
    "\n",
    "    # --------- Labels ---------\n",
    "    dtf_tr = compute_days_to_failure(df_tr)\n",
    "    y_tr   = create_labels_from_dtf(dtf_tr, lookahead_days)\n",
    "    print(f\"TRAIN labels: pos={int(y_tr.sum()):,} ({100*y_tr.mean():.5f}%)\")\n",
    "    if y_tr.sum() < 50:\n",
    "        raise ValueError(f\"Insufficient positive samples in TRAIN: {y_tr.sum()}\")\n",
    "\n",
    "    dtf_dev = compute_days_to_failure(df_dev) if not df_dev.empty else np.array([], dtype=np.int64)\n",
    "    y_dev   = create_labels_from_dtf(dtf_dev, lookahead_days) if not df_dev.empty else np.array([], dtype=np.int8)\n",
    "\n",
    "    # --------- Features ---------\n",
    "    X_tr, X_dev, X_te, ts_tr, ts_dev, ts_te, feature_names, cat_indices = create_features_joined_cat(\n",
    "        df_tr, df_dev, df_te, dataset_type, add_rolling=True\n",
    "    )\n",
    "    cat_cols = [feature_names[i] for i in cat_indices]\n",
    "\n",
    "    # --------- CV por disco (OOF diag) ---------\n",
    "    serials_tr = df_tr['serial_number']\n",
    "    oof_proba = np.zeros(len(y_tr), dtype=np.float32)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(make_group_folds(serials_tr, y_tr, n_splits=n_splits, random_state=random_state), start=1):\n",
    "        X_tr_fold, y_tr_fold = X_tr.iloc[tr_idx].reset_index(drop=True), y_tr[tr_idx]\n",
    "        dtf_tr_fold = dtf_tr[tr_idx]\n",
    "        ts_tr_fold = ts_tr[tr_idx]\n",
    "\n",
    "        X_va_fold, y_va_fold = X_tr.iloc[va_idx].reset_index(drop=True), y_tr[va_idx]\n",
    "        ts_va_fold = ts_tr[va_idx]\n",
    "\n",
    "        print(f\"\\nFold {fold}/{n_splits}: train={len(y_tr_fold):,} (pos={int(y_tr_fold.sum()):,}) | val={len(y_va_fold):,} (pos={int(y_va_fold.sum()):,})\")\n",
    "\n",
    "        # 1) Hard-negatives (control tamaño, enfoca frontera)\n",
    "        Xb, yb, sel_idx = sample_negatives_hard(\n",
    "            X_tr_fold, y_tr_fold, dtf_tr_fold,\n",
    "            lookahead=lookahead_days,\n",
    "            neg_pos_ratio=hard_neg_ratio,\n",
    "            hard_window=hard_window,\n",
    "            hard_fraction=hard_fraction,\n",
    "            seed=random_state\n",
    "        )\n",
    "        ts_b = ts_tr_fold[sel_idx]\n",
    "        print(f\"  After hard-neg: {len(yb):,} (pos={int(yb.sum()):,}, neg={len(yb)-int(yb.sum()):,})\")\n",
    "\n",
    "        # 2) Balancing (sólo compatibles)\n",
    "        ts_bal = ts_b\n",
    "        balancing_mode_for_cb = 'none'\n",
    "        class_weight_ratio = None\n",
    "\n",
    "        if balancing == 'under':\n",
    "            X_bal, y_bal, ts_bal = undersample_with_timestamps(\n",
    "                Xb, yb, timestamps=ts_b,\n",
    "                target_neg_pos=balancing_neg_pos_ratio,\n",
    "                max_total=max_under_samples,\n",
    "                seed=random_state\n",
    "            )\n",
    "        elif balancing == 'class_auto':\n",
    "            X_bal, y_bal = Xb, yb\n",
    "            balancing_mode_for_cb = 'class_auto'\n",
    "        elif balancing == 'class_manual':\n",
    "            X_bal, y_bal = Xb, yb\n",
    "            # ratio = n_neg/n_pos sobre el subset corriente\n",
    "            n_pos = max(1, int((y_bal == 1).sum()))\n",
    "            n_neg = max(1, int((y_bal == 0).sum()))\n",
    "            class_weight_ratio = float(n_neg / n_pos) if manual_pos_weight is None else float(manual_pos_weight)\n",
    "            balancing_mode_for_cb = 'class_manual'\n",
    "        else:  # 'none'\n",
    "            X_bal, y_bal = Xb, yb\n",
    "\n",
    "        print(f\"  After balancing [{balancing}]: {len(y_bal):,} (pos={int(y_bal.sum()):,}, neg={len(y_bal)-int(y_bal.sum()):,})\")\n",
    "\n",
    "        # 3) CatBoost\n",
    "        model = get_catboost(\n",
    "            depth=cb_depth,\n",
    "            iterations=cb_iterations,\n",
    "            learning_rate=cb_learning_rate,\n",
    "            l2_leaf_reg=cb_l2_leaf_reg,\n",
    "            border_count=cb_border_count,\n",
    "            rsm=cb_rsm,\n",
    "            random_seed=random_state,\n",
    "            bootstrap_type=cb_bootstrap_type,\n",
    "            subsample=cb_subsample,\n",
    "            balancing_mode=balancing_mode_for_cb,\n",
    "            class_weight_ratio=class_weight_ratio\n",
    "        )\n",
    "        train_pool = Pool(X_bal, label=y_bal, cat_features=cat_indices, timestamp=ts_bal)\n",
    "        valid_pool = Pool(X_va_fold, label=y_va_fold, cat_features=cat_indices, timestamp=ts_va_fold)\n",
    "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True, early_stopping_rounds=200)\n",
    "\n",
    "        proba_va = model.predict_proba(valid_pool)[:, 1]\n",
    "        thr_oof, pr_auc = pick_threshold_precision_first(\n",
    "            y_va_fold, proba_va, min_precision=min_precision, min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "        )\n",
    "        oof_proba[va_idx] = proba_va\n",
    "        m = metrics_at_threshold(y_va_fold, proba_va, thr_oof)\n",
    "        m.update({'pr_auc': float(pr_auc), 'threshold': float(thr_oof), 'fold': int(fold),\n",
    "                  'best_iteration': int(model.get_best_iteration())})\n",
    "        fold_metrics.append(m)\n",
    "        print(f\"  Fold {fold} @thr={thr_oof:.4f} | P={m['precision']:.3f} R={m['recall']:.3f} F1={m['f1']:.3f} PR-AUC={pr_auc:.4f}\")\n",
    "\n",
    "        del X_tr_fold, y_tr_fold, X_va_fold, y_va_fold, Xb, yb, X_bal, y_bal, model, train_pool, valid_pool\n",
    "        cleanup()\n",
    "\n",
    "    # --------- OOF (diagnóstico) ---------\n",
    "    thr_oof_global, pr_auc_oof = pick_threshold_precision_first(\n",
    "        y_tr, oof_proba, min_precision=min_precision, min_recall=min_recall,\n",
    "        top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "    )\n",
    "    agg = metrics_at_threshold(y_tr, oof_proba, thr_oof_global)\n",
    "    agg.update({'pr_auc': float(pr_auc_oof), 'threshold': float(thr_oof_global)})\n",
    "    print(\"\\nOOF (diag):\")\n",
    "    print(json.dumps(agg, indent=2))\n",
    "\n",
    "    # --------- Calibración en DEV ---------\n",
    "    thr_prod = float(thr_oof_global)\n",
    "    dev_metrics = None\n",
    "    if not df_dev.empty:\n",
    "        # Repite subset + balancing en TODO TRAIN para modelo final de calibración\n",
    "        Xb_full, yb_full, sel_idx_full = sample_negatives_hard(\n",
    "            X_tr, y_tr, dtf_tr, lookahead=lookahead_days,\n",
    "            neg_pos_ratio=hard_neg_ratio, hard_window=hard_window,\n",
    "            hard_fraction=hard_fraction, seed=random_state\n",
    "        )\n",
    "        ts_full = ts_tr[sel_idx_full]\n",
    "\n",
    "        ts_bal_full = ts_full\n",
    "        balancing_mode_for_cb = 'none'\n",
    "        class_weight_ratio = None\n",
    "\n",
    "        if balancing == 'under':\n",
    "            X_bal_full, y_bal_full, ts_bal_full = undersample_with_timestamps(\n",
    "                Xb_full, yb_full, timestamps=ts_full,\n",
    "                target_neg_pos=balancing_neg_pos_ratio,\n",
    "                max_total=max_under_samples,\n",
    "                seed=random_state\n",
    "            )\n",
    "        elif balancing == 'class_auto':\n",
    "            X_bal_full, y_bal_full = Xb_full, yb_full\n",
    "            balancing_mode_for_cb = 'class_auto'\n",
    "        elif balancing == 'class_manual':\n",
    "            X_bal_full, y_bal_full = Xb_full, yb_full\n",
    "            n_pos = max(1, int((y_bal_full == 1).sum()))\n",
    "            n_neg = max(1, int((y_bal_full == 0).sum()))\n",
    "            class_weight_ratio = float(n_neg / n_pos) if manual_pos_weight is None else float(manual_pos_weight)\n",
    "            balancing_mode_for_cb = 'class_manual'\n",
    "        else:\n",
    "            X_bal_full, y_bal_full = Xb_full, yb_full\n",
    "\n",
    "        final_model_dev = get_catboost(\n",
    "            depth=cb_depth, iterations=cb_iterations, learning_rate=cb_learning_rate,\n",
    "            l2_leaf_reg=cb_l2_leaf_reg, border_count=cb_border_count, rsm=cb_rsm,\n",
    "            random_seed=random_state, bootstrap_type=cb_bootstrap_type, subsample=cb_subsample,\n",
    "            balancing_mode=balancing_mode_for_cb, class_weight_ratio=class_weight_ratio\n",
    "        )\n",
    "        train_pool_full = Pool(X_bal_full, label=y_bal_full, cat_features=cat_indices, timestamp=ts_bal_full)\n",
    "        final_model_dev.fit(train_pool_full, use_best_model=False)\n",
    "\n",
    "        dev_pool = Pool(X_dev, label=y_dev, cat_features=cat_indices, timestamp=ts_dev)\n",
    "        proba_dev = final_model_dev.predict_proba(dev_pool)[:, 1]\n",
    "\n",
    "        thr_prod, pr_auc_dev = pick_threshold_precision_first(\n",
    "            y_dev, proba_dev, min_precision=min_precision, min_recall=min_recall,\n",
    "            top_k_rate=top_k_rate, min_alerts=min_alerts\n",
    "        )\n",
    "        dev_metrics = metrics_at_threshold(y_dev, proba_dev, thr_prod)\n",
    "        dev_metrics.update({'pr_auc': float(pr_auc_dev), 'threshold': float(thr_prod)})\n",
    "        print(\"\\nDEV calibration (threshold for PROD):\")\n",
    "        print(json.dumps(dev_metrics, indent=2))\n",
    "\n",
    "        del final_model_dev, train_pool_full, dev_pool\n",
    "        cleanup()\n",
    "\n",
    "    # --------- TEST ---------\n",
    "    test_metrics = None\n",
    "    if not df_te.empty:\n",
    "        Xb_full, yb_full, sel_idx_full = sample_negatives_hard(\n",
    "            X_tr, y_tr, dtf_tr, lookahead=lookahead_days,\n",
    "            neg_pos_ratio=hard_neg_ratio, hard_window=hard_window,\n",
    "            hard_fraction=hard_fraction, seed=random_state\n",
    "        )\n",
    "        ts_full = ts_tr[sel_idx_full]\n",
    "\n",
    "        ts_bal_full = ts_full\n",
    "        balancing_mode_for_cb = 'none'\n",
    "        class_weight_ratio = None\n",
    "\n",
    "        if balancing == 'under':\n",
    "            X_bal_full, y_bal_full, ts_bal_full = undersample_with_timestamps(\n",
    "                Xb_full, yb_full, timestamps=ts_full,\n",
    "                target_neg_pos=balancing_neg_pos_ratio,\n",
    "                max_total=max_under_samples,\n",
    "                seed=random_state\n",
    "            )\n",
    "        elif balancing == 'class_auto':\n",
    "            X_bal_full, y_bal_full = Xb_full, yb_full\n",
    "            balancing_mode_for_cb = 'class_auto'\n",
    "        elif balancing == 'class_manual':\n",
    "            X_bal_full, y_bal_full = Xb_full, yb_full\n",
    "            n_pos = max(1, int((y_bal_full == 1).sum()))\n",
    "            n_neg = max(1, int((y_bal_full == 0).sum()))\n",
    "            class_weight_ratio = float(n_neg / n_pos) if manual_pos_weight is None else float(manual_pos_weight)\n",
    "            balancing_mode_for_cb = 'class_manual'\n",
    "        else:\n",
    "            X_bal_full, y_bal_full = Xb_full, yb_full\n",
    "\n",
    "        final_model = get_catboost(\n",
    "            depth=cb_depth, iterations=cb_iterations, learning_rate=cb_learning_rate,\n",
    "            l2_leaf_reg=cb_l2_leaf_reg, border_count=cb_border_count, rsm=cb_rsm,\n",
    "            random_seed=random_state, bootstrap_type=cb_bootstrap_type, subsample=cb_subsample,\n",
    "            balancing_mode=balancing_mode_for_cb, class_weight_ratio=class_weight_ratio\n",
    "        )\n",
    "        train_pool_full = Pool(X_bal_full, label=y_bal_full, cat_features=cat_indices, timestamp=ts_bal_full)\n",
    "        final_model.fit(train_pool_full, use_best_model=False)\n",
    "\n",
    "        y_test = create_labels_from_dtf(compute_days_to_failure(df_te), lookahead_days)\n",
    "        test_pool = Pool(X_te, label=y_test, cat_features=cat_indices, timestamp=ts_te)\n",
    "        proba_test = final_model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "        thr_used = float(thr_prod)\n",
    "        test_metrics = metrics_at_threshold(y_test, proba_test, thr_used)\n",
    "        test_metrics.update({'pr_auc': float(average_precision_score(y_test, proba_test)), 'threshold_used': thr_used})\n",
    "\n",
    "        stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        prefix = os.path.join(output_dir, f\"{dataset_type}_cb_prec_v8_{balancing}_{stamp}\")\n",
    "        final_model.save_model(f\"{prefix}.cbm\")\n",
    "        with open(f\"{prefix}_features.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                'feature_names': feature_names,\n",
    "                'cat_indices': cat_indices,\n",
    "                'threshold': thr_used,\n",
    "                'calibration': 'dev_years',\n",
    "                'balancing': balancing\n",
    "            }, f, indent=2)\n",
    "        print(f\"\\n✓ Final model saved: {prefix}.cbm\")\n",
    "        del final_model, train_pool_full, test_pool\n",
    "        cleanup()\n",
    "\n",
    "    # --------- Metadata ---------\n",
    "    meta = {\n",
    "        'dataset_type': dataset_type,\n",
    "        'train_years': train_years,\n",
    "        'dev_years': dev_years,\n",
    "        'test_years': test_years,\n",
    "        'lookahead_days': lookahead_days,\n",
    "        'n_splits': n_splits,\n",
    "        'hard_negative_sampling': {\n",
    "            'neg_pos_ratio': hard_neg_ratio,\n",
    "            'hard_window': hard_window,\n",
    "            'hard_fraction': hard_fraction\n",
    "        },\n",
    "        'balancing': balancing,\n",
    "        'catboost_params': {\n",
    "            'depth': cb_depth, 'iterations': cb_iterations, 'learning_rate': cb_learning_rate,\n",
    "            'l2_leaf_reg': cb_l2_leaf_reg, 'border_count': cb_border_count, 'rsm': cb_rsm,\n",
    "            'bootstrap_type': cb_bootstrap_type, 'subsample': cb_subsample,\n",
    "            'task_type': 'GPU', 'logging_level': 'Silent'\n",
    "        },\n",
    "        'threshold_objective': {'min_precision': min_precision, 'min_recall': min_recall,\n",
    "                                'top_k_rate': top_k_rate, 'min_alerts': min_alerts},\n",
    "        'oof_metrics': {\n",
    "            'precision': agg['precision'], 'recall': agg['recall'], 'f1': agg['f1'],\n",
    "            'confusion_matrix': agg['confusion_matrix'], 'fpr': agg['fpr'],\n",
    "            'pr_auc': agg['pr_auc'], 'threshold': agg['threshold']\n",
    "        },\n",
    "        'dev_metrics': dev_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'feature_names': feature_names,\n",
    "        'cat_indices': cat_indices,\n",
    "        'normalization': 'per-model robust z + log1p on *_raw'\n",
    "    }\n",
    "    meta_path = os.path.join(output_dir, f\"{dataset_type}_cb_prec_v8_{balancing}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_metadata.json\")\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"\\n✓ Metadata saved: {meta_path}\")\n",
    "    print(\"=\"*92)\n",
    "    return meta\n",
    "\n",
    "\n",
    "# ===================== WRAPPERS =====================\n",
    "\n",
    "def train_ssd_cb_v8():\n",
    "    \"\"\"SSD: muy desbalanceado; empezar con UNDER o CLASS_AUTO, y calibrar en 2024.\"\"\"\n",
    "    return train_catboost_precision_pipeline(\n",
    "        train_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/SSD_FULL_CLEAN.parquet',\n",
    "        dataset_type='SSD',\n",
    "        train_years=[2020, 2021, 2022, 2023],\n",
    "        dev_years=[2024],\n",
    "        test_years=[2025],\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "\n",
    "        hard_neg_ratio=3,\n",
    "        hard_window=60,\n",
    "        hard_fraction=0.7,\n",
    "\n",
    "        balancing='under',                # 'under' o 'class_auto'\n",
    "        balancing_neg_pos_ratio=3,\n",
    "        max_under_samples=40_000,\n",
    "\n",
    "        cb_depth=8,\n",
    "        cb_iterations=1500,\n",
    "        cb_learning_rate=0.06,\n",
    "        cb_l2_leaf_reg=4.0,\n",
    "        cb_border_count=64,\n",
    "        cb_rsm=0.9,\n",
    "        cb_bootstrap_type='Bernoulli',\n",
    "        cb_subsample=0.8,\n",
    "\n",
    "        min_precision=0.90,\n",
    "        min_recall=0.03,\n",
    "        top_k_rate=1e-4,\n",
    "        min_alerts=20,\n",
    "        output_dir='./models_cb_ssd_prec_v8',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "def train_hdd_cb_v8():\n",
    "    \"\"\"HDD: dataset grande; suele ir mejor UNDER suave o CLASS_AUTO (no ambos).\"\"\"\n",
    "    return train_catboost_precision_pipeline(\n",
    "        train_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        test_parquet='./Procesados/finales/HDD_FULL_CLEAN.parquet',\n",
    "        dataset_type='HDD',\n",
    "        train_years=[2020, 2021, 2022, 2023],\n",
    "        dev_years=[2024],\n",
    "        test_years=[2025],\n",
    "        lookahead_days=7,\n",
    "        n_splits=5,\n",
    "\n",
    "        hard_neg_ratio=5,\n",
    "        hard_window=90,\n",
    "        hard_fraction=0.7,\n",
    "\n",
    "        balancing='under',                # probar 'class_auto' si no haces under\n",
    "        balancing_neg_pos_ratio=5,\n",
    "        max_under_samples=60_000,\n",
    "\n",
    "        cb_depth=8,\n",
    "        cb_iterations=1800,\n",
    "        cb_learning_rate=0.06,\n",
    "        cb_l2_leaf_reg=6.0,\n",
    "        cb_border_count=128,\n",
    "        cb_rsm=0.9,\n",
    "        cb_bootstrap_type='Bernoulli',\n",
    "        cb_subsample=0.8,\n",
    "\n",
    "        min_precision=0.90,\n",
    "        min_recall=0.03,\n",
    "        top_k_rate=7.5e-5,\n",
    "        min_alerts=30,\n",
    "        output_dir='./models_cb_hdd_prec_v8',\n",
    "        random_state=42\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ad4a93-5310-4476-943d-1f1d8417a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "CATBOOST (GPU, PRECISION v8) - SSD  Train=[2020, 2021, 2022, 2023] | Dev=[2024] | Test=[2025]\n",
      "Balancing: under\n",
      "============================================================================================\n",
      "Loading years=[2020, 2021, 2022, 2023] from ./Procesados/finales/SSD_FULL_CLEAN.parquet ...\n",
      "Loaded 2,124,111 rows\n",
      "Loading years=[2024] from ./Procesados/finales/SSD_FULL_CLEAN.parquet ...\n",
      "Loaded 1,220,745 rows\n",
      "Loading years=[2025] from ./Procesados/finales/SSD_FULL_CLEAN.parquet ...\n",
      "Loaded 0 rows\n",
      "TRAIN labels: pos=1,325 (0.06238%)\n",
      "Creating features (temporal join) for SSD with TRAIN∪DEV∪TEST...\n",
      "  Found 13 SMART attributes (raw)\n",
      "  Final features: 97 (cat=2, num≈95)\n",
      "\n",
      "Fold 1/5: train=1,709,653 (pos=1,079) | val=414,458 (pos=246)\n",
      "  After hard-neg: 4,316 (pos=1,079, neg=3,237)\n",
      "  After balancing [under]: 4,316 (pos=1,079, neg=3,237)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1 @thr=0.7843 | P=1.000 R=0.045 F1=0.086 PR-AUC=0.2639\n",
      "\n",
      "Fold 2/5: train=1,707,701 (pos=1,054) | val=416,410 (pos=271)\n",
      "  After hard-neg: 4,216 (pos=1,054, neg=3,162)\n",
      "  After balancing [under]: 4,216 (pos=1,054, neg=3,162)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2 @thr=0.8877 | P=0.900 R=0.033 F1=0.064 PR-AUC=0.2755\n",
      "\n",
      "Fold 3/5: train=1,682,081 (pos=1,061) | val=442,030 (pos=264)\n",
      "  After hard-neg: 4,244 (pos=1,061, neg=3,183)\n",
      "  After balancing [under]: 4,244 (pos=1,061, neg=3,183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3 @thr=0.7458 | P=1.000 R=0.049 F1=0.094 PR-AUC=0.3433\n",
      "\n",
      "Fold 4/5: train=1,691,757 (pos=1,061) | val=432,354 (pos=264)\n",
      "  After hard-neg: 4,244 (pos=1,061, neg=3,183)\n",
      "  After balancing [under]: 4,244 (pos=1,061, neg=3,183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4 @thr=0.9003 | P=1.000 R=0.030 F1=0.059 PR-AUC=0.3314\n",
      "\n",
      "Fold 5/5: train=1,705,252 (pos=1,045) | val=418,859 (pos=280)\n",
      "  After hard-neg: 4,180 (pos=1,045, neg=3,135)\n",
      "  After balancing [under]: 4,180 (pos=1,045, neg=3,135)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 5 @thr=0.8969 | P=1.000 R=0.032 F1=0.062 PR-AUC=0.2810\n",
      "\n",
      "OOF (diag):\n",
      "{\n",
      "  \"precision\": 0.975609756097561,\n",
      "  \"recall\": 0.03018867924528302,\n",
      "  \"f1\": 0.05856515373352855,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      2122785,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      1285,\n",
      "      40\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 4.710790442371487e-07,\n",
      "  \"pr_auc\": 0.2520734140833044,\n",
      "  \"threshold\": 0.8857017755508423\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEV calibration (threshold for PROD):\n",
      "{\n",
      "  \"precision\": 0.0,\n",
      "  \"recall\": 0.0,\n",
      "  \"f1\": 0.0,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      1220396,\n",
      "      122\n",
      "    ],\n",
      "    [\n",
      "      227,\n",
      "      0\n",
      "    ]\n",
      "  ],\n",
      "  \"fpr\": 9.995755900363616e-05,\n",
      "  \"pr_auc\": 0.0006149018328227931,\n",
      "  \"threshold\": 0.9080645798309006\n",
      "}\n",
      "\n",
      "✓ Metadata saved: ./models_cb_ssd_prec_v8/SSD_cb_prec_v8_under_20251105_235058_metadata.json\n",
      "============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_type': 'SSD',\n",
       " 'train_years': [2020, 2021, 2022, 2023],\n",
       " 'dev_years': [2024],\n",
       " 'test_years': [2025],\n",
       " 'lookahead_days': 7,\n",
       " 'n_splits': 5,\n",
       " 'hard_negative_sampling': {'neg_pos_ratio': 3,\n",
       "  'hard_window': 60,\n",
       "  'hard_fraction': 0.7},\n",
       " 'balancing': 'under',\n",
       " 'catboost_params': {'depth': 8,\n",
       "  'iterations': 1500,\n",
       "  'learning_rate': 0.06,\n",
       "  'l2_leaf_reg': 4.0,\n",
       "  'border_count': 64,\n",
       "  'rsm': 0.9,\n",
       "  'bootstrap_type': 'Bernoulli',\n",
       "  'subsample': 0.8,\n",
       "  'task_type': 'GPU',\n",
       "  'logging_level': 'Silent'},\n",
       " 'threshold_objective': {'min_precision': 0.9,\n",
       "  'min_recall': 0.03,\n",
       "  'top_k_rate': 0.0001,\n",
       "  'min_alerts': 20},\n",
       " 'oof_metrics': {'precision': 0.975609756097561,\n",
       "  'recall': 0.03018867924528302,\n",
       "  'f1': 0.05856515373352855,\n",
       "  'confusion_matrix': [[2122785, 1], [1285, 40]],\n",
       "  'fpr': 4.710790442371487e-07,\n",
       "  'pr_auc': 0.2520734140833044,\n",
       "  'threshold': 0.8857017755508423},\n",
       " 'dev_metrics': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'confusion_matrix': [[1220396, 122], [227, 0]],\n",
       "  'fpr': 9.995755900363616e-05,\n",
       "  'pr_auc': 0.0006149018328227931,\n",
       "  'threshold': 0.9080645798309006},\n",
       " 'test_metrics': None,\n",
       " 'feature_names': ['model',\n",
       "  'vendor',\n",
       "  'capacity_bytes',\n",
       "  'smart_5_raw',\n",
       "  'smart_187_raw',\n",
       "  'smart_194_raw',\n",
       "  'smart_231_raw',\n",
       "  'smart_233_raw',\n",
       "  'smart_241_raw',\n",
       "  'smart_242_raw',\n",
       "  'smart_9_raw',\n",
       "  'smart_173_raw',\n",
       "  'smart_174_raw',\n",
       "  'smart_184_raw',\n",
       "  'smart_199_raw',\n",
       "  'smart_230_raw',\n",
       "  'delta_smart_5_raw',\n",
       "  'max_smart_5_raw',\n",
       "  'delta_smart_187_raw',\n",
       "  'max_smart_187_raw',\n",
       "  'delta_smart_194_raw',\n",
       "  'max_smart_194_raw',\n",
       "  'delta_smart_231_raw',\n",
       "  'max_smart_231_raw',\n",
       "  'delta_smart_233_raw',\n",
       "  'max_smart_233_raw',\n",
       "  'delta_smart_241_raw',\n",
       "  'max_smart_241_raw',\n",
       "  'delta_smart_242_raw',\n",
       "  'max_smart_242_raw',\n",
       "  'delta_smart_9_raw',\n",
       "  'max_smart_9_raw',\n",
       "  'delta_smart_173_raw',\n",
       "  'max_smart_173_raw',\n",
       "  'delta_smart_174_raw',\n",
       "  'max_smart_174_raw',\n",
       "  'delta_smart_184_raw',\n",
       "  'max_smart_184_raw',\n",
       "  'delta_smart_199_raw',\n",
       "  'max_smart_199_raw',\n",
       "  'delta_smart_230_raw',\n",
       "  'max_smart_230_raw',\n",
       "  'rmean7_smart_5_raw',\n",
       "  'rstd7_smart_5_raw',\n",
       "  'rmean7_smart_187_raw',\n",
       "  'rstd7_smart_187_raw',\n",
       "  'rmean7_smart_194_raw',\n",
       "  'rstd7_smart_194_raw',\n",
       "  'rmean7_smart_231_raw',\n",
       "  'rstd7_smart_231_raw',\n",
       "  'rmean7_smart_233_raw',\n",
       "  'rstd7_smart_233_raw',\n",
       "  'rmean7_smart_241_raw',\n",
       "  'rstd7_smart_241_raw',\n",
       "  'rmean7_smart_242_raw',\n",
       "  'rstd7_smart_242_raw',\n",
       "  'rmean7_smart_9_raw',\n",
       "  'rstd7_smart_9_raw',\n",
       "  'rmean7_smart_173_raw',\n",
       "  'rstd7_smart_173_raw',\n",
       "  'rmean7_smart_174_raw',\n",
       "  'rstd7_smart_174_raw',\n",
       "  'rmean7_smart_184_raw',\n",
       "  'rstd7_smart_184_raw',\n",
       "  'rmean7_smart_199_raw',\n",
       "  'rstd7_smart_199_raw',\n",
       "  'rmean7_smart_230_raw',\n",
       "  'rstd7_smart_230_raw',\n",
       "  'age_days',\n",
       "  'month',\n",
       "  'day_of_week',\n",
       "  'z_smart_5_raw',\n",
       "  'log1p_smart_5_raw',\n",
       "  'z_smart_187_raw',\n",
       "  'log1p_smart_187_raw',\n",
       "  'z_smart_194_raw',\n",
       "  'log1p_smart_194_raw',\n",
       "  'z_smart_231_raw',\n",
       "  'log1p_smart_231_raw',\n",
       "  'z_smart_233_raw',\n",
       "  'log1p_smart_233_raw',\n",
       "  'z_smart_241_raw',\n",
       "  'log1p_smart_241_raw',\n",
       "  'z_smart_242_raw',\n",
       "  'log1p_smart_242_raw',\n",
       "  'z_smart_9_raw',\n",
       "  'log1p_smart_9_raw',\n",
       "  'z_smart_173_raw',\n",
       "  'log1p_smart_173_raw',\n",
       "  'z_smart_174_raw',\n",
       "  'log1p_smart_174_raw',\n",
       "  'z_smart_184_raw',\n",
       "  'log1p_smart_184_raw',\n",
       "  'z_smart_199_raw',\n",
       "  'log1p_smart_199_raw',\n",
       "  'z_smart_230_raw',\n",
       "  'log1p_smart_230_raw'],\n",
       " 'cat_indices': [0, 1],\n",
       " 'normalization': 'per-model robust z + log1p on *_raw'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssd_cb_v8()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
